<doc id="4194" url="http://en.wikipedia.org/wiki?curid=4194" title="Bohrium">
Bohrium

Bohrium is a chemical element with symbol Bh and atomic number 107, named in honor of Danish physicist Niels Bohr. It is a synthetic element (an element that can be created in a laboratory but is not found in nature) and radioactive; the most stable known isotope, 270Bh, has a half-life of approximately 61 seconds.
In the periodic table of the elements, it is a d-block transactinide element. It is a member of the 7th period and belongs to the group 7 elements. Chemistry experiments have confirmed that bohrium behaves as the heavier homologue to rhenium in group 7. The chemical properties of bohrium are characterized only partly, but they compare well with the chemistry of the other group 7 elements.
History.
Official discovery.
Bohrium was first convincingly synthesized in 1981 by a German research team led by Peter Armbruster and Gottfried Münzenberg at the Institute for Heavy Ion Research (Gesellschaft für Schwerionenforschung) in Darmstadt. The team bombarded a target of bismuth-209 with accelerated nuclei of chromium-54 to produce 5 atoms of the isotope bohrium-262:
The IUPAC/IUPAP Transfermium Working Group (TWG) recognised the GSI collaboration as official discoverers in their 1992 report.
Proposed names.
The German group suggested the name "nielsbohrium" with symbol "Ns" to honor the Danish physicist Niels Bohr. The Soviet scientists at the Joint Institute for Nuclear Research in Dubna, Russia had suggested this name be given to element 105 (which was finally called dubnium) and the German team wished to recognise both Bohr and the fact that the Dubna team had been the first to propose the cold fusion reaction to solve the controversial problem of the naming of element 105. The Dubna team agreed with the German group's naming proposal for element 107.
There was an element naming controversy as to what the elements from 104 to 106 were to be called; the IUPAC adopted "unnilseptium" (symbol "Uns") as a temporary, systematic element name for this element. In 1994 a committee of IUPAC recommended that element 107 be named "bohrium", not "nielsbohrium", since there was no precedence for using a scientist's complete name in the naming of an element. This was opposed by the discoverers as there was some concern that the name might be confused with boron and in particular the distinguishing of the names of their respective oxyanions, "bohrate" and "borate". The matter was handed to the Danish branch of IUPAC which, despite this, voted in favour of the name "bohrium", and thus the name "bohrium" for element 107 was recognized internationally in 1997. The IUPAC subsequently decided that bohrium salts should be called "bohriates" instead of "bohrates".
Isotopes.
Bohrium has no stable or naturally-occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Eleven different isotopes of bohrium have been reported with atomic masses 260–262, 264–267, 270–272, 274, one of which, bohrium-262, has a known metastable state. All of these decay only through alpha decay, although some unknown bohrium isotopes are predicted to undergo spontaneous fission.
Stability and half-lives.
The lighter isotopes usually have shorter half-lives; half-lives of under 100 ms for 260Bh, 261Bh, 262Bh, and 262mBh were observed. 264Bh, 265Bh, 266Bh, and 271Bh are more stable at around 1 s, and 267Bh and 272Bh have half-lives of about 10 s. The heaviest isotopes are the most stable, with 270Bh and 274Bh having measured half-lives of about 61 s and 54 s respectively. The unknown isotopes 273Bh and 275Bh are predicted to have even longer half-lives of around 90 minutes and 40 minutes respectively. Before its discovery, 274Bh was also predicted to have a long half-life of 90 minutes, but it was found to have a shorter half-life of only about 54 seconds.
The proton-rich isotopes with masses 260, 261, and 262 were directly produced by cold fusion, those with mass 262 and 264 were reported in the decay chains of meitnerium and roentgenium, while the neutron-rich isotopes with masses 265, 266, 267 were created in irradiations of actinide targets. The four most neutron-rich ones with masses 270, 271, 272, and 274 appear in the decay chains of 282113, 287115, 288115, and 294117 respectively. These eleven isotopes have half-lives ranging from 8 miliseconds to 1 minute.
Chemical properties.
Extrapolated.
Bohrium is the fourth member of the 6d series of transition metals and the heaviest member of group VII in the Periodic Table, below manganese, technetium and rhenium. All the members of the group readily portray their group oxidation state of +7 and the state becomes more stable as the group is descended. Thus bohrium is expected to form a stable +7 state. Technetium also shows a stable +4 state whilst rhenium exhibits stable +4 and +3 states. Bohrium may therefore show these lower states as well.
The heavier members of the group are known to form volatile heptoxides M2O7, so bohrium should also form the volatile oxide Bh2O7. The oxide should dissolve in water to form perbohric acid, HBhO4.
Rhenium and technetium form a range of oxyhalides from the halogenation of the oxide. The chlorination of the oxide forms the oxychlorides MO3Cl, so BhO3Cl should be formed in this reaction. Fluorination results in MO3F and MO2F3 for the heavier elements in addition to the rhenium compounds ReOF5 and ReF7. Therefore, oxyfluoride formation for bohrium may help to indicate eka-rhenium properties.
Bohrium is expected to be a solid under normal conditions and assume a hexagonal close-packed crystal structure ("c"/"a" = 1.62), similar to its lighter congener rhenium.
Experimental.
In 1995, the first report on attempted isolation of the element was unsuccessful.
In 2000, it was confirmed that although relativistic effects are important, the 107th element does behave like a typical group 7 element.
In 2000, a team at the PSI conducted a chemistry reaction using atoms of 267Bh produced in the reaction between 249Bk and 22Ne ions. The resulting atoms were thermalised and reacted with a HCl/O2 mixture to form a volatile oxychloride. The reaction also produced isotopes of its lighter homologues, technetium (as 108Tc) and rhenium (as 169Re). The isothermal adsorption curves were measured and gave strong evidence for the formation of a volatile oxychloride with properties similar to that of rhenium oxychloride. This placed bohrium as a typical member of group 7.

</doc>
<doc id="4195" url="http://en.wikipedia.org/wiki?curid=4195" title="Barbara Olson">
Barbara Olson

Barbara Kay Olson (born Barbara Kay Bracher; December 27, 1955 September 11, 2001) was a lawyer and conservative American television commentator who worked for CNN, Fox News Channel, and several other outlets. She was a passenger on American Airlines Flight 77 en route to a taping of Bill Maher's television show "Politically Incorrect" when it was flown into the Pentagon in the September 11 attacks.
Early life.
Olson was born Barbara Kay Bracher in Houston, Texas. (Her older sister, Toni Bracher-Lawrence, has been a member of the Houston City Council since 2004.) She graduated from Waltrip High School and earned a Bachelor of Arts from the University of Saint Thomas in Houston. She earned a Juris Doctor degree from Yeshiva University's Benjamin N. Cardozo School of Law.
Career.
As a newcomer, she achieved a surprising measure of success, working for HBO and Stacy Keach Productions. In the early 1990s, she worked as an associate at the Washington, D.C.-based law firm of Wilmer Cutler & Pickering where she did civil litigation for several years before becoming an Assistant U.S. Attorney. 
Olson's support in 1991 of Supreme Court nominee Clarence Thomas led to the formation of the Independent Women's Forum. At that time, Olson and friend Rosalie (Ricky) Gaull Silberman started an informal network of women who supported the Thomas nomination to the Supreme Court despite allegations of sexual harassment by Anita Hill, a former subordinate of Thomas at the Equal Employment Opportunity Commission. Olson, who had also worked under Thomas at the EEOC and was a close friend of Thomas, spoke out on his behalf during his contentious Senate confirmation hearings. Olson later helped edit "The Real Anita Hill", a book by David Brock that savaged Hill and portrayed the harassment claim as a political dirty trick (Brock later recanted his claims and apologized to Hill). The Independent Women's Forum continued on with a goal of remaining a high profile group of women to advocate for economic and political freedom and personal responsibility. 
In 1994, Olson became chief investigative counsel for the U.S. House of Representatives Committee on Oversight and Government Reform. In that position, she led the Travelgate and Filegate investigations into the Clinton administration. She was later a partner in the Washington, D.C. office of the Birmingham, Alabama law firm Balch & Bingham. 
Personal life.
She married Theodore Olson in 1996. Theodore went on to successfully represent presidential candidate George W. Bush in the Supreme Court case of "Bush v. Gore", and subsequently served as U.S. Solicitor General in the Bush administration. 
Olson was a frequent critic of the Bill Clinton administration and wrote a book about then First Lady Hillary Rodham Clinton, "Hell to Pay: The Unfolding Story of Hillary Rodham Clinton" (1999). Olson's second book, "The Final Days: The Last, Desperate Abuses of Power by the Clinton White House" was published posthumously.
She was a resident of Great Falls, Virginia.
Death and legacy.
Olson was a passenger on American Airlines Flight 77 on her way to a taping of "Politically Incorrect" in Los Angeles, when it was flown into the Pentagon in the September 11 attacks. At the National September 11 Memorial, Olson's name is located on Panel S-70 of the South Pool, along with those of other passengers of Flight 77.

</doc>
<doc id="4196" url="http://en.wikipedia.org/wiki?curid=4196" title="Barnard's Star">
Barnard's Star

Barnard's Star is a very low-mass red dwarf star about six light-years away from Earth in the constellation of Ophiuchus, the Snake-holder. Barnard's Star is the fourth-closest known individual star to the Sun, after the three components of the Alpha Centauri system, and the closest star in the Northern Hemisphere. Despite its proximity, Barnard's Star, at a dim apparent magnitude of about nine, is not visible with the unaided eye; however, it is much brighter in the infrared than it is in visible light. The star is named for American astronomer E.E. Barnard. He was not the first to observe the star (it appeared on Harvard College University plates in 1888 and 1890), but in 1916 he measured its proper motion as 10.3 arcseconds (20,000 inverse radians) per year, which remains the largest-known proper motion of any star relative to the Solar System.
Barnard's Star has been the subject of much study, and it has probably received more attention from astronomers than any other class M dwarf star due to its proximity and favorable location for observation near the celestial equator. Historically, research on Barnard's Star has focused on measuring its stellar characteristics, its astrometry, and also refining the limits of possible extrasolar planets. Although Barnard's Star is an ancient star, some observations suggest that it still experiences star flare events.
Barnard's Star has also been the subject of some controversy. For a decade, from the early 1960s to the early 1970s, Peter van de Kamp claimed that there were one or more gas giants in orbit around it. Although the presence of small terrestrial planets around Barnard's Star remains a possibility, Van de Kamp's specific claims of large gas giants were refuted in the mid-1970s.
Barnard's Star is also notable as the target for Project Daedalus, a study on the possibility of fast, unmanned travel to nearby star systems.
Overview.
Barnard's Star is a red dwarf of the dim spectral type M4, and it is too faint to see without a telescope. Its apparent magnitude is 9.54. This compares with a magnitude of −1.5 for Sirius – the brightest star in the night sky – and about 6.0 for the faintest visible objects with the naked eye (this magnitude scale is logarithmic, and so the magnitude of 9.54 is only about 1/27th of the brightness of the faintest star that can be seen with the naked eye under good viewing conditions).
At seven to 12 billion years of age, Barnard's Star is considerably older than the Sun billion, and it might be among the oldest stars in the Milky Way galaxy. Barnard's Star has lost a great deal of rotational energy, and the periodic slight changes in its brightness indicate that it rotates just once in 130 days (the Sun rotates in 25). Given its age, Barnard's Star was long assumed to be quiescent in terms of stellar activity. However in 1998, astronomers observed an intense stellar flare, surprisingly showing that Barnard's Star is a flare star. Barnard's Star has the variable star designation V2500 Ophiuchi. In 2003, Barnard's Star presented the first detectable change in the radial velocity of a star caused by its motion. Further variability in the radial velocity of Barnard's Star was attributed to its stellar activity.
The proper motion of Barnard's Star corresponds to a relative lateral speed ("sideways" relative to our line of sight to the Sun) of 90 km/s. The 10.3 seconds of arc it travels annually amounts to a quarter of a degree in a human lifetime, roughly half the angular diameter of the full Moon.
The radial velocity of Barnard's Star towards the Sun can be measured by its blue shift. Two measurements are given in catalogues: 106.8 km/s in SIMBAD, which refers to a 1967 compilation of older measurements, and 110.8 km/s in ARICNS and similar values in all modern astronomical references. These measurements, combined with proper motion, suggest a true velocity relative to the Sun of 139.7 and 142.7 km/s, respectively. Barnard's Star will make its closest approach to the Sun around AD 11,800, when it approaches to within about 3.75 light-years. However, at that time, Barnard's Star will not be the nearest star, since Proxima Centauri will have moved even closer to the Sun. Barnard's Star will still be too dim to be seen with the naked eye at the time of its closest approach, since its apparent magnitude will be about 8.5 then. After that it will gradually recede from the Sun.
Barnard's Star has approximately 14% of a solar mass, and it has a radius 15% to 20% of that of the Sun. In 2003, its radius was estimated as 0.20±0.008 of the solar radius, at the high end of the ranges that were typically calculated in the past, indicating that previous estimates of the radius of Barnard's Star probably underestimated the actual value. Thus, although Barnard's Star has roughly 150 times the mass of Jupiter, its radius is only 1.5 to 2.0 times larger, reflecting the tendency of objects in the brown dwarf range to be about the same size. Its effective temperature is 3,134(±102) kelvin, and it has a visual luminosity just 4/10,000ths of solar luminosity, corresponding to a bolometric luminosity of 34.6/10,000ths. Barnard's Star is so faint that if it were at the same distance from Earth as the Sun is, it would appear only 100 times brighter than a full moon, comparable to the brightness of the Sun at 80 Astronomical Units.
In a broad survey of the metallicity of M-class dwarf stars, Barnard's Star's was placed between −0.5 and −1.0 on the metallicity scale, which is roughly 10 to 32% of the value for the Sun. Metallicity, the proportion of stellar mass made up of elements heavier than helium, helps classify stars relative to the galactic population. Barnard's Star seems to be typical of the old, red dwarf population II stars, yet these are also generally metal-poor halo stars. While sub-solar, Barnard's Star's metallicity is higher than a halo star and is in keeping with the low end of the metal-rich disk star range; this, plus its high space motion, have led to the designation "Intermediate Population II star", between a halo and disk star.
Claims of a planetary system.
For a decade from 1963 to about 1973, a substantial number of astronomers accepted a claim by Peter van de Kamp that he had detected, by using astrometry, a perturbation in the proper motion of Barnard's Star consistent with its having one or more planets comparable in mass with Jupiter. Van de Kamp had been observing the star from 1938, attempting, with colleagues at the Swarthmore College observatory, to find minuscule variations of one micrometre in its position on photographic plates consistent with orbital perturbations (wobbles) in the star that would indicate a planetary companion; this involved as many as ten people averaging their results in looking at plates, to avoid systemic, individual errors. Van de Kamp's initial suggestion was a planet having about 1.6 the Jovian mass at a distance of 4.4 AU in a slightly eccentric orbit, and these measurements were apparently refined in a 1969 paper. Later that year, Van de Kamp suggested that there were two planets of 1.1 and 0.8 Jovian masses.
Other astronomers subsequently repeated Van de Kamp's measurements, and two important papers in 1973 undermined the claim of a planet or planets. George Gatewood and Heinrich Eichhorn, at a different observatory and using newer plate measuring techniques, failed to verify the planetary companion. Another paper published by John L. Hershey four months earlier, also using the Swarthmore observatory, found that changes in the astrometric field of various stars correlated to the timing of adjustments and modifications that had been carried out on the refractor telescope's objective lens; the planetary "discovery" was an artifact of maintenance and upgrade work. The affair has been discussed as part of a broader scientific review.
Van de Kamp never acknowledged any error and published a further confirmation of two planets' existence as late as 1982; he died in 1995. Wulff Heintz, Van de Kamp's successor at Swarthmore and an expert on double stars, questioned his findings and began publishing criticisms from 1976 onwards. The two men were reported to have become estranged from each other because of this.
Refining planetary boundaries.
While not completely ruling out the possibility of planets, null results for planetary companions continued throughout the 1980s and 1990s, the latest based on interferometric work with the Hubble Space Telescope in 1999. By refining the values of a star's motion, the mass and orbital boundaries for possible planets are tightened: in this way astronomers are often able to describe what types of planets "cannot" orbit a given star.
M dwarfs such as Barnard's Star are more easily studied than larger stars in this regard because their lower masses render perturbations more obvious. Gatewood was thus able to show in 1995 that planets with 10 times the mass of Jupiter (the lower limit for brown dwarfs) were impossible around Barnard's Star, in a paper which helped refine the negative certainty regarding planetary objects in general. In 1999, work with the Hubble Space Telescope further excluded planetary companions of 0.8 times the mass of Jupiter with an orbital period of less than 1,000 days (Jupiter's orbital period is 4,332 days), while Kuerster determined in 2003 that within the habitable zone around Barnard's Star, planets are not possible with an ""M" sin "i"" value greater than 7.5 times the mass of the Earth, or with a mass greater than 3.1 times the mass of Neptune (much lower than van de Kamp's smallest suggested value).
Even though this research has greatly restricted the possible properties of planets around Barnard's Star, it has not ruled them out completely; terrestrial planets would be difficult to detect. NASA's Space Interferometry Mission, which was to begin searching for extrasolar Earth-like planets, was reported to have chosen Barnard's Star as an early search target. However, this mission was shut down in 2010. ESA's similar Darwin interferometry mission had the same goal, but was stripped of funding in 2007.
Project Daedalus.
Excepting the planet controversy, the best known study of Barnard's Star was part of Project Daedalus. Undertaken between 1973 and 1978, it suggested that rapid, unmanned travel to another star system is possible with existing or near-future technology. Barnard's Star was chosen as a target, partly because it was believed to have planets.
The theoretical model suggested that a nuclear pulse rocket employing nuclear fusion (specifically, electron bombardment of deuterium and helium-3) and accelerating for four years could achieve a velocity of 12% of the speed of light. The star could then be reached in 50 years, within a human lifetime. Along with detailed investigation of the star and any companions, the interstellar medium would be examined and baseline astrometric readings performed.
The initial Project Daedalus model sparked further theoretical research. In 1980, Robert Freitas suggested a more ambitious plan: a self-replicating spacecraft intended to search for and make contact with extraterrestrial life. Built and launched in Jovian orbit, it would reach Barnard's Star in 47 years under parameters similar to those of the original Project Daedalus. Once at the star, it would begin automated self-replication, constructing a factory, initially to manufacture exploratory probes and eventually to create a copy of the original spacecraft after 1,000 years.
The flare in 1998.
The observation of a stellar flare on Barnard's Star has added another element of interest to its study. Noted by William Cochran, University of Texas at Austin, based on changes in the spectral emissions on July 17, 1998 (during an unrelated search for planetary "wobbles"), it was four more years before the flare was fully analyzed. At that point Diane Paulson "et al.", now of Goddard Space Flight Center, suggested that the flare's temperature was 8000 K, more than twice the normal temperature of the star, although simply analyzing the spectra cannot precisely determine the flare's total output. Given the essentially random nature of flares, she noted "the star would be fantastic for amateurs to observe".
The flare was surprising because intense stellar activity is not expected around stars of such age. Flares are not completely understood, but are believed to be caused by strong magnetic fields which suppress plasma convection and lead to sudden outbursts: strong magnetic fields occur in rapidly rotating stars, while old stars tend to rotate slowly. An event of such magnitude around Barnard's Star is thus presumed to be a rarity. Research on the star's periodicity, or changes in stellar activity over a given timescale, also suggest it ought to be quiescent; 1998 research showed weak evidence for periodic variation in Barnard's Star's brightness, noting only one possible starspot over 130 days.
Stellar activity of this sort has created interest in using Barnard's Star as a proxy to understand similar stars. Photometric studies of its X-ray and UV emissions are hoped to shed light on the large population of old M dwarfs in the galaxy. Such research has astrobiological implications: given that the habitable zones of M dwarfs are close to the star, any planets would be strongly influenced by solar flares, winds, and plasma ejection events.
The star's neighborhood.
Barnard's Star shares much the same neighborhood as the Sun. The neighbors of Barnard's Star are generally of red dwarf size, the smallest and most common star type. Its closest neighbor is currently the red dwarf Ross 154, at 1.66 parsecs or 5.41 light years distance. The Sun and Alpha Centauri are, respectively, the next closest systems. From Barnard's Star, the Sun would appear on the diametrically opposite side of the sky at coordinates RA=, Dec=, in the eastern part of the constellation Monoceros. The absolute magnitude of the Sun is 4.83 and at a distance of 1.834 parsecs, it would be an impressively bright first-magnitude star, as Pollux is from the Earth.

</doc>
<doc id="4199" url="http://en.wikipedia.org/wiki?curid=4199" title="Bayer designation">
Bayer designation

A Bayer designation is a stellar designation in which a specific star is identified by a Greek letter, followed by the genitive form of its parent constellation's Latin name. The original list of Bayer designations contained 1,564 stars.
Most of the brighter stars were assigned their first systematic names by the German astronomer Johann Bayer in 1603, in his star atlas "Uranometria". Bayer assigned a lower-case Greek letter, such as alpha (α), beta (β), gamma (γ), etc., to each star he catalogued, combined with the Latin name of the star’s parent constellation in genitive (possessive) form. (See 88 modern constellations for the genitive forms.) For example, Aldebaran is designated "α Tauri" (pronounced "Alpha Tauri"), which means "Alpha of the constellation Taurus".
A single constellation may contain fifty or more stars, but the Greek alphabet has only twenty-four letters. When these ran out, Bayer began using Latin letters: upper case "A", followed by lower case "b" through "z" (omitting "j" and "v"), for a total of another 24 letters. Bayer never went beyond "z", but later astronomers added more designations using both upper and lower case Latin letters, the upper case letters following the lower case ones in general. Examples include "s Carinae" ("s" of the constellation Carina), "d Centauri" ("d" of the constellation Centaurus), "G Scorpii" ("G" of the constellation Scorpius), and "N Velorum" ("N" of the constellation Vela). The last upper-case letter used in this way was "Q".
Is Alpha always the brightest star?
For the most part, Bayer assigned Greek and Latin letters to stars in rough order of apparent brightness, from brightest to dimmest, within a particular constellation. Since in a majority of constellations the brightest star is designated Alpha (α), many people wrongly assume that Bayer meant to put the stars exclusively in order of their brightness, but in his day there was no way to measure stellar brightness precisely. Traditionally, the stars were assigned to one of six magnitude classes, and Bayer's catalog lists all the first-magnitude stars, followed by all the second-magnitude stars, and so on. Within each magnitude class, Bayer made no attempt to arrange stars by relative brightness.
Bayer did not always follow this rule; he sometimes assigned letters to stars according to their location within a constellation (for example: the northern, southern, eastern, or western part of a constellation), according to either the order in which they rise in the east, to historical or mythological information on specific stars within a constellation, or to his own arbitrary choosing.
Of the 88 modern constellations, there are at least 30 in which "Alpha" is not the brightest star, and four of those lack an alpha star altogether. (Constellations with no alpha include Vela and Puppis, both formerly part of Argo Navis whose alpha is Canopus in Carina.)
Bayer designations in Orion.
Orion provides a good example of Bayer's method. (The lower the magnitude, the brighter the star; additionally, there is a precise definition: a "2nd-magnitude" star ranks between 1.51 and 2.50, inclusive.) Bayer first designated the two 1st-magnitude stars, Betelgeuse and Rigel, as Alpha and Beta, with Betelgeuse (the shoulder) coming ahead of Rigel (the foot), even though the latter is usually the brighter. (Betelgeuse, a variable star, can at its maximum occasionally be brighter than Rigel.) He then repeated the procedure for the stars of the 2nd magnitude. As is evident from the map and chart, he again followed a "top-down" ("north-south") route.
Various Bayer designation arrangements.
This "First to Rise in the East" method is done in a number of other instances, even for Castor and Pollux of Gemini. Although Pollux is brighter than Castor, the latter was assigned "alpha" because it rises in the east ahead of the former. Bayer may also have assigned the stars Castor and Pollux in terms of historical or mythological knowledge. Both historically and mythologically, Castor's name is almost always mentioned first (Castor and Pollux) whenever the twins are mentioned, and that may have compelled him to assign "alpha" (α) to Castor and "beta" (β) to Pollux.
Although the brightest star in Draco is Eltanin (Gamma Draconis), Thuban was assigned "alpha" (α) by Bayer because Thuban, in history, was once the north pole star, 4,000 years ago. Almost every star with a history of being the North Star, including Vega, Alderamin, and Polaris, was designated as the "alpha" (α) of its parent constellation by Bayer.
Sometimes, indeed, there's no apparent order, as exemplified by the stars in Sagittarius, where Bayer's designations appear almost random to the modern eye. Alpha and Beta Sagittarii are perhaps the most anomalously designated stars in the sky; they are more than two magnitudes fainter than the brightest star (designated Epsilon), lie several degrees south of the main pattern (the "teapot" asterism), are more than 20 degrees off the ecliptic in a Zodiacal constellation, and do not even rise from Bayer's native Germany while Epsilon and several other brighter stars do. The order of the letters assigned in Sagittarius does correspond to the magnitudes as illustrated on Bayer's chart; but the latter don't agree with modern determinations of the magnitudes.
Bayer designations added by later astronomers generally were ordered by magnitude, but care was usually taken to avoid conflict with designations already assigned. In Libra, for example, the new designations sigma, tau, and upsilon were chosen to avoid conflict with Bayer's earlier designations, even though several stars with earlier letters are not as bright.
Bayer's miscellaneous labels.
Although Bayer did not use upper-case Latin letters (except "A") for "fixed stars", he did use them to label other items shown on his charts, such as neighboring constellations, miscellaneous astronomical objects, or reference lines like the Tropic of Cancer. In Cygnus, for example, Bayer's fixed stars run through "g", and on this chart Bayer employs "H" through "P" as miscellaneous labels, mostly for neighboring constellations. Bayer did not intend such labels as catalog designations, but some have survived to refer to astronomical objects: P Cygni for example is still used as a designation for Nova Cyg 1600. In charts for constellations that did not exhaust the Greek letters, Bayer sometimes used the left-over Greek letters for miscellaneous labels as well.
Revised Bayer designations.
Ptolemy designated three stars as "border stars", each shared by two constellations; and Bayer assigned these a Greek letter from both constellations: , , and . When the International Astronomical Union (IAU) outlined the official 88 constellations with definite boundaries in 1930, it declared that stars and other celestial objects can be assigned to only one constellation. Consequently, the redundant Bayer designations for those three stars were scrapped in favor of Beta Tauri, Alpha Andromedae, and Nu Boötis. 
Other cases of multiple Bayer designations arose when stars named by Bayer in one constellation were transferred to a different constellation. Bayer's Gamma and Omicron Scorpii, for example, were later reassigned from Scorpius to Libra and given the new names Sigma and Upsilon Librae. (To add to the confusion, the star now known as Omicron Scorpii was not named by Bayer but was assigned the designation o Scorpii (Latin lower case 'o') by Lacaille – which later astronomers misinterpreted as omicron once Bayer's omicron had been reassigned to Libra.) 
A few stars no longer lie (according to the modern constellation boundaries) within the constellation for which they are named. The proper motion of Rho Aquilae, for example, recently carried it across the boundary into Delphinus. Nonetheless, these designations have proved useful and are widely used today.
Bayer designation styles.
Greek letters can be used together with the standard 3-letter abbreviation of the constellation, as in α CMa or β Per. Or the two can be combined (α Canis Majoris). Earlier 4-letter abbreviations (such as α CMaj) are rarely used today.
Bayer designations are sometimes written out out in full, as in Alpha Canis Majoris or Beta Persei.
Other Bayer designations.
The Latin-letter extended designations are not as commonly used as the Greek-letter ones, but there are some exceptions such as h Persei (which is actually a star cluster) and P Cygni. Uppercase Latin Bayer designations in modern use do not go beyond Q; names such as R Leporis and W Ursae Majoris are variable star designations, not Bayer designations.
A further complication is the use of numeric superscripts to distinguish neighboring stars that Bayer (or a later astronomer) labeled with a common letter. Usually these are double stars (mostly optical doubles rather than true binary stars), but there are some exceptions such as the chain of stars π1, π2, π3, π4, π5 and π6 Orionis.

</doc>
<doc id="4200" url="http://en.wikipedia.org/wiki?curid=4200" title="Boötes">
Boötes

Boötes is a constellation in the northern sky, located between 0° and +60° declination, and 13 and 16 hours of right ascension on the celestial sphere. The name comes from the Greek Βοώτης, "Boōtēs", meaning herdsman or plowman (literally, ox-driver; from "boos", related to the Latin "bovis", “cow”). The "ö" in the name is a diaeresis, not an umlaut, meaning that each 'o' is to be pronounced separately.
Boötes was one of the 48 constellations described by the 2nd century astronomer Ptolemy and is now one of the 88 modern constellations. It contains the fourth brightest star in the night sky, Arcturus. Boötes is home to many other bright stars, including eight above the fourth magnitude and an additional 21 above the fifth magnitude, making a total of 29 stars easily visible to the naked eye.
History and mythology.
In ancient Babylon the stars of Boötes were known as SHU.PA. They were apparently depicted as the god Enlil, who was the leader of the Babylonian pantheon and special patron of farmers.
The name "Boötes" was first used by Homer in his Odyssey as a celestial reference point for navigation, described as "late-setting" or "slow to set", translated as the "Plowman". Exactly whom Boötes is supposed to represent in Greek mythology is not clear. According to one version, he was a son of Demeter, Philomenus, twin brother of Plutus, a ploughman who drove the oxen in the constellation Ursa Major. This is corroborated by the constellation's name, which itself means "ox-driver" or "herdsman." The ancient Greeks saw the asterism now called the "Big Dipper" or "Plough" as a cart with oxen. This influenced the name's etymology, derived from the Greek for "noisy" or "ox-driver". Another myth associated with Boötes tells that he invented the plow and was memorialized for his ingenuity as a constellation.
Another myth associated with Boötes by Hyginus is that of Icarius, who was schooled as a grape farmer and winemaker by Dionysus. Icarius made wine so strong that those who drank it appeared poisoned, which caused shepherds to avenge their supposedly poisoned friends by killing Icarius. Maera, Icarius's dog, brought his daughter Erigone to her father's body, whereupon both she and the dog committed suicide. Zeus then chose to honor all three by placing them in the sky as constellations: Icarius as Boötes, Erigone as Virgo, and Maera as Canis Major or Canis Minor.
Following another reading, the constellation is identified with Arcas and also referred to as Arcas and Arcturus, son of Zeus and Callisto. Arcas was brought up by his maternal grandfather Lycaon, to whom one day Zeus went and had a meal. To verify that the guest was really the king of the gods, Lycaon killed his grandson and prepared a meal made from his flesh. Zeus noticed and became very angry, transforming Lycaon into a wolf and gave back life to his son. In the meantime Callisto had been transformed into a she-bear, by Zeus' wife, Hera, who was angry at Zeus' infidelity. This is corroborated by the Greek name for Boötes, "Arctophylax", which means "Bear Watcher". Callisto in form of a bear was almost killed by her son who was out hunting. Zeus rescued her, taking her into the sky where she became Ursa Major, "the Great Bear". The name Arcturus (the constellation's brightest star) comes from the Greek word meaning "guardian of the bear". Sometimes Arcturus is depicted as leading the hunting dogs of nearby Canes Venatici and driving the bears of Ursa Major and Ursa Minor.
Several former constellations were formed from stars now included in Boötes. Quadrans Muralis, the Quadrant, was a constellation created near Beta Boötis from faint stars. It was invented in 1795 by Jérôme Lalande, an astronomer who used a quadrant to perform detailed astronometric measurements. Lalande worked with Nicole-Reine Lepaute and others to predict the 1758 return of Halley's Comet. Quadrans Muralis was formed from the stars of eastern Boötes, western Hercules, and Draco. It was originally called "Le Mural" by Jean Fortin in his 1795 Atlas Céleste; it was not given the name "Quadrans Muralis" until Johann Bode's 1801 "Uranographia". The constellation was quite faint, with its brightest stars reaching the 5th magnitude. Mons Maenalus, representing the Maenalus mountains, was created by Johannes Hevelius in 1687 at the foot of the constellation's figure. The mountain was named for the son of Lycaon, Maenalus. The mountain, one of Diana's hunting grounds, was also holy to Pan.
Non-Western astronomy.
The stars of Boötes were incorporated into many different Chinese constellations. Arcturus was part of the most prominent of these, variously designated as the celestial king's throne ("Tian Wang") or the Blue Dragon's horn ("Daijiao"); the name "Daijiao", meaning "great horn", is more common. Arcturus was given such importance in Chinese celestial mythology because of its status marking the beginning of the lunar calendar, as well as its status as the brightest star in the northern night sky. Two constellations flanked "Daijiao", "Yousheti" to the right and "Zuosheti" to the left; they represented companions that orchestrated the seasons. "Zuosheti" was formed from modern Zeta, Omicron, and Pi Boötis, while "Yousheti" was formed from modern Eta, Tau, and Upsilon Boötis. "Dixi", the Emperor's ceremonial banquet mat, was north of Arcturus, consisting of the stars 12, 11, and 9 Boötis. Another northern constellation was "Qigong", the Seven Dukes, which was mostly across the Boötes-Hercules border. It included either Delta Boötis or Beta Boötis as its terminus.
The other Chinese constellations made up of the stars of Boötes existed in the modern constellation's north; they are all representations of weapons. "Tianqiang", the spear, was formed from Iota, Kappa, and Theta Boötis; "Genghe", variously representing a lance or shield, was formed from Epsilon, Rho, and Sigma Boötis. There were also two weapons made up of a singular star. "Xuange", the halberd, was represented by Lambda Boötis, and "Zhaoyao", either the sword or the spear, was represented by Gamma Boötis.
Two Chinese constellations have an uncertain placement in Boötes. "Kangchi", the lake, was placed south of Arcturus, though its specific location is disputed. It may have been placed entirely in Boötes, on either side of the Boötes-Virgo border, or on either side of the Virgo-Libra border. The constellation "Zhouding", a bronze tripod-mounted container used for food, was sometimes cited as the stars 1, 2, and 6 Boötis. However, it has also been associated with three stars in Coma Berenices.
Antares had several names that described its significance to indigenous Polynesians. In the Society Islands, Arcturus, called "Ana-tahua-taata-metua-te-tupu-mavae" ("a pillar to stand by"), was one of the ten "pillars of the sky", bright stars that represented the ten heavens of the Tahitian afterlife. In Hawaii, the pattern of Boötes was called "Hoku-iwa", meaning "stars of the frigate bird". This constellation marked the path for Hawaiiloa on his return to Hawaii from the South Pacific Ocean. The Hawaiians called Arcturus "Hoku-lea". It was equated to the Tuamotuan constellation "Te Kiva", meaning "frigate-bird", which could either represent the figure of Boötes or just Arcturus. However, Arcturus may instead be the Tuamotuan star called "Turu". The Hawaiian name for Arcturus as a single star was likely "Hoku-lea", which means "star of gladness". In the Marquesas Islands, Arcturus was probably called "Tau-tou" and was the star that ruled the month approximating January. The Maori and Moriori called it "Tautoru", a variant of the Marquesan name and a name shared with Orion's Belt.
Characteristics.
Boötes is a constellation bordered by Virgo to the south, Coma Berenices and Canes Venatici to the west, Ursa Major to the northwest, Draco to the northeast, and Hercules, Corona Borealis and Serpens Caput to the east. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is 'Boo'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of 16 segments. In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates stretch from +7.36° to +55.1°. Covering 907 square degrees, Boötes culminates at midnight around 2 May and ranks 13th in area.
Colloquially, its pattern of stars has been likened to a kite or ice cream cone. However, depictions of Boötes have varied historically. Aratus described him circling the north pole, herding the two bears. Later ancient Greek depictions, described by Ptolemy, have him holding the reins of his hunting dogs (Canes Venatici) in his left hand, with a spear, club, or staff in his right hand. After Hevelius introduced Mons Maenalus in 1681, Boötes was often depicted standing on the Peloponnese mountain. By 1801, when Johann Bode published his "Uranographia", Boötes had acquired a sickle, which was also held in his left hand.
The placement of Arcturus has also been mutable through the centuries. Traditionally, Arcturus lay between his thighs, as Ptolemy depicted him. However, Germanicus Caesar deviated from this tradition by placing Arcturus "where his garment is fastened by a knot".
Notable features.
Stars.
In his "Uranometria", Johann Bayer used the Greek letters Alpha through to Omega and then A to k to label what he saw as the most prominent 35 stars in the constellation, with subsequent astronomers splitting Kappa, Mu, Nu and Pi as two stars each. Nu is also the same star as Psi Herculis. John Flamsteed numbered 54 stars for the constellation.
Arcturus, or Alpha Boötis, is the brightest star in Boötes and the fourth brightest star in the sky at an apparent magnitude of −0.04; it has an absolute magnitude of −0.2. It is also the brightest star north of the celestial equator, barely brighter than Vega and Capella. Its name comes from the Greek for "bear-keeper". An orange giant with color visible from Earth at a distance of 37 light-years, its diameter is 27 solar diameters, equivalent to approximately 32 million kilometers, though its mass is approximately one solar mass. Its luminosity is 115 L☉ and its spectral class is K2. Bayer located Arcturus above the herdsman's left knee in his "Uranometria".
Marking the herdsman's head is Beta Boötis, or Nekkar, a yellow giant of magnitude 3.5. Its common name comes from the Arabic phrase for "ox-driver". It is 219 light-years away and has a luminosity of 58 L☉. Its absolute magnitude is 0.3 and its spectral class is G8. Eta Boötis or Muphrid is a 2.68 magnitude star that is 37 light-years away with a spectral class of G0. It has a luminosity of 6.5 L☉. It is the uppermost star denoting the herdsman's left leg. Muphrid and Arcturus lie only 3.3 light years away from each other. Viewed from Arcturus, Muphrid would have a visual magnitude of -2½, while Arcturus would be around visual magnitude -4½ when seen from Muphrid. Gamma Boötis, or Seginus, is a white star of magnitude 3.03. It is 85 light-years away. Its spectral class is A7, and it has a luminosity of 53 L☉. Rho and Sigma Boötis denote the herdsman's waist. With a visual magnitude of 3.58, Rho has a luminosity of 105 L☉ and is 183 light-years from Earth. Sigma is of magnitude 4.46, with a spectral class of F2 and an absolute magnitude of 3.0. The magnitude 4.05 Theta Boötis has a spectral type of F7 and an absolute magnitude of 3.8. There are two dimmer F-type stars, magnitude 4.83 12 Boötis, class F8; and magnitude 4.93 45 Boötis, class F5. Xi Boötis is a G8 yellow dwarf of magnitude 4.55, and absolute magnitude is 5.5. Two dimmer G-type stars are magnitude 4.86 31 Boötis, class G8, and magnitude 4.76 44 Boötis, class G0.
Of apparent magnitude 4.06, Upsilon Boötis has a spectral class of K5 and an absolute magnitude of −0.3. Dimmer than Upsilon Boötis is magnitude 4.54 Phi Boötis, with a spectral class of K2 and an absolute magnitude of −0.1. Just slightly dimmer than Phi at magnitude 4.60 is O Boötis, which, like Izar, has a spectral class of K0. O Boötis has an absolute magnitude of 0.2. The other four dim stars are magnitude 4.91 6 Boötis, class K4; magnitude 4.86 20 Boötis, class K3; magnitude 4.81 Omega Boötis, class K4; and magnitude 4.83 A Boötis, class K1.
There is one bright B-class star in Boötes; magnitude 4.93 Pi1 Boötis, also called Alazal. It has a spectral class of B9 and is 40 parsecs from Earth. There is also one M-type star, magnitude 4.81 34 Boötis. It is of class gM0.
Multiple stars.
Epsilon Boötis, also known as Izar or Pulcherrima, is a close triple star popular with amateur astronomers and the most prominent binary star in Boötes. The primary is a yellow- or orange-hued magnitude 2.5 giant star, the secondary is a magnitude 4.6 blue-hued main-sequence star, and the tertiary is a magnitude 12.0 star. The system is 210 light-years away. The name "Izar" comes from the Arabic word for "girdle" or "loincloth", referring to its location in the constellation. The name "Pulcherrima" comes from the Latin phrase for "most beautiful", referring to its contrasting colors in a telescope. The primary and secondary stars are separated by 2.9 arcseconds at an angle of 341 degrees; the primary's spectral class is K0 and it has a luminosity of 200 L☉. To the naked eye, Izar has a magnitude of 2.37.
Mu Boötis, known as Alkalurops, is a triple star popular with amateur astronomers. It has an overall magnitude of 4.3 and is 121 light-years away. Its name is from the Arabic phrase for "club" or "staff". The primary appears to be of magnitude 4.3 and is blue-white. The secondary appears to be of magnitude 6.5, but is actually a close double star itself with a primary of magnitude 7.0 and a secondary of magnitude 7.6. The secondary and tertiary stars have an orbital period of 260 years. The primary has an absolute magnitude of 2.6 and is of spectral class F0. The secondary and tertiary stars are separated by 2 arcseconds; the primary and secondary are separated by 109.1 arcseconds at an angle of 171 degrees.
Besides Pulcherrima and Alkalurops, there are several other binary stars in Boötes:
Variable stars.
Boötes possesses several variable stars. T Boötis was a nova observed in April 1860 at a magnitude of 9.7. It has never been observed since, but that does not preclude the possibility of it being a highly irregular variable star or a recurrent nova.
Two of the brighter Mira-type variables stars in the constellations are R Boötis and S Boötis. R Boötis is an M-type star that ranges in magnitude from a minimum of 13.1 to a maximum of 6.2 over a period of 223.4 days. S Boötis is another M-type Mira variable that ranges in magnitude from a minimum of 13.8 to a maximum of 7.8 over a period of 270.7 days.
44 Boötis (i Boötis) is a double variable star 42 light-years away. It has an overall magnitude of 4.8 and appears yellow to the naked eye. The primary is of magnitude 5.3 and the secondary is of magnitude 6.1; their orbital period is 220 years. The secondary is itself an eclipsing variable star with a range of 0.6 magnitudes; its orbital period is 6.4 hours. It is a W Ursae Majoris variable that ranges in magnitude from a minimum of 7.1 to a maximum of 6.5 every 0.27 days. Both stars are G-type stars. Another W UMa-type star is ZZ Boötis, which has two G2-type components and ranges in magnitude from a minimum of 6.4 to a maximum of 5.8. It has a period of 5.0 days.
Boötes has two semi-regular variable stars. V Boötis is an M-type star that ranges in magnitude from a minimum of 12.0 to a maximum of 7.0, over a period of 258 days. W Boötis is a brighter M-type star; it has a minimum magnitude of 5.4 and a maximum magnitude of 4.7, with a period of 450 days.
BL Boötis is the prototype of its class of pulsating variable stars, the anomalous Cepheids. These stars are somewhat similar to Cepheid variables, but they do not have the same relationship between their period and luminosity. Their periods are similar to RRAB variables; however, they are far brighter than these stars. BL Boötis is a member of the cluster NGC 5466. Anomalous Cepheids are metal poor and have masses not much larger than the Sun's, on average, 1.5 solar masses. BL Boötis type stars are a subtype of RR Lyrae variables.
An apparent magnitude 4.18 type A0p star, Lambda Boötis is the prototype of a class of chemically peculiar stars, only some of which pulsate as Delta Scuti type stars. The distinction between the Lambda Boötis stars as a class of stars with peculiar spectra, and the delta Scuti stars whose class describes pulsation in low-overtone pressure modes, is an important one. While many Lambda Boötis stars pulsate and are delta Scuti stars, not many delta Scuti stars have Lambda Boötis peculiarities, since the Lambda Boötis stars are a much rarer class whose members can be found both inside and outside the delta Scuti instability strip. Lambda Boötis stars are dwarf stars that can be either spectral class A or F. Like BL Boötis-type stars they are metal-poor. Scientists have had difficulty explaining the characteristics of Lambda Boötis stars, partly because only around 60 confirmed members exist, but also due to heterogeneity in the literature. Lambda has an absolute magnitude of 1.8.
Stars with planetary systems.
Extrasolar planets have been discovered encircling ten stars in Boötes as of 2012. Tau Boötis is orbited by a large planet, discovered in 1999. The host star itself is a magnitude 4.5 star of type F7V, 15.6 parsecs from Earth. It has a mass of 1.3 solar masses and a radius of 1.331 solar radii; a companion, GJ527B, orbits at a distance of 240 AU. Tau Boötis b, the sole planet discovered in the system, orbits at a distance of 0.046 AU every 3.31 days. Discovered through radial velocity measurements, it has a mass of 5.95 Jupiter masses. This makes it a hot Jupiter. The host star and planet are tidally locked, meaning that the planet's orbit and the star's particularly high rotation are synchronized. Furthermore, a slight variability in the host star's light may be caused by magnetic interactions with the planet. Carbon monoxide is present in the planet's atmosphere. Tau Boötis b does not transit its star, rather, its orbit is inclined 46 degrees. Like Tau Boötis b, HAT-P-4 b is also a hot Jupiter. It is noted for orbiting a particularly metal-rich host star and being of low density. Discovered in 2007, HAT-P-4 b has a mass of 0.68 Jupiter masses and a radius of 1.27 Jupiter radii. It orbits every 3.05 days at a distance of 0.04 AU. HAT-P-4, the host star, is an F-type star of magnitude 11.2, 310 parsecs from Earth. It is larger than the Sun, with a mass of 1.26 solar masses and a radius of 1.59 solar radii.
Boötes is also home to multiple-planet systems. HD 128311 is the host star for a two-planet system, consisting of HD 128311 b and HD 128311 c, discovered in 2002 and 2005, respectively. HD 128311 b is the smaller planet, with a mass of 2.18 Jupiter masses; it was discovered through radial velocity observations. It orbits at almost the same distance as Earth, at 1.099 AU; however, its orbital period is significantly longer at 448.6 days. The larger of the two, HD 128311 c, has a mass of 3.21 Jupiter masses and was discovered in the same manner. It orbits every 919 days inclined at 50°, and is 1.76 AU from the host star. The host star, HD 128311, is a K0V-type star located 16.6 parsecs from Earth. It is smaller than the Sun, with a mass of 0.84 solar masses and a radius of 0.73 solar radii; it also appears below the threshold of naked-eye visibility at an apparent magnitude of 7.51.
There are several single-planet systems in Boötes. HD 132406 is a sun-like star of spectral type G0V with an apparent magnitude of 8.45, 231.5 light-years from Earth. It has a mass of 1.09 solar masses and a radius of 1 solar radius. The star is orbited by a gas giant, HD 132406 b, discovered in 2007. HD 132406 orbits 1.98 AU from its host star with a period of 974 days and has a mass of 5.61 Jupiter masses. The planet was discovered by the radial velocity method. WASP-23 is a star with one orbiting planet, WASP-23 b. The planet, discovered by the transit method in 2010, orbits every 2.944 very close to its sun, at 0.0376 AU. It is smaller than Jupiter, at 0.884 Jupiter masses and 0.962 Jupiter radii. Its star is a K1V type star of apparent magnitude 12.7, far below naked-eye visibility, and smaller than the Sun at 0.78 solar masses and 0.765 solar radii. HD 131496 is also encircled by one planet, HD 131496 b. The star is of type K0 and is located 110 parsecs from Earth; it appears at a visual magnitude of 7.96. It is significantly larger than the Sun, with a mass of 1.61 solar masses and a radius of 4.6 solar radii. Its one planet, discovered in 2011 by the radial velocity method, has a mass of 2.2 Jupiter masses; its radius is as yet undetermined. HD 131496 b orbits at a distance of 2.09 AU with a period of 883 days.
Another single planetary system in Boötes is the HD 132563 system, a triple star system. The parent star, technically HD 132563B, is a star of magnitude 9.47, 96 parsecs from Earth. It is almost exactly the size of the sun, with the same radius and a mass only 1% greater. Its planet, HD 132563B b, was discovered in 2011 by the radial velocity method. 1.49 times the mass of Jupiter, it orbits 2.62 AU from its star with a period of 1544 days. Its orbit is somewhat elliptical, with an eccentricity of 0.22. HD 132563B b is one of very few planets found in triple star systems; it orbits the isolated member of the system, which is separated from the other components, a spectroscopic binary, by 400 AU. Also discovered through the radial velocity method, albeit a year earlier, is HD 136418 b, a 2-Jupiter mass planet that orbits the star HD 136418 at a distance of 1.32 AU with a period of 464.3 days. Its host star is a magnitude 7.88 G5-type star, 98.2 parsecs from Earth. It has a radius of 3.4 solar radii and a mass of 1.33 solar masses.
WASP-14 b is one of the most massive and dense exoplanets known, with a mass of 7.341 Jupiter masses and a radius of 1.281 Jupiter radii. Discovered via the transit method, it orbits 0.036 AU from its host star with a period of 2.24 days. WASP-14 b has a density of 4.6 grams per cubic centimeter, making it one of the densest exoplanets known. Its host star, WASP-14, is an F5V-type star of magnitude 9.75, 160 parsecs from Earth. It has a radius of 1.306 solar radii and a mass of 1.211 solar masses. It also has a very high proportion of lithium.
Deep-sky objects.
Boötes is in a part of the celestial sphere facing away from the plane of our home Milky Way galaxy, and so does not have open clusters or nebulae. Instead, it has one bright globular cluster and many faint galaxies. The globular cluster NGC 5466 has an overall magnitude of 9.1 and a diameter of 11 arcminutes. It is a very loose globular cluster with fairly few stars and may appear as a rich, concentrated open cluster in a telescope. NGC 5466 is classified as a Shapley-Sawyer Concentration Class 12 cluster, reflecting its sparsity. Its fairly large diameter means that it has a low surface brightness, so it appears far dimmer than the catalogued magnitude of 9.1 and requires a large amateur telescope to view. Only approximately 12 stars are resolved by an amateur instrument.
Boötes has two bright galaxies. NGC 5248 (Caldwell 45) is a type Sc galaxy (a variety of spiral galaxy) of magnitude 10.2. It measures 6.5 by 4.9 arcminutes. 50 million light-years from Earth, NGC 5248 is a member of the Virgo Cluster of galaxies; it has dim outer arms and obvious H II regions, dust lanes, and young star clusters. NGC 5676 is another type Sc galaxy of magnitude 10.9. It measures 3.9 by 2.0 arcminutes. Other galaxies include NGC 5008, a type Sc emission-line galaxy, NGC 5548, a type S Seyfert galaxy, NGC 5653, a type S HII galaxy, NGC 5778 (also classified as NGC 5825), a type E galaxy that is the brightest of its cluster, NGC 5886, and NGC 5888, a type SBb galaxy. NGC 5698 is a barred spiral galaxy, notable for being the host of the 2005 supernova SN 2005bc, which peaked at magnitude 15.3.
Much further away lies the 250 million light-year diameter Boötes void, a huge space largely empty of galaxies. Its centre is roughly 700 million light years from Earth. Beyond it and within the bounds of the constellation, lie two superclusters at around 830 million and 1 billion light years distant.
Meteor showers.
Boötes is home to the Quadrantid meteor shower, the most prolific annual meteor shower. It was discovered in January 1835 and named in 1864 by Alexander Hershell. The radiant is located in northern Boötes near Kappa Boötis, in its namesake former constellation of Quadrans Muralis. Quadrantid meteors are dim, but have a peak visible hourly rate of approximately 100 per hour on January 3–4. The zenithal hourly rate of the Quadrantids is approximately 130 meteors per hour at their peak; it is also a very narrow shower. The Quadrantids are notoriously difficult to observe because of a low radiant and often inclement weather. The parent body of the meteor shower has been disputed for decades; however, Peter Jenniskens has proposed 2003 EH1, a minor planet, as the parent. 2003 EH1 may be linked to C/1490 Y1, a comet previously thought to be a potential parent body for the Quadrantids. 2003 EH1 is a short-period comet of the Jupiter family; 500 years ago, it experienced a catastrophic breakup event. It is now dormant. The Quadrantids had notable displays in 1982, 1985, and 2004. Meteors from this shower often appear to have a blue hue and travel at a moderate speed of 41.5–43 kilometers per second.
On April 28, 1984, a remarkable outburst of the normally placid Alpha Bootids was observed by visual observer Frank Witte from 00:00 to 2:30 UTC. In a 6 cm telescope, he observed 433 meteors in a field of view near Arcturus with a diameter of less than 1°. Peter Jenniskens comments that this outburst resembled a "typical dust trail crossing". The Alpha Bootids normally begin on April 14, peaking on April 27th and 28th, and finishing on May 12. Its meteors are slow-moving, with a velocity of 20.9 kilometers per second. They may be related to Comet 73P/Schwassmann-Wachmann 3, but this connection is only theorized.
The June Bootids, also known as the Iota Draconids, is a meteor shower associated with the comet 7P/Pons-Winnecke, first recognized on May 27, 1916 by William F. Denning. The shower, with its slow meteors, was not observed prior to 1916 because Earth did not cross the comet's dust trail until Jupiter perturbed Pons-Winnecke's orbit, causing it to come within 0.03 AU of Earth's orbit the first year the June Bootids were observed. In 1982, E. A. Reznikov discovered that the 1916 outburst was caused by material released from the comet in 1819. Another outburst of the June Bootids was not observed until 1998, because Comet Pons-Winnecke's orbit was not in a favorable position. However, on June 27, 1998, an outburst of meteors radiating from Boötes, later confirmed to be associated with Pons-Winnecke, was observed. They were incredibly long-lived, with trails of the brightest meteors lasting several seconds at times. Many fireballs, green-hued trails, and even some meteors that cast shadows were observed throughout the outburst, which had a maximum zenithal hourly rate of 200–300 meteors per hour. In 2002, two Russian astronomers determined that material ejected from the comet in 1825 was responsible for the 1998 outburst. Ejecta from the comet dating to 1819, 1825, and 1830 was predicted to enter Earth's atmosphere on June 23, 2004. The predictions of a shower less spectacular than the 1998 showing were borne out in a display that had a maximum zenithal hourly rate of 16–20 meteors per hour that night. The June Bootids are not expected to have another outburst in the next 50 years. Typically, only 1–2 dim, very slow meteors are visible per hour; the average June Bootid has a magnitude of 5.0. It is related to the Alpha Draconids and the Bootids-Draconids. The shower lasts from June 27 to July 5, with a peak on the night of June 28. The June Bootids are classified as a class III shower (variable), and has an average entry velocity of 18 kilometers per second. Its radiant is located 7 degrees north of Beta Boötis.
The Beta Bootids is a weak shower that begins on January 5, peaks on January 16, and ends on January 18. Its meteors travel at 43 kilometers/second. The January Bootids is a short, young meteor shower that begins on January 9, peaks from January 16 to January 18, and ends on January 18th. The Phi Bootids is another weak shower radiating from Boötes. It begins on April 16, peaks on April 30 and May 1, and ends on May 12. Its meteors are slow-moving, with a velocity of 15.1 km/s. They were discovered in 2006. The shower's peak hourly rate can be as high as 6 meteors per hour. Though named for a star in Boötes, the Phi Bootid radiant has moved into Hercules. The meteor stream is associated with three different asteroids: 1620 Geographos, 2062 Aten, and 1978 CA. The Lambda Bootids, part of the Bootid-Coronae Borealid Complex, are a weak annual shower with moderately fast meteors; 41.75 km/s. The complex includes the Lambda Bootids, as well as the Theta Coronae Borealids and Xi Coronae Borealids. All of the Bootid-Coronae Borealid showers are Jupiter family comet showers; the streams in the complex have highly inclined orbits.
There are several minor showers in Boötes, some of whose existence is yet to be verified. The Rho Bootids radiate from near the namesake star, and were hypothesized in 2010. The average Rho Bootid has an entry velocity of 43 km/s. It peaks in November and lasts for 3 days. The Rho Bootid shower is part of the SMA complex, a group of meteor showers related to the Taurids, which is in turn linked to the comet 2P/Encke. However, the link to the Taurid shower remains unconfirmed and may be a chance correlation. Another such shower is the Gamma Bootids, which were hypothesized in 2006. Gamma Bootids have an entry velocity of 50.3 km/s. The Nu Bootids, hypothesized in 2012, have faster meteors, with an entry velocity of 62.8 km/s.
References.
Citations
References

</doc>
<doc id="4203" url="http://en.wikipedia.org/wiki?curid=4203" title="Bernardino Ochino">
Bernardino Ochino

Bernardino Ochino (1487–1564) was an Italian, who was raised a Roman Catholic and later turned to Protestantism.
Biography.
Bernardino Ochino was born in Siena, the son of the barber Domenico Ochino, and at the age of 7 or 8, in around 1504, was entrusted to the order of Franciscan Friars. From 1510 he studied medicine at Perugia.
Transfer to the Capuchins.
At the age of 38, Ochino transferred himself in 1534 to the newly founded Order of Friars Minor Capuchin. By then he was the close friend of Juan de Valdés, Pietro Bembo, Vittoria Colonna, Pietro Martire, Carnesecchi. In 1538 he was elected vicar-general of his order. In 1539, urged by Bembo, he visited Venice and delivered a course of sermons showing a sympathy with justification by faith, which appeared more clearly in his "Dialogues" published the same year. He was suspected and denounced, but nothing ensued until the establishment of the Inquisition in Rome in June 1542, at the instigation of Cardinal Giovanni Pietro Carafa. Ochino received a citation to Rome, and set out to obey it about the middle of August. According to his own statement, he was deterred from presenting himself at Rome by the warnings of Cardinal Contarini, whom he found at Bologna, dying of poison administered by the reactionary party.
Escape to Geneva.
Ochino turned aside to Florence, and after some hesitation went across the Alps to Geneva. He was cordially received by John Calvin, and published within two years several volumes of "Prediche", controversial tracts rationalizing his change of religion. He also addressed replies to marchioness Vittoria Colonna, Claudio Tolomei, and other Italian sympathizers who were reluctant to go to the same length as himself. His own breach with the Roman Catholic Church was final.
Augsburg and England.
In 1545 Ochino became minister of the Italian Protestant congregation at Augsburg. From this time dates his contact with Caspar Schwenckfeld. He was compelled to flee when, in January 1547, the city was occupied by the imperial forces for the Diet of Augsburg.
Ochino found asylum in England, where he was made a prebendary of Canterbury Cathedral, received a pension from Edward VI's privy purse, and composed his major work, the "Tragoedie or Dialoge of the unjuste usurped primacie of the Bishop of Rome". This text, originally written in Latin, is extant only in the 1549 translation of Bishop John Ponet. The form is a series of dialogues. Lucifer, enraged at the spread of Jesus's kingdom, convokes the fiends in council, and resolves to set up the pope as antichrist. The state, represented by the emperor Phocas, is persuaded to connive at the pope's assumption of spiritual authority; the other churches are intimidated into acquiescence; Lucifer's projects seem fully accomplished, when Heaven raises up Henry VIII of England and his son for their overthrow.
Several of Ochino's "Prediche" were translated into English by Anna Cooke; and he published numerous controversial treatises on the Continent.
Zürich.
In 1553 the accession of Mary I drove Ochino from England. He went to Basel, where Lelio Sozzini and the lawyer Martino Muralto were sent to secure Ochino as pastor of the Italian church at Zürich, which Ochino accepted. The Italian congregation there was composed mainly of refugees from Locarno. There for 10 years Ochino wrote books which gave increasing evidence of his alienation from the orthodoxy around him. The most important of these was the "Labyrinth", a discussion of the freedom of the will, covertly undermining the Calvinistic doctrine of predestination.
In 1563 a long simmering storm burst on Ochino with the publication of his "Thirty Dialogues", in one of which his adversaries maintained that he had justified polygamy under the disguise of a pretended refutation. His dialogues on divorce and against the Trinity were also considered heretical.
Poland, and death.
Ochino was not given opportunity to defend himself, and was banished from Zürich. After being refused admission by other Protestant cities, he directed his steps towards Poland, at that time the most tolerant state in Europe. He had not resided there long when an edict appeared (August 8, 1564) banishing all foreign dissidents. Fleeing the country, he encountered the plague at Pińczów; three of his four children were carried off; and he himself, worn out by misfortune, died in solitude and obscurity at Slavkov in Moravia, about the end of 1564.
Legacy.
Ochino's reputation among Protestants was low. He was charged by Thomas Browne in 1643 with the authorship of the legendary-apocryphal heretical treatise "De tribus Impostoribus", as well as with having carried his alleged approval of polygamy into practice.
His biographer Karl Benrath justified him, representing him as a fervent evangelist and at the same time as a speculative thinker with a passion for free inquiry. The picture is of Ochino always learning and unlearning and arguing out difficult questions with himself in his dialogues, frequently without attaining to any absolute conviction. 

</doc>
<doc id="4204" url="http://en.wikipedia.org/wiki?curid=4204" title="Bay of Quinte">
Bay of Quinte

The Bay of Quinte is a long, narrow bay shaped like the letter "Z" on the northern shore of Lake Ontario in the province of Ontario, Canada. It is just west of the head of the Saint Lawrence River that drains the Great Lakes into the Gulf of Saint Lawrence. It is located about east of Toronto and west of Montreal.
The name "Quinte" is derived from ""Kente"", which was the name of an early French Catholic mission located on the south shore of what is now Prince Edward County. Officially, in the Mohawk language, the community is called "Kenhtè:ke" which means "the place of the bay". The Cayuga name is "Tayęda:ne:gęˀ or Detgayę:da:negęˀ", "land of two logs."
The Bay, as it is known locally, provides some of the best trophy Walleye angling in North America as well as most sport fish common to the great lakes. The bay is subject to algae blooms in late summer which are a naturally occurring phenomenon and do not indicate pollution other than from agricultural runoff. Zebra mussels as well as the other invasive species found in the great lakes are present.
The Quinte area played a vital role in bootlegging during Prohibition in the United States, with large volumes of booze being produced in the area, and shipped via boat on the Bay to Lake Ontario finally arriving in New York State where it was distributed. Illegal sales of liquor accounted for many fortunes in and around Belleville.
Tourism in the area is significant, especially in the summer months due to the Bay of Quinte and its fishing, local golf courses, provincial parks, and wineries.
Geography.
The northern side of the bay is defined by Ontario's mainland, while the southern side follows the shore of the Prince Edward County headland. Beginning in the east with the outlet to Lake Ontario, the bay runs west-southwest for to Picton (although this section is also called Adolphus Reach), where it turns north-northwest for another as far as Deseronto. From there it turns south-southwest again for another , running past Big Island on the south and Belleville on the north. The width of the bay rarely exceeds two kilometers. The bay ends at Trenton (Quinte West) and the Trent River, both also on the north side. The Murray Canal has been cut through the "Carrying Place", the few miles separating the end of the bay and Lake Ontario on the west side. The Trent River is part of the Trent-Severn Waterway, a canal connecting Lake Ontario to Lake Simcoe and then Georgian Bay on Lake Huron.
There are several sub-bays off the Bay of Quinte, including Hay Bay, Big Bay, and Muscote Bay.
Quinte Region.
Quinte is also a region comprising several communities situated along the Bay of Quinte, including Belleville which is the largest city in the Quinte Region, and represents a midpoint between Montreal, Ottawa, and Toronto.
The Greater Bay of Quinte area includes the municipalities of Brighton, Quinte West, Belleville, Prince Edward County, and Greater Napanee as well as the Native Tyendinaga Mohawk Territory. Overall population of the area exceeds 200,000.
Mohawks of the Bay of Quinte.
The Mohawks of the Bay of Quinte (Kenhtè:ke Kanyen'kehá:ka) on traditional Tyendinaga Mohawk Territory. Their reserve Band number 244, their current land base, is a 73 km² (18000-acre) on the Bay of Quinte in southeastern Ontario, Canada, east of Belleville and immediately to the west of Deseronto.
The community takes its name from a variant spelling of Mohawk leader Joseph Brant's traditional Mohawk name, Thayendanegea (standardized spelling Thayentiné:ken), which means 'two pieces of fire wood beside each other'. Officially, in the Mohawk language, the community is called "Kenhtè:ke" (Tyendinaga) which means "on the bay" the birthplace of Tekanawí:ta. The Cayuga name is Tyendinaga, "Tayęda:ne:gęˀ or Detgayę:da:negęˀ", "land of two logs.")
Education.
The Quinte Region, specifically the City of Belleville, is home to "Loyalist College of Applied Arts and Technology." Other post-secondary schools in the region include; "Maxwell College of Advanced Technology," "CDI College," "Ontario Business College," and "Quinte Literacy." Secondary Schools in the region include "Albert College" (private school) and "Sir James Whitney" (a school for the deaf and severely hearing-impaired).
Industry and employment.
The Quinte Region is home to a large number of national and international food processing manufacturers. Quinte also houses a large number of industries in the plastics & packaging sector, transportation sector, logistics sector and advanced manufacturing sector, including the following (just a few of over 350 industries located in the Bay of Quinte Region) :

</doc>
<doc id="4207" url="http://en.wikipedia.org/wiki?curid=4207" title="Bassoon">
Bassoon

The bassoon is a woodwind instrument in the double reed family that typically plays music written in the bass and tenor clefs, and occasionally the treble. Appearing in its modern form in the 19th century, the bassoon figures prominently in orchestral, concert band and chamber music literature. The bassoon is a non-transposing instrument known for its distinctive tone color, wide range, variety of character and agility. Listeners often compare its warm, dark, reedy timbre to that of a male baritone voice. Someone who plays the bassoon is called a bassoonist.
Etymology.
The word bassoon comes from French "basson" and from Italian "bassone" ("basso" with the augmentative suffix "-one").
Range.
The range of the bassoon begins at B1 (the first one below the bass staff) and extends upward over three octaves, roughly to the G above the treble staff (G5). Higher notes are possible but difficult to produce, and rarely called for: orchestral and concert band parts rarely go higher than C5 or D5. Even Stravinsky's famously difficult opening solo in "The Rite of Spring" only ascends to D5.
A1 is possible with a special extension to the instrument—see "Extended techniques" below.
Construction.
The bassoon disassembles into six main pieces, including the reed. The bell (6), extending upward; the bass joint (or long joint) (5), connecting the bell and the boot; the boot (or butt) (4), at the bottom of the instrument and folding over on itself; the wing joint (3), which extends from boot to bocal; and the bocal (or crook) (2), a crooked metal tube that attaches the wing joint to a reed (1) (). Bassoons are double reed instruments like the oboe and the English horn.
A modern beginner's bassoon is generally made of maple, or plastic, with medium-hardness types such as sycamore maple and sugar maple preferred. Less-expensive models are also made of materials such as polypropylene and ebonite, primarily for student and outdoor use; metal bassoons were made in the past but have not been produced by any major manufacturer since 1889. The bore of the bassoon is conical, like that of the oboe and the saxophone, and the two adjoining bores of the boot joint are connected at the bottom of the instrument with a U-shaped metal connector. Both bore and tone holes are precision-machined, and each instrument is finished by hand for proper tuning. The walls of the bassoon are thicker at various points along the bore; here, the tone holes are drilled at an angle to the axis of the bore, which reduces the distance between the holes on the exterior. This ensures coverage by the fingers of the average adult hand. Wooden instruments are lined with hard rubber along the interior of the wing and boot joints to prevent damage from moisture; wooden instruments are also stained and varnished. The end of the bell is usually fitted with a ring, either of metal, plastic or ivory. The joints between sections consist of a tenon fitting into a socket; the tenons are wrapped in either cork or string as a seal against air leaks. The bocal connects the reed to the rest of the instrument and is inserted into a socket at the top of the wing joint. Bocals come in many different lengths and styles, depending on the desired tuning and playing characteristics.
Folded upon itself, the bassoon stands tall, but the total sounding length is . Playing is facilitated by doubling the tube back on itself and by closing the distance between the widely spaced holes with a complex system of key work, which extends throughout nearly the entire length of the instrument. There are also short-reach bassoons made for the benefit of young or petite players.
Development.
Early history.
Music historians generally consider the dulcian to be the forerunner of the modern bassoon, as the two instruments share many characteristics: a double reed fitted to a metal crook, obliquely drilled tone holes and a conical bore that doubles back on itself. The origins of the dulcian are obscure, but by the mid-16th century it was available in as many as eight different sizes, from soprano to great bass. A full consort of dulcians was a rarity; its primary function seems to have been to provide the bass in the typical wind band of the time, either loud (shawms) or soft (recorders), indicating a remarkable ability to vary dynamics to suit the need. Otherwise, dulcian technique was rather primitive, with eight finger holes and two keys, indicating that it could play in only a limited number of key signatures.
The dulcian came to be known as "fagotto" in Italy. However, the usual etymology that equates "fagotto" with "bundle of sticks" is somewhat misleading, as the latter term did not come into general use until later. Some think it may resemble the Roman Fasces, a standard of bound sticks with an ax. A further discrepancy lies in the fact that the dulcian was carved out of a single block of wood—in other words, a single "stick" and not a bundle.
Circumstantial evidence indicates that the baroque bassoon was a newly invented instrument, rather than a simple modification of the old dulcian. The dulcian was not immediately supplanted, but continued to be used well into the 18th century by Bach and others. The man most likely responsible for developing the true bassoon was Martin Hotteterre (d.1712), who may also have invented the three-piece "flûte traversière" and the "hautbois" (baroque oboe). Some historians believe that sometime in the 1650s, Hotteterre conceived the bassoon in four sections (bell, bass joint, boot and wing joint), an arrangement that allowed greater accuracy in machining the bore compared to the one-piece dulcian. He also extended the compass down to B by adding two keys. An alternate view maintains Hotteterre was one of several craftsmen responsible for the development of the early bassoon. These may have included additional members of the Hotteterre family, as well as other French makers active around the same time. No original French bassoon from this period survives, but if it did, it would most likely resemble the earliest extant bassoons of Johann Christoph Denner and Richard Haka from the 1680s. Sometime around 1700, a fourth key (G♯) was added, and it was for this type of instrument that composers such as Antonio Vivaldi, Bach, and Georg Philipp Telemann wrote their demanding music. A fifth key, for the low E, was added during the first half of the 18th century. Notable makers of the 4-key and 5-key baroque bassoon include J.H. Eichentopf (c. 1678–1769), J. Poerschmann (1680–1757), Thomas Stanesby, Jr. (1668–1734), G.H. Scherer (1703–1778), and Prudent Thieriot (1732–1786).
Modern history.
Increasing demands on capabilities of instruments and players in the 19th century—particularly larger concert halls requiring greater volume and the rise of virtuoso composer-performers—spurred further refinement. Increased sophistication, both in manufacturing techniques and acoustical knowledge, made possible great improvements in the instrument's playability.
The modern bassoon exists in two distinct primary forms, the Buffet system and the Heckel system. Most of the world plays the Heckel system, while the Buffet system is primarily played in France, Belgium, and parts of Latin America.
Heckel (German) system.
The design of the modern bassoon owes a great deal to the performer, teacher, and composer Carl Almenräder. Assisted by the German acoustic researcher Gottfried Weber, he developed the 17-key bassoon with a range spanning four octaves. Almenräder's improvements to the bassoon began with an 1823 treatise describing ways of improving intonation, response, and technical ease of playing by augmenting and rearranging the keywork. Subsequent articles further developed his ideas. His employment at Schott gave him the freedom to construct and test instruments according to these new designs, and he published the results in "Caecilia", Schott's house journal. Almenräder continued publishing and building instruments until his death in 1846, and Ludwig van Beethoven himself requested one of the newly made instruments after hearing of the papers. In 1831, Almenräder left Schott to start his own factory with a partner, Johann Adam Heckel.
Heckel and two generations of descendants continued to refine the bassoon, and their instruments became the standard, with other makers following. Because of their superior singing tone quality (an improvement upon one of the main drawbacks of the Almenräder instruments), the Heckel instruments competed for prominence with the reformed Wiener system, a Boehm-style bassoon, and a completely keyed instrument devised by Charles-Joseph Sax, father of Adolphe Sax. F.W. Kruspe implemented a latecomer attempt in 1893 to reform the fingering system, but it failed to catch on. Other attempts to improve the instrument included a 24-keyed model and a single-reed mouthpiece, but both these had adverse effects on tone and were abandoned.
Coming into the 20th century, the Heckel-style German model of bassoon dominated the field. Heckel himself had made over 1,100 instruments by the turn of the 20th century (serial numbers begin at 3,000), and the British makers' instruments were no longer desirable for the changing pitch requirements of the symphony orchestra, remaining primarily in military band use.
Except for a brief 1940s wartime conversion to ball bearing manufacture, the Heckel concern has produced instruments continuously to the present day. Heckel bassoons are considered by many to be the best, although a range of Heckel-style instruments is available from several other manufacturers, all with slightly different playing characteristics. Companies that manufacture Heckel-system bassoons include: Wilhelm Heckel, Yamaha, Fox Products, W. Schreiber & Söhne, Püchner, Conn-Selmer, Linton, Moosmann Kohlert, Moennig/Adler, B.H. Bell, Walter, and Guntram Wolf. In addition, several factories in the People's Republic of China are producing inexpensive instruments under such labels as Laval, Haydn, and Lark, and these have been available in the West for some time now. However, they are generally of marginal quality and are usually avoided by serious players.
Because its mechanism is primitive compared to most modern woodwinds, makers have occasionally attempted to "reinvent" the bassoon. In the 1960s, Giles Brindley began to develop what he called the "logical bassoon," which aimed to improve intonation and evenness of tone through use of an electrically activated mechanism, making possible key combinations too complex for the human hand to manage. Brindley's logical bassoon was never marketed.
Buffet (French) system.
The Buffet system bassoon achieved its basic acoustical properties somewhat earlier than the Heckel. Thereafter it continued to develop in a more conservative manner. While the early history of the Heckel bassoon included a complete overhaul of the instrument in both acoustics and keywork, the development of the Buffet system consisted primarily of incremental improvements to the keywork. This minimalist approach deprived the Buffet of the improved consistency, ease of operation, and increased power found in the Heckel bassoons, but the Buffet is considered by some to have a more vocal and expressive quality. The conductor John Foulds lamented in 1934 the dominance of the Heckel-style bassoon, considering them too homogeneous in sound with the horn.
Compared to the Heckel bassoon, Buffet system bassoons have a narrower bore and simpler mechanism, requiring different fingerings for many notes. Switching between Heckel and Buffet requires extensive retraining. Buffet instruments are known for a reedier sound and greater facility in the upper registers, reaching e" and f" with far greater ease and less air pressure. French woodwind tone in general exhibits a certain amount of "edge," with more of a vocal quality than is usual elsewhere, and the Buffet bassoon is no exception. This type of sound can be beneficial in music by French composers, but has drawn criticism for being too intrusive. As with all bassoons, the tone varies considerably, depending on individual instrument and performer. In the hands of a lesser player, the Heckel bassoon can sound flat and woody, but good players succeed in producing a vibrant, singing tone. Conversely, a poorly played Buffet can sound buzzy and nasal, but good players succeed in producing a warm, expressive sound, different from—but not inferior to—the Heckel.
Though the United Kingdom once favored the French system, Buffet-system instruments are no longer made there and the last prominent British player of the French system retired in the 1980s. However, with continued use in some regions and its distinctive tone, the Buffet continues to have a place in modern bassoon playing, particularly in France. Buffet-model bassoons are currently made in Paris by Buffet Crampon and The Selmer Company. Some players, for example the late Gerald Corey in Canada, have learned to play both types and will alternate between them depending on the repertoire.
Use in ensembles.
Earlier ensembles.
Orchestras first used the bassoon to reinforce the bass line, and as the bass of the double reed choir (oboes and taille). Baroque composer Jean-Baptiste Lully and his "Les Petits Violons" included oboes and bassoons along with the strings in the 16-piece (later 21-piece) ensemble, as one of the first orchestras to include the newly invented double reeds. Antonio Cesti included a bassoon in his 1668 opera "Il pomo d'oro" (The Golden Apple). However, use of bassoons in concert orchestras was sporadic until the late 17th century when double reeds began to make their way into standard instrumentation. This was largely due to the spread of the "hautbois" to countries outside of France. Increasing use of the bassoon as a "basso continuo" instrument meant that it began to be included in opera orchestras, first in France and later in Italy, Germany and England. Meanwhile, composers such as Joseph Bodin de Boismortier, Michel Corrette, Johann Ernst Galliard, Jan Dismas Zelenka, Johann Friedrich Fasch and Telemann wrote demanding solo and ensemble music for the instrument. Antonio Vivaldi brought the bassoon to prominence by featuring it in 37 concerti for the instrument.
By the mid-18th century, the bassoon's function in the orchestra was still mostly limited to that of a continuo instrument—since scores often made no specific mention of the bassoon, its use was implied, particularly if there were parts for oboes or other winds. Beginning in the early Rococo era, composers such as Joseph Haydn, Michael Haydn, Johann Christian Bach, Giovanni Battista Sammartini and Johann Stamitz included parts that exploited the bassoon for its unique color, rather than for its perfunctory ability to double the bass line. Orchestral works with fully independent parts for the bassoon would not become commonplace until the Classical era. Wolfgang Amadeus Mozart's "Jupiter" symphony is a prime example, with its famous bassoon solos in the first movement. The bassoons were generally paired, as in current practice, though the famed Mannheim orchestra boasted four.
Another important use of the bassoon during the Classical era was in the "Harmonie", a chamber ensemble consisting of pairs of oboes, horns and bassoons; later, two clarinets would be added to form an octet. The "Harmonie" was an ensemble maintained by German and Austrian noblemen for private music-making, and was a cost-effective alternative to a full orchestra. Haydn, Mozart, Ludwig van Beethoven and Franz Krommer all wrote considerable amounts of music for the "Harmonie".
Modern ensembles.
The modern symphony orchestra typically calls for two bassoons, often with a third playing the contrabassoon. Some works call for four or more players. The first player is frequently called upon to perform solo passages. The bassoon's distinctive tone suits it for both plaintive, lyrical solos such as Maurice Ravel's "Boléro" and more comical ones, such as the grandfather's theme in "Peter and the Wolf". Its agility suits it for passages such as the famous running line (doubled in the violas and cellos) in the overture to "The Marriage of Figaro". In addition to its solo role, the bassoon is an effective bass to a woodwind choir, a bass line along with the cellos and double basses, and harmonic support along with the French horns.
A wind ensemble will usually also include two bassoons and sometimes contrabassoon, each with independent parts; other types of concert wind ensembles will often have larger sections, with many players on each of first or second parts; in simpler arrangements there will be only one bassoon part and no contrabassoon. The bassoon's role in the concert band is similar to its role in the orchestra, though when scoring is thick it often cannot be heard above the brass instruments also in its range. "La Fiesta Mexicana", by H. Owen Reed, features the instrument prominently, as does the transcription of Malcolm Arnold's "Four Scottish Dances", which has become a staple of the concert band repertoire.
The bassoon is also part of the standard wind quintet instrumentation, along with the flute, oboe, clarinet, and horn; it is also frequently combined in various ways with other woodwinds. Richard Strauss's "Duet-Concertino" pairs it with the clarinet as "concertante" instruments, with string orchestra in support.
The bassoon quartet has also gained favor in recent times. The bassoon's wide range and variety of tone colors make it ideally suited to grouping in like-instrument ensembles. Peter Schickele's "Last Tango in Bayreuth" (after themes from "Tristan und Isolde") is a popular work; Schickele's fictional alter ego P. D. Q. Bach exploits the more humorous aspects with his quartet "Lip My Reeds," which at one point calls for players to perform on the reed alone. It also calls for a low A at the very end of the prelude section in the fourth bassoon part. It is written so that the first bassoon does not play; instead, his or her role is to place an extension in the bell of the fourth bassoon so that the note can be played.
Jazz.
The bassoon is infrequently used as a jazz instrument and rarely seen in a jazz ensemble. It first began appearing in the 1920s, including specific calls for its use in Paul Whiteman's group, the unusual octets of Alec Wilder, and a few other session appearances. The next few decades saw the instrument used only sporadically, as symphonic jazz fell out of favor, but the 1960s saw artists such as Yusef Lateef and Chick Corea incorporate bassoon into their recordings; Lateef's diverse and eclectic instrumentation saw the bassoon as a natural addition, while Corea employed the bassoon in combination with flautist Hubert Laws.
More recently, Illinois Jacquet, Ray Pizzi, Frank Tiberi, and Marshall Allen have both doubled on bassoon in addition to their saxophone performances. Bassoonist Karen Borca, a performer of free jazz, is one of the few jazz musicians to play only bassoon; Michael Rabinowitz, the Spanish bassoonist Javier Abad, and James Lassen, an American resident in Bergen, Norway, are others. Katherine Young plays the bassoon in the ensembles of Anthony Braxton. Lindsay Cooper, Paul Hanson, the Brazilian bassoonist Alexandre Silverio, Trent Jacobs and Daniel Smith are also currently using the bassoon in jazz. French bassoonists Jean-Jacques Decreux and Alexandre Ouzounoff have both recorded jazz, exploiting the flexibility of the Buffet system instrument to good effect.
Popular music.
The bassoon is even rarer as a regular member of rock bands. However, several 1960s pop music hits feature the bassoon, including "The Tears of a Clown" by Smokey Robinson and the Miracles (the bassoonist was Charles R. Sirard), "Jennifer Juniper" by Donovan, "The Turtles" "Happy Together"(third verse,overdub), "59th Street Bridge Song" by Harpers Bizarre, and the oompah bassoon underlying The New Vaudeville Band's "Winchester Cathedral". From 1974 to 1978, the bassoon was played by Lindsay Cooper in the British avant-garde band Henry Cow. In the 1970s it was played, in the British medieval/progressive rock band Gryphon, by Brian Gulland, as well as by the American band Ambrosia, where it was played by drummer Burleigh Drummond. The Belgian Rock in Opposition-band Univers Zero is also known for its use of the bassoon.
In the 1990s, Madonna Wayne Gacy provided bassoon for the alternative metal band Marilyn Manson as did Aimee DeFoe, in what is self-described as "grouchily lilting garage bassoon" in the indie-rock band Blogurt from Pittsburgh, Pennsylvania. More recently, These New Puritans's 2010 album Hidden makes heavy use of the instrument throughout; their principal songwriter, Jack Barnett, claimed repeatedly to be ""writing a lot of music for bassoon"" in the run-up to its recording. In early 2011, American hip-hop artist Kanye West updated his Twitter account to inform followers that he recently added the bassoon to a yet unnamed song.
The rock band Better Than Ezra took their name from a passage in Ernest Hemingway's "A Moveable Feast" in which the author comments that listening to an annoyingly talkative person is still “better than Ezra learning how to play the bassoon,” referring to Ezra Pound.
Technique.
The bassoon is held diagonally in front of the player, but unlike the flute, oboe and clarinet, it cannot be supported by the player's hands alone. Some means of additional support is required; the most common ones are a seat strap attached to the base of the boot joint, which is laid across the chair seat prior to sitting down, or a neck strap or shoulder harness attached to the top of the boot joint. Occasionally a spike similar to those used for the cello or the bass clarinet is attached to the bottom of the boot joint and rests on the floor. It is possible to play while standing up if the player uses a neck strap or similar harness, or if the seat strap is tied to the belt. Sometimes a device called a "balance hanger" is used when playing in a standing position. This is installed between the instrument and the neck strap, and shifts the point of support closer to the center of gravity.
The bassoon is played with both hands in a stationary position, the left above the right, with five main finger holes on the front of the instrument (nearest the audience) plus a sixth that is activated by an open-standing key. Five additional keys on the front are controlled by the little fingers of each hand. The back of the instrument (nearest the player) has twelve or more keys to be controlled by the thumbs, the exact number varying depending on model.
To stabilize the right hand, many bassoonists use an adjustable comma-shaped apparatus called a "crutch," or a hand rest, which mounts to the boot joint. The crutch is secured with a thumb screw, which also allows the distance that it protrudes from the bassoon to be adjusted. Players rest the curve of the right hand where the thumb joins the palm against the crutch. The crutch also keeps the right hand from tiring and enables the player to keep the finger pads flat on the finger holes and keys.
An aspect of bassoon technique not found on any other woodwind is called "flicking". It involves the left hand thumb momentarily pressing, or 'flicking' the high A, C and D keys at the beginning of certain notes in the middle octave. This eliminates cracking, or brief multiphonics that happens without the use of this technique.
Flicking is not universal amongst bassoonists; some American players, principally on the East Coast, use it sparingly, if at all. The rest use it virtually 100% of the time—it has become in essence part of the fingering.
The alternative method is "venting", which requires that the register key be used as part of the full fingering as opposed to being open momentarily at the start of the note.
While flicking is used to higher notes, the whisper key is used for lower notes. From the A right below middle C and lower, the whisper key is pressed with the left thumb and held for the duration of the note. This prevents cracking, as low notes can sometimes crack into a higher octave. Both flicking and using the whisper key is especially important to ensure notes speak properly during slurring between high and low registers.
While bassoons are usually critically tuned at the factory, the player nonetheless has a great degree of flexibility of pitch control through the use of breath support and embouchure and reed profile. Players can also use alternate fingerings to adjust the pitch of many notes. Similar to other woodwind instruments, the length of the bassoon can be increased to lower pitch or decreased to raise pitch. On the bassoon, this is done preferably by changing the bocal to one of a different length, (lengths are denoted by a number on the bocal, usually starting at 0 for the shortest length, and 3 for the longest, but there are some manufacturers who will use other numbers) but it is possible to push the bocal in or out.
Embouchure.
The bassoon embouchure is a very important aspect of producing a full, round bassoon tone, but can be difficult to obtain as a beginner. The bassoon embouchure is made by putting one's lips together as if one were whistling and then dropping the jaw down as in a yawning motion (without actually yawning or opening the mouth). Both sets of teeth should be covered by the lips in order to protect the reed. The reed is then placed in the mouth, forming a seal around the reed with the lips and facial muscles.
Extended techniques.
Many extended techniques can be performed on the bassoon, such as multiphonics, flutter-tonguing, circular breathing, double tonguing, and harmonics. In the case of the bassoon, flutter-tonguing may be accomplished by "gargling" in the back of the throat as well as by the conventional method of rolling Rs.
Also, using certain fingerings, notes may be produced on the instrument that sound lower pitches than the actual range of the instrument. These "impossible notes" tend to sound very gravelly and out of tune, but technically sound below the low B. Alternatively, lower notes can be produced by inserting a small paper or rubber tube into the end of the bell, which converts the lower B into a lower note such as an A natural; this lowers the pitch of the instrument, but has the positive effect of bringing the lowest register (which is typically quite sharp) into tune. A notable piece that calls for the use of a low A bell is Carl Nielsen's Wind Quintet, op. 43, which includes an optional low A for the final cadence of the work. Bassoonists sometimes use the end bell segment of an English horn or clarinet if one is available instead of a specially made extension. This often yields unsatisfactory results, though, as the resultant A can be quite sharp. The idea of using low A was begun by Richard Wagner, who wanted to extend the range of the bassoon. Many passages in his later operas require the low A as well as the B-flat above. (This is impossible on a normal bassoon using an A extension as the fingering for the B-flat yields the low A.) These passages are typically realized on the contrabassoon, as recommended by the composer. Some bassoons have been made to allow bassoonists to realize similar passages. These bassoons are made with a "Wagner bell," which is an extended bell with a key for both the low A and the low B-flat. Bassoons with Wagner bells suffer similar intonational deficiencies as a bassoon with an A extension. Another composer who has required the bassoon to be chromatic down to low A is Gustav Mahler. Richard Strauss also calls for the low A in his opera "Intermezzo".
Learning the bassoon.
The complicated fingering and the problem of reeds make the bassoon more difficult to learn than some of the other woodwind instruments. Cost is another factor in a person's decision to pursue the bassoon. Prices range from $8,000 up to $25,000 for a good-quality instrument. In North America, schoolchildren typically take up bassoon only after starting on another reed instrument, such as clarinet or saxophone.
Students in America often begin to pursue the study of bassoon performance and technique in the middle years of their music education. Students are often provided with a school instrument and encouraged to pursue lessons with private instructors. Students typically receive instruction in proper posture, hand position, embouchure, tone production, and reed making.
Reeds and reed construction.
Modern reeds.
Bassoon reeds, made of "Arundo donax" cane, are often made by the players themselves, although beginner bassoonists tend to buy their reeds from professional reed makers or use reeds made by their teachers. Reeds begin with a length of tube cane that is split into three or four pieces. The cane is then trimmed and "gouged" to the desired thickness, leaving the bark attached. After soaking, the gouged cane is cut to the proper shape and milled to the desired thickness, or "profile", by removing material from the bark side. This can be done by hand with a file; more frequently it is done with a machine or tool designed for the purpose. After the profiled cane has soaked once again it is folded over in the middle. Prior to soaking, the reed maker will have lightly scored the bark with parallel lines with a knife; this ensures that the cane will assume a cylindrical shape during the forming stage. On the bark portion, the reed maker binds on one, two, or three coils or loops of brass wire to aid in the final forming process. The exact placement of these loops can vary somewhat depending on the reed maker. The bound reed blank is then wrapped with thick cotton or linen thread to protect it, and a conical steel mandrel (which sometimes has been heated in a flame) is quickly inserted in between the blades. Using a special pair of pliers, the reed maker presses down the cane, making it conform to the shape of the mandrel. (The steam generated by the heated mandrel causes the cane to permanently assume the shape of the mandrel.) The upper portion of the cavity thus created is called the "throat," and its shape has an influence on the final playing characteristics of the reed. The lower, mostly cylindrical portion will be reamed out with a special tool, allowing the reed to fit on the bocal.
After the reed has dried, the wires are tightened around the reed, which has shrunk after drying, or replaced completely. The lower part is sealed (a nitrocellulose-based cement such as Duco may be used) and then wrapped with thread to ensure both that no air leaks out through the bottom of the reed and that the reed maintains its shape. The wrapping itself is often sealed with Duco or clear nail varnish (polish). The bulge in the wrapping is sometimes referred to as the "Turk's head"—it serves as a convenient handle when inserting the reed on the bocal.
To finish the reed, the end of the reed blank, originally at the center of the unfolded piece of cane, is cut off, creating an opening. The blades above the first wire are now roughly long. For the reed to play, a slight bevel must be created at the tip with a knife, although there is also a machine that can perform this function. Other adjustments with the knife may be necessary, depending on the hardness and profile of the cane and the requirements of the player. The reed opening may also need to be adjusted by squeezing either the first or second wire with the pliers. Additional material may be removed from the sides (the "channels") or tip to balance the reed. Additionally, if the "e" in the staff is sagging in pitch, it may be necessary to "clip" the reed by removing from its length.
Playing styles of individual bassoonists vary greatly; because of this, most advanced players will make their own reeds, in the process customizing them to their individual playing requirements. Many companies and individuals do offer reeds for sale, but even with store-bought reeds, the player must know how to make adjustments to suit his particular playing style.
Early reeds.
Little is known about the early construction of the bassoon reed, as few examples survive, and much of what is known is only what can be gathered from artistic representations. The earliest known written instructions date from the middle of the 17th century, describing the reed as being held together by wire or resined thread; the earliest actual reeds that survive are more than a century younger, a collection of 21 reeds from the late 18th-century Spanish "bajon".

</doc>
<doc id="4210" url="http://en.wikipedia.org/wiki?curid=4210" title="Bipedalism">
Bipedalism

Bipedalism is a form of terrestrial locomotion where an organism moves by means of its two rear limbs, or legs. An animal or machine that usually moves in a bipedal manner is known as a biped , meaning "two feet" (from the Latin "bi" for "two" and "ped" for "foot"). Types of bipedal movement include walking, running, or hopping, on two appendages (typically legs).
Few modern species are habitual bipeds whose normal method of locomotion is two-legged. Within mammals, habitual bipedalism has evolved multiple times, with the macropods, kangaroo rats and mice, springhare, hopping mice, pangolins and homininan apes, as well as various other extinct groups evolving the trait independently. In the Triassic period some groups of archosaurs (a group that includes the ancestors of crocodiles) developed bipedalism; among their descendants the dinosaurs, all the early forms and many later groups were habitual or exclusive bipeds; the birds descended from one group of exclusively bipedal dinosaurs.
A larger number of modern species utilise bipedal movement for a short time. Several non-archosaurian lizard species move bipedally when running, usually to escape from threats. Many primate and bear species will adopt a bipedal gait in order to reach food or explore their environment. Several arboreal primate species, such as gibbons and indriids, exclusively utilise bipedal locomotion during the brief periods they spend on the ground. Many animals rear up on their hind legs whilst fighting or copulating. A few animals commonly stand on their hind legs, in order to reach food, to keep watch, to threaten a competitor or predator, or to pose in courtship, but do not move bipedally.
Definition.
The word is derived from the Latin words "bi(s)" 'two (2)' and "ped-" 'foot', as contrasted with quadruped 'four feet'.
Advantages.
Limited and exclusive bipedalism can offer a species several advantages. Bipedalism raises the head; this allows a greater field of vision with improved detection of distant dangers or resources, access to deeper water for wading animals and allows the animals to reach higher food sources with their mouths. While upright, non-locomotory limbs become free for other uses, including manipulation (in primates and rodents), flight (in birds), digging (in giant pangolin), combat (in bears and the large monitor lizard) or camouflage (in certain species of octopus). The maximum bipedal speed appears less fast than the maximum speed of quadrupedal movement with a flexible backbone – the ostrich reaches speeds of and the red kangaroo , while the cheetah can exceed .
Bipedality in kangaroo rats has been hypothesized to improve locomotor performance, which could aid in escaping from predators.
Facultative and obligate bipedalism.
Zoologists often label behaviors, including bipedalism, as "facultative" (i.e. optional) or "obligate" (the animal has no reasonable alternative). Even this distinction is not completely clear-cut — for example, humans normally walk and run in biped fashion, but almost all can crawl on hands and knees when necessary. There are even reports of humans who normally walk on all fours with their feet but not their knees on the ground, but these cases are a result of conditions such as Uner Tan syndrome — very rare genetic neurological disorders rather than normal behavior. Even if one ignores exceptions caused by some kind of injury or illness, there are many unclear cases, including the fact that "normal" humans can crawl on hands and knees. This article therefore avoids the terms "facultative" and "obligate", and focuses on the range of styles of locomotion "normally" used by various groups of animals.
Movement.
There are a number of states of movement commonly associated with bipedalism.
Bipedal animals.
The great majority of living terrestrial vertebrates are quadrupeds, with bipedalism exhibited by only a handful of living groups. Humans, gibbons and large birds walk by raising one foot at a time. On the other hand most macropods, smaller birds, lemurs and bipedal rodents move by hopping on both legs simultaneously. Tree kangaroos are able to utilize either form of locomotion, most commonly alternating feet when moving arboreally and hopping on both feet simultaneously when on the ground.
Amphibians.
There are no known living or fossil bipedal amphibians.
Extant reptiles.
Many species of lizards become bipedal during high-speed, sprint locomotion, including the world's fastest lizard, the spiny-tailed iguana (genus "Ctenosaura").
Early reptiles and lizards.
The first known biped is the bolosaurid "Eudibamus" whose fossils date from 290 million years ago. Its long hindlegs, short forelegs, and distinctive joints all suggest bipedalism. The species was extinct before the dinosaurs appeared.
Archosaurs (include birds, crocodiles, and dinosaurs).
Birds.
All birds are bipeds when on the ground, a feature inherited from their dinosaur ancestors.
Other archosaurs.
Bipedalism evolved more than once in archosaurs, the group that includes both dinosaurs and crocodilians. All dinosaurs are believed to be descended from a fully bipedal ancestor, perhaps similar to "Eoraptor". Bipedal movement also re-evolved in a number of other dinosaur lineages such as the iguanodons. Some extinct members of the crocodilian line, a sister group to the dinosaurs and birds, also evolved bipedal forms - a crocodile relative from the triassic, "Effigia okeeffeae", is believed to be bipedal. Pterosaurs were previously thought to have been bipedal, but recent trackways have all shown quadrupedal locomotion. Bipedalism also evolved independently among the dinosaurs. Dinosaurs diverged from their archosaur ancestors approximately 230 million years ago during the Middle to Late Triassic period, roughly 20 million years after the Permian-Triassic extinction event wiped out an estimated 95% of all life on Earth. Radiometric dating of fossils from the early dinosaur genus "Eoraptor" establishes its presence in the fossil record at this time. Paleontologists believe "Eoraptor" resembles the common ancestor of all dinosaurs; if this is true, its traits suggest that the first dinosaurs were small, bipedal predators. The discovery of primitive, dinosaur-like ornithodirans such as "Marasuchus" and "Lagerpeton" in Argentinian Middle Triassic strata supports this view; analysis of recovered fossils suggests that these animals were indeed small, bipedal predators.
Mammals.
A number of groups of extant mammals have independently evolved bipedalism as their main form of locomotion - for example humans, giant pangolins, the extinct giant ground sloths, numerous species of jumping rodents and macropods. Humans, as their bipedalism has been extensively studied are documented in the next section. Macropods are believed to have evolved bipedal hopping only once in their evolution, at some time no later than 45 million years ago.
Bipedal movement is less common among mammals, most of which are quadrupedal. All primates possess some bipedal ability, though most species primarily use quadrupedal locomotion on land. Primates aside, the macropods (kangaroos, wallabies and their relatives), kangaroo rats and mice, hopping mice and springhare move bipedally by hopping. Very few mammals other than primates commonly move bipedally by an alternating gait rather than hopping. Exceptions are the ground pangolin and in some circumstances the tree kangaroo. [http://blogs.scientificamerican.com/tetrapod-zoology/2011/10/17/tree-kangaroos-come-first/]
Primates.
Most bipedal animals move with their backs close to horizontal, using a long tail to balance the weight of their bodies. The primate version of bipedalism is unusual because the back is close to upright (completely upright in humans). Many primates can stand upright on their hind legs without any support. 
Chimpanzees, bonobos, gibbons and baboons exhibit forms of bipedalism. Injured chimpanzees and bonobos have been capable of sustained bipedalism.
Geladas, although often quadrupedal, will move between adjacent feeding patches with a squatting, shuffling bipedal form of locomotion [http://pin.primate.wisc.edu/factsheets/entry/gelada_baboon/].
Three captive primates, one macaque Natasha and two chimps, Oliver and Poko (chimpanzee), were found to move bipedally ]. Natasha switched to exclusive bipedalism after an illness, while Poko was discovered in captivity in a tall, narrow cage. Oliver reverted to knuckle-walking after developing arthritis.
In addition, non-human primates often use bipedal locomotion when carrying food. One hypothesis for human bipedalism is thus that it evolved as a result of differentially successful survival from carrying food to share with group members, although there are other hypotheses, as discussed below.
Limited bipedalism.
Limited bipedalism in mammals.
Other mammals engage in limited, non-locomotory, bipedalism. A number of other animals, such as rats, raccoons, and beavers will squat on their hindlegs to manipulate some objects but revert to four limbs when moving (the beaver may also move bipedally if transporting wood for their dams). Bears will fight in a bipedal stance to use their forelegs as weapons. A number of mammals will adopt a bipedal stance in specific situations such as for feeding or fighting. Ground squirrels and meerkats will stand on hind legs to survey their surroundings, but will not walk bipedally. Dogs can stand or move on two legs if trained, or if birth defect or injury precludes quadrupedalism. The gerenuk antelope stands on its hind legs while eating from trees, as did the extinct giant ground sloth and chalicotheres. The spotted skunk will also use limited bipedalism when threatened, rearing up on its forelimbs while facing the attacker so its anal glands, capable of spraying an offensive oil, face its attacker.
Limited bipedalism in non-mammals.
Bipedalism is unknown among the amphibians. Among the non-archosaur reptiles bipedalism is rare, but it is found in the 'reared-up' running of lizards such as agamids and monitor lizards. Many reptile species will also temporarily adopt bipedalism while fighting. One genus of basilisk lizard can run bipedally across the surface of water for some distance. Among arthropods, cockroaches are known to move bipedally at high speeds. Bipedalism is rarely found outside terrestrial animals, though at least two types of octopus walk bipedally on the sea floor using two of their arms, allowing the remaining arms to be used to camouflage the octopus as a mat of algae or a floating coconut.
Evolution of human bipedalism.
There are at least twelve distinct hypotheses as to how and why bipedalism evolved in humans, and also some debate as to when. Bipedalism evolved well before the large human brain or the development of stone tools. Bipedal specializations are found in "Australopithecus" fossils from 4.2-3.9 million years ago. Recent evidence regarding modern human sexual dimorphism (physical differences between male and female) in the lumbar spine has been seen in pre-modern primates such as "Australopithecus africanus". This dimorphism has been seen as an evolutionary adaptation of females to bear lumbar load better during pregnancy, an adaptation that non-bipedal primates would not need to make. The different hypotheses are not necessarily mutually exclusive and a number of selective forces may have acted together to lead to human bipedalism. It is important to distinguish between adaptations for bipedalism and adaptations for running, which came later still.
Possible reasons for the evolution of human bipedalism include freeing the hands for tool use and carrying, sexual dimorphism in food gathering, changes in climate and habitat (from jungle to savanna) that favored a more elevated eye-position, and to reduce the amount of skin exposed to the tropical sun.
Savanna-based theory.
According to the savanna-based theory, hominines descended from the trees and adapted to life on the savanna by walking erect on two feet. The theory suggests that early hominids were forced to adapt to bipedal locomotion on the open savanna after they left the trees. In fact, Elizabeth Vrba’s turnover pulse hypothesis supports the savanna-based theory by explaining the shrinking of forested areas due to global warming and cooling, which forced animals out into the open grasslands and caused the need for hominids to acquire bipedality.
Rather, the bipedal adaptation hominines had already achieved was used in the savanna. The fossil record shows that early bipedal hominines were still adapted to climbing trees at the time they were also walking upright. Hominine fossils found in dry grassland environments led anthropologists to believe hominines lived, slept, walked upright, and died only in those environments because no hominine fossils were found in forested areas. However, fossilization is a rare occurrence—the conditions must be just right in order for an organism that dies to become fossilized for somebody to find later, which is also a rare occurrence. The fact that no hominine fossils were found in forests does not ultimately lead to the conclusion that no hominines ever died there. The convenience of the savanna-based theory caused this point to be overlooked for over a hundred years.
Some of the fossils found actually showed that there was still an adaptation to arboreal life. For example, Lucy, the famous "Australopithecus afarensis", found in Hadar in Ethiopia, which may have been forested at the time of Lucy’s death, had curved fingers that would still give her the ability to grasp tree branches, but she walked bipedally. “Little Foot,” the collection of "Australopithecus africanus" foot bones, has a divergent big toe as well as the ankle strength to walk upright. “Little Foot” could grasp things using his feet like an ape, perhaps tree branches, and he was bipedal. Ancient pollen found in the soil in the locations in which these fossils were found suggest that the area used to be covered in thick vegetation and has only recently become the arid desert it is now.
Traveling efficiency hypothesis.
An alternative explanation is the mixture of savanna and scattered forests increased terrestrial travel by proto-humans between clusters of trees, and bipedalism offered greater efficiency for long-distance travel between these clusters than quadrupedalism.
Postural feeding hypothesis.
The postural feeding hypothesis has been recently supported by Dr. Kevin Hunt, a professor at Indiana University. This hypothesis asserts that chimpanzees were only bipedal when they ate. While on the ground, they would reach up for fruit hanging from small trees and while in trees, bipedalism was utilized by grabbing for an overhead branch. These bipedal movements may have evolved into regular habits because they were so convenient in obtaining food. Also, Hunt hypothesises that these movements coevolved with chimpanzee arm-hanging, as this movement was very effective and efficient in harvesting food. When analyzing fossil anatomy, "Australopithecus afarensis" has very similar features of the hand and shoulder to the chimpanzee, which indicates hanging arms. Also, the "Australopithecus" hip and hind limb very clearly indicate bipedalism, but these fossils also indicate very inefficient locomotive movement when compared to humans. For this reason, Hunt argues that bipedalism evolved more as a terrestrial feeding posture than as a walking posture.
Provisioning model.
One theory on the origin of bipedalism is the behavioral model presented by C. Owen Lovejoy, known as "male provisioning". Lovejoy theorizes that the evolution of bipedalism was linked to monogamy. In the face of long inter-birth intervals and low reproductive rates typical of the apes, early hominids engaged in pair-bonding that enabled greater parental effort directed towards rearing offspring. Lovejoy proposes that male provisioning of food would improve the offspring survivorship and increase the pair's reproductive rate. Thus the male would leave his mate and offspring to search for food and return carrying the food in his arms walking on his legs. This model is supported by the reduction ("feminization") of the male canine teeth in early hominids such as "Sahelanthropus tchadensis" and "Ardipithecus ramidus", which along with low body size dimorphism in "Ardipithecus" and "Australopithecus", suggests a reduction in inter-male antagonistic behavior in early hominids. In addition, this model is supported by a number of modern human traits associated with concealed ovulation (permanently enlarged breasts, lack of sexual swelling) and low sperm competition (moderate sized testes, low sperm mid-piece volume) that argues against recent adaptation to a polygynous reproductive system.
However, this model has generated some controversy, as others have argued that early bipedal hominids were instead polygynous. Among most monogamous primates, males and females are about the same size. That is sexual dimorphism is minimal, and other studies have suggested that Australopithecus afarensis males were nearly twice the weight of females. However, Lovejoy's model posits that the larger range a provisioning male would have to cover (to avoid competing with the female for resources she could attain herself) would select for increased male body size to limit predation risk. Furthermore, as the species became more bipedal, specialized feet would prevent the infant from conveniently clinging to the mother - hampering the mother's freedom and thus make her and her offspring more dependent on resources collected by others. Modern monogamous primates such as gibbons tend to be also territorial, but fossil evidence indicates that "Australopithecus afarensis" lived in large groups. However, while both gibbons and hominids have reduced canine sexual dimorphism, female gibbons enlarge ('masculinize') their canines so they can actively share in the defense of their home territory. Instead, the reduction of the male hominid canine is consistent with reduced inter-male aggression in a group living primate.
Early bipedalism in homininae model.
Recent studies of 4.4 million years old "Ardipithecus ramidus" suggest bipedalism, it is thus possible that bipedalism evolved very early in homininae and was reduced in chimpanzee and gorilla when they became more specialized. According to Richard Dawkins in his book "The Ancestor's Tale", chimps and bonobos are descended from "Australopithecus" gracile type species while gorillas are descended from Paranthropus. These apes may have once been bipedal, but then lost this ability when they were forced back into an arboreal habitat, presumably by those australopithecines who eventually became us (see Homininae). Early homininaes such as "Ardipithecus ramidus" may have possessed an arboreal type of bipedalism that later independently evolved towards knuckle-walking in chimpanzees and gorillas and towards efficient walking and running in modern humans (see figure). It is also proposed that one cause of Neanderthal extinction was a less efficient running.
Warning display (aposematic) model.
Joseph Jordania from the University of Melbourne recently (2011) suggested that bipedalism was one of the central elements of the general defense strategy of early hominids, based on aposematism, or warning display and intimidation of potential predators and competitors with exaggerated visual and audio signals. According to this model, hominids were trying to stay as visible and as loud as possible all the time. Several morphological and behavioral developments were employed to achieve this goal: upright bipedal posture, longer legs, long tightly coiled hair on the top of the head, body painting, threatening synchronous body movements, loud voice and extremely loud rhythmic singing/stomping/drumming on external subjects. Slow locomotion and strong body odor (both characteristic for hominids and humans) are other features often employed by aposematic species to advertise their non-profitability for potential predators.
Other behavioural models.
There are a variety of ideas which promote a specific change in behaviour as the key driver for the evolution of hominid bipedalism. For example, Wescott (1967) and later Jablonski & Chaplin (1993) suggest that bipedal threat displays could have been the transitional behaviour which led to some groups of apes beginning to adopt bipedal postures more often. Others ("e.g." Dart 1925) have offered the idea that the need for more vigilance against predators could have provided the initial motivation. Dawkins ("e.g." 2004) has argued that it could have begun as a kind of fashion that just caught on and then escalated through sexual selection. And it has even been suggested ("e.g." Tanner 1981:165) that male phallic display could have been the initial incentive.
Thermoregulatory model.
The thermoregulatory model explaining the origin of bipedalism is one of the simplest theories so far advanced, but it is a viable explanation. Dr. Peter Wheeler, a professor of evolutionary biology, proposes that bipedalism raises the amount of body surface area higher above the ground which results in a reduction in heat gain and helps heat dissipation. When a hominid is higher above the ground, the organism accesses more favorable wind speeds and temperatures. During heat seasons, greater wind flow results in a higher heat loss, which makes the organism more comfortable. Also, Wheeler explains that a vertical posture minimizes the direct exposure to the sun whereas quadrupedalism exposes more of the body to direct exposure. Analysis and interpretations of Ardipithecus reveal that this hypothesis needs modification to consider that the forest and woodland environmental preadaptation of early-stage hominid bipedalism preceded further refinement of bipedalism by the pressure of natural selection. This then allowed for the more efficient exploitation of the hotter conditions ecological niche, rather than the hotter conditions being hypothetically bipedalism's initial stimulus.
Carrying models.
Charles Darwin wrote that "Man could not have attained his present dominant position in the world without the use of his hands, which are so admirably adapted to the act of obedience of his will" Darwin (1871:52) and many models on bipedal origins are based on this line of thought. Gordon Hewes (1961) suggested that the carrying of meat "over considerable distances" (Hewes 1961:689) was the key factor. Isaac (1978) and Sinclair et al. (1986) offered modifications of this idea as indeed did Lovejoy (1981) with his 'provisioning model' described above. Others, such as Nancy Tanner (1981) have suggested that infant carrying was key, whilst others have suggested stone tools and weapons drove the change.
Wading models.
Several theories have been proposed regarding the influence of water on human bipedalism. The aquatic ape hypothesis, promoted for several decades by Elaine Morgan, proposed that swimming, diving and aquatic food sources exerted a strong influence on many aspects of human evolution, including bipedalism. It is not accepted by or considered a serious theory within the anthropological scholarly community. Others, however, cite bipedalism among a cluster of other adaptations unique among primates, including voluntary control of breathing, hairlessness, subcutaneous fat and several other traits that are difficult to explain with more conventional theories.
Other theories have been proposed that suggest wading and the exploitation of aquatic food sources (providing essential nutrients for human brain evolution or critical fallback foods) may have exerted evolutionary pressures on human ancestors promoting adaptations which later assisted full-time bipedalism.
Physiology.
Bipedal movement occurs in a number of ways, and requires many mechanical and neurological adaptations. Some of these are described below.
Biomechanics.
Standing.
Energy-efficient means of standing bipedally involve constant adjustment of balance, and of course these must avoid overcorrection. The difficulties associated with simple standing in upright humans are highlighted by the greatly increased risk of falling present in the elderly, even with minimal reductions in control system effectiveness.
Walking.
Walking is characterized by an "inverted pendulum" movement in which the center of gravity vaults over a stiff leg with each step. Force plates can be used to quantify the whole-body kinetic & potential energy, with walking displaying an out-of-phase relationship indicating exchange between the two. Interestingly, this model applies to all walking organisms regardless of the number of legs, and thus bipedal locomotion does not differ in terms of whole-body kinetics.
In humans, walking is composed of several separate processes:
Running.
Running is characterized by a spring-mass movement. Kinetic and potential energy are in phase, and the energy is stored & released from a spring-like limb during foot contact. Again, the whole-body kinetics are similar to animals with more limbs.
Musculature.
Bipedalism requires strong leg muscles, particularly in the thighs. Contrast in domesticated poultry the well muscled legs, against the small and bony wings. Likewise in humans, the quadriceps and hamstring muscles of the thigh are both so crucial to bipedal activities that each alone is much larger than the well-developed biceps of the arms.
Respiration.
A biped also has the ability to breathe while it runs. Humans usually take a breath every other stride when their aerobic system is functioning. During a sprint, at which point the anaerobic system kicks in, breathing slows until the anaerobic system can no longer sustain a sprint.
Bipedal robots.
For nearly the whole of the 20th century, bipedal robots were very difficult to construct and robot locomotion involved only wheels, treads, or multiple legs. Recent cheap and compact computing power has made two-legged robots more feasible. Some notable biped robots are ASIMO, HUBO and QRIO. Recently, spurred by the success of creating a fully passive, un-powered bipedal walking robot, those working on such machines have begun using principles gleaned from the study of human and animal locomotion, which often relies on passive mechanisms to minimize power consumption.

</doc>
<doc id="4211" url="http://en.wikipedia.org/wiki?curid=4211" title="Bootstrapping">
Bootstrapping

In general parlance, bootstrapping usually refers to the starting of a self-sustaining process that is supposed to proceed without external input. In computer technology the term (usually shortened to booting) usually refers to the process of loading the basic software into the memory of a computer after power-on or general reset, especially the operating system which will then take care of loading other software as needed.
The term appears to have originated in the early 19th century United States (particularly in the phrase "pull oneself over a fence by one's bootstraps"), to mean an absurdly impossible action, an adynaton.
Etymology.
Tall boots may have a tab, loop or handle at the top known as a bootstrap, allowing one to use fingers or a boot hook tool to help pulling the boots on. The saying "to pull oneself up by one's bootstraps" was already in use during the 19th century as an example of an impossible task. The idiom dates at least to 1834, when it appeared in the "Workingman's Advocate": "It is conjectured that Mr. Murphee will now be enabled to hand himself over the Cumberland river or a barn yard fence by the straps of his boots." In 1860 it appeared in a comment on metaphysical philosophy: "The attempt of the mind to analyze itself an effort analogous to one who would lift himself by his own bootstraps." Bootstrap as a metaphor, meaning to better oneself by one's own unaided efforts, was in use in 1922. This metaphor spawned additional metaphors for a series of self-sustaining processes that proceed without external help.
The term is sometimes attributed to Rudolf Erich Raspe's story "", where the main character pulls himself (and his horse) out of a swamp by his hair (specifically, his pigtail), but Baron Münchhausen (1720–1797), a recounter of tall tales, does not, in fact, pull himself out by his bootstraps – and there's no sign that anyone has found an explicit reference to bootstraps in the various versions of the Munchausen tales.
Applications.
Computing.
Software loading and execution.
Booting is the process of starting a computer, specifically in regards to starting its software. The process involves a chain of stages, in which at each stage a smaller simpler program loads and then executes the larger more complicated program of the next stage. It is in this sense that the computer "pulls itself up by its bootstraps", i.e. it improves itself by its own efforts. Booting is a chain of events that starts with execution of hardware-based procedures and may then hand-off to firmware and software which is loaded into main memory. Booting often involves processes such as performing self-tests, loading configuration settings, loading a BIOS, resident monitors, a hypervisor, an operating system, or utility software.
The computer term bootstrap began as a metaphor in the 1950s. In computers, pressing a bootstrap button caused a hardwired program to read a bootstrap program from an input unit. The computer would then execute the bootstrap program, which caused it to read more program instructions. It became a self-sustaining process that proceeded without external help from manually entered instructions. As a computing term, bootstrap has been used since at least 1953.
Software development.
Bootstrapping can also refer to the development of successively more complex, faster programming environments. The simplest environment will be, perhaps, a very basic text editor (e.g., ed) and an assembler program. Using these tools, one can write a more complex text editor, and a simple compiler for a higher-level language and so on, until one can have a graphical IDE and an extremely high-level programming language.
Historically, bootstrapping also refers to an early technique for computer program development on new hardware. The technique described in this paragraph has been replaced by the use of a cross compiler executed by a pre-existing computer. Bootstrapping in program development began during the 1950s when each program was constructed on paper in decimal code or in binary code, bit by bit (1s and 0s), because there was no high-level computer language, no compiler, no assembler, and no linker. A tiny assembler program was hand-coded for a new computer (for example the IBM 650) which converted a few instructions into binary or decimal code: A1. This simple assembler program was then rewritten in its just-defined assembly language but with extensions that would enable the use of some additional mnemonics for more complex operation codes. The enhanced assembler's source program was then assembled by its predecessor's executable (A1) into binary or decimal code to give A2, and the cycle repeated (now with those enhancements available), until the entire instruction set was coded, branch addresses were automatically calculated, and other conveniences (such as conditional assembly, macros, optimisations, etc.) established. This was how the early assembly program SOAP (Symbolic Optimal Assembly Program) was developed. Compilers, linkers, loaders, and utilities were then coded in assembly language, further continuing the bootstrapping process of developing complex software systems by using simpler software.
The term was also championed by Doug Engelbart to refer to his belief that organizations could better evolve by improving the process they use for improvement (thus obtaining a compounding effect over time). His SRI team that developed the NLS hypertext system applied this strategy by using the tool they had developed to improve the tool.
Compilers.
The development of compilers for new programming languages first developed in an existing language but then rewritten in the new language and compiled by itself, is another example of the bootstrapping notion. Using an existing language to bootstrap a new language is one way to solve the "chicken or the egg" causality dilemma.
Installers.
During the installation of computer programs it is sometimes necessary to update the installer or package manager itself. The common pattern for this is to use a small executable bootstrapper file (e.g. setup.exe) which updates the installer and starts the real installation after the update. Sometimes the bootstrapper also installs other prerequisites for the software during the bootstrapping process.
Overlay networks.
A bootstrapping node, also known as a rendezvous host, is a node in an overlay network that provides initial configuration information to newly joining nodes so that they may successfully join the overlay network.
Discrete event simulation.
A type of computer simulation called discrete event simulation represents the operation of a system as a chronological sequence of events. A technique called "bootstrapping the simulation model" is used, which bootstraps initial data points using a pseudorandom number generator to schedule an initial set of pending events, which schedule additional events, and with time, the distribution of event times approaches its steady state—the bootstrapping behavior is overwhelmed by steady-state behavior.
Artificial intelligence and machine learning.
Bootstrapping is a technique used to iteratively improve a classifier's performance. Seed AI is a hypothesized type of artificial intelligence capable of recursive self-improvement. Having improved itself, it would become better at improving itself, potentially leading to an exponential increase in intelligence. No such AI is known to exist, but it remains an active field of research.
Seed AI is a significant part of some theories about the technological singularity: proponents believe that the development of seed AI will rapidly yield ever-smarter intelligence (via bootstrapping) and thus a new era.
Research.
Bootstrapping is a database searching technique. One may perform an inexact search (using keywords, for instance) and retrieve numerous "hits", some of which will be on-target. When the researcher looks at a relevant document that comes through in the mix, subject headings will be located within the document. The researcher can then execute a new search using authorized subject headings that will yield more focused, pinpointed results.
Statistics.
Bootstrapping is a resampling technique used to obtain estimates of summary statistics.
Business.
Bootstrapping in business means starting a business without external help or capital. Such startups fund the development of their company through internal cash flow and are cautious with their expenses. Generally at the start of a venture, a small amount of money will be set aside for the bootstrap process. Bootstrapping can also be a supplement for econometric models. Bootstrapping was also expanded upon in the book "Bootstrap Business", by Richard Christiansen.
Biology.
Richard Dawkins in his book "River Out of Eden" used the computer bootstrapping concept to explain how biological cells differentiate: "Different cells receive different combinations of chemicals, which switch on different combinations of genes, and some genes work to switch other genes on or off. And so the bootstrapping continues, until we have the full repertoire of different kinds of cells."
Phylogenetics.
Bootstrapping analysis gives a way to judge the strength of support for clades on phylogenetic trees. A number is written by a node, which reflects the percentage of bootstrap trees which also resolve the clade at the endpoints of that branch.
Law.
Bootstrapping is a rule preventing the admission of hearsay evidence in conspiracy cases.
Linguistics.
Bootstrapping is a theory of language acquisition.
Physics.
Bootstrapping is using very general consistency criteria to determine the form of a quantum theory from some assumptions on the spectrum of particles.
Electronics.
Bootstrapping is a form of positive feedback in analog circuit design.
Electric power grid.
Typically an electric power grid is never brought down intentionally. Generators and power stations are started and shut down as necessary. A power station requires all kinds of power to be working before it ever sends power on to the grid. This power is obtained the same way as anyone, from the grid. This presents a puzzle in case the entire grid is crashed. Power stations can contribute when the grid is energized, and the grid is energized when power stations contribute to it.
A black start is the process of restoring a power station to operation without relying on the external electric power transmission network. In the absence of grid power, a so-called black start needs to be performed to bootstrap the power grid into operation.
Cellular networks.
A Bootstrapping Server Function (BSF) is an intermediary element in cellular networks which provides application independent functions for mutual authentication of user equipment and servers unknown to each other and for 'bootstrapping' the exchange of secret session keys afterwards. The term 'bootstrapping' is related to building a security relation with a previously unknown device first and to allow installing security elements (keys) in the device and the BSF afterwards.

</doc>
<doc id="4213" url="http://en.wikipedia.org/wiki?curid=4213" title="Baltic languages">
Baltic languages

The Baltic languages are part of the Balto-Slavic branch of the Indo-European language family spoken by the Balts. Baltic languages are spoken mainly in areas extending east and southeast of the Baltic Sea in Northern Europe. They are usually considered a single family divided into two groups: Western Baltic, containing only extinct languages, and Eastern Baltic, containing two living languages, Lithuanian and Latvian. The range of the Eastern Balts once reached to the Ural mountains. Although related, the Lithuanian, the Latvian, and particularly the Old Prussian vocabularies differ substantially from one another and are not mutually intelligible. The now-extinct Old Prussian language is considered the most archaic of the Baltic languages.
Branches.
The Baltic languages are generally thought to form a single family with two branches, Eastern and Western. However, these are sometimes classified as independent branches of Balto-Slavic.
Eastern Baltic languages.
"("†"—Extinct language)"
Nadruvian was spoken near the isogloss of Eastern and Western Baltic, but is too poorly attested to classify.
Geographic distribution.
Speakers of modern Baltic languages are generally concentrated within the borders of Lithuania and Latvia, and in emigrant communities in the United States, Canada, Australia and states of the former Soviet Union. Historically the languages were spoken over a larger area: West to the mouth of the Vistula river in present-day Poland, at least as far East as the Dniepr river in present-day Belarus, perhaps even to Moscow, perhaps as far south as Kiev. Key evidence of Baltic language presence in these regions is found in hydronyms (names of bodies of water) in the regions that are characteristically Baltic. Use of hydronyms is generally accepted to determine the extent of these cultures' influence, but "not" the date of such influence. Historical expansion of the usage of Slavic languages in the South and East, and Germanic languages in the West reduced the geographic distribution of Baltic languages to a fraction of the area that they formerly covered.
Prehistory and history.
Although the various Baltic tribes were mentioned by ancient historians as early as 98 B.C., the first attestation of a Baltic language was in about 1350, with the creation of the "Elbing Prussian Vocabulary", a German to Prussian translation dictionary. It is also believed that Baltic languages are among the most archaic of the remaining Indo-European languages, despite their late attestation. Lithuanian was first attested in a hymnal translation in 1545; the first printed book in Lithuanian, a Catechism by Martynas Mažvydas was published in 1547 in Königsberg, Prussia (now Kaliningrad, Russia). Latvian appeared in a hymnal in 1530 and in a printed Catechism in 1585. One reason for the late attestation is that the Baltic peoples resisted Christianization longer than any other Europeans, which delayed the introduction of writing and isolated their languages from outside influence.
With the establishment of a German state in Prussia, and the eradication or flight of much of the Baltic Prussian population in the 13th century, the remaining Prussians began to be assimilated, and by the end of the 17th century, the Prussian language had become extinct.
During the years of the Polish-Lithuanian Commonwealth (1569–1795), official documents were written in Polish, Ruthenian and Latin, with Lithuanian being mostly an oral language, with small quantities of written documents. After the Partitions of Commonwealth, most of the Baltic lands were under the rule of the Russian Empire, where the native languages were sometimes prohibited from being written down, or used publicly (see Lithuanian press ban).
Relationship with other Indo-European languages.
The Baltic languages are of particular interest to linguists because they retain many archaic features, which are believed to have been present in the early stages of the Proto-Indo-European language. However, linguists have had a hard time establishing the precise relationship of the Baltic languages to other languages in the Indo-European family. Several of the extinct Baltic languages have a limited or nonexistent written record, their existence being known only from the records of ancient historians and personal or place names. All of the languages in the Baltic group (including the living ones) were first written down relatively late in their probable existence as distinct languages. These two factors combined with others have obscured the history of the Baltic languages, leading to a number of theories regarding their position in the Indo-European family.
The Baltic languages show a close relationship with the Slavic languages, and are grouped with them in a Balto-Slavic family by most scholars. This family is considered to have developed from a common ancestor, Proto-Balto-Slavic. Later on, several lexical, phonological and morphological dialectisms developed, separating the various Balto-Slavic languages from each other. Although it is generally agreed upon that the Slavic languages developed from a single more-or-less unified dialect (Proto-Slavic) that split off from common Balto-Slavic, there is more disagreement about the relationship between the Baltic languages.
The traditional view is that the Balto-Slavic languages split into two branches, Baltic and Slavic, with each branch developing as a single common language (Proto-Baltic and Proto-Slavic) for some time afterwards. Proto-Baltic is then thought to have split into East Baltic and West Baltic branches. However, more recent scholarship has suggested that there was no unified Proto-Baltic stage, but that Proto-Balto-Slavic split directly into three groups: Slavic, East Baltic and West Baltic. Under this view, the Baltic family is paraphyletic, and consists of all Balto-Slavic languages that are not Slavic. This would imply that Proto-Baltic, the last common ancestor of all Baltic languages, would be identical to Proto-Balto-Slavic itself, rather than distinct from it.
Finally, there is a minority of scholars who argue that Baltic descended directly from Proto-Indo-European, without an intermediate common Balto-Slavic stage. They argue that the many similarities and shared innovations between Baltic and Slavic are due to several millennia of contact between the groups, rather than shared heritage.

</doc>
<doc id="4214" url="http://en.wikipedia.org/wiki?curid=4214" title="Bioinformatics">
Bioinformatics

Bioinformatics is an interdisciplinary scientific field that develops methods and software tools for storing, retrieving, organizing and analyzing biological data. As an interdisciplinary field, bioinformatics combines computer science, statistics, mathematics and engineering to study biological data and processes.
Information systems such as databases and ontologies are used to store and organize biological data. Analyzing biological data to produce meaningful information involves writing and running software programs that use algorithms from graph theory, artificial intelligence, soft computing, data mining, image processing, and computer simulation. The algorithms in turn depend on theoretical foundations such as discrete mathematics, control theory, system theory, information theory, and statistics.
Bioinformatics is similar but distinct science from biological computation and computational biology. Biological computation uses bioengineering and biology to build biological computers, whereas bioinformatics uses computation to better understand biology. Bioinformatics and computational biology have similar aims and approaches, but differ in scale: bioinformatics organizes and analyzes basic biological data, whereas computational biology builds theoretical models of biological systems, just as mathematical biology does with mathematical models.
Commonly used software tools and technologies in the field include Java, C#, XML, Perl, C, C++, Python, R, SQL, CUDA, MATLAB, and spreadsheet applications.
Introduction.
Bioinformatics has become an important part of many areas of biology. In experimental molecular biology, bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics and genomics, it aids in sequencing and annotating genomes and their observed mutations. It plays a role in the textual mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in the comparison of genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA, RNA, and protein structures as well as molecular interactions.
History.
Paulien Hogeweg coined the term "Bioinformatics" in 1970 to refer to the study of information processes in biotic systems. This definition placed bioinformatics as a field parallel to biophysics (the study of physical processes in biological systems) or biochemistry (the study of chemical processes in biological systems).
Sequences. Computers became essential in molecular biology when protein sequences became available after Frederick Sanger determined the sequence of insulin in the early 1950s. Comparing multiple sequences manually turned out to be impractical. A pioneer in the field was Margaret Oakley Dayhoff, who has been hailed by David Lipman, director of the National Center for Biotechnology Information, as the "mother and father of bioinformatics." Dayhoff compiled one of the first protein sequence databases, initially published as books and pioneered methods of sequence alignment and molecular evolution. Another early contributor to bioinformatics was Elvin A. Kabat, who pioneered biological sequence analysis in 1970 with his comprehensive volumes of antibody sequences released with Tai Te Wu between 1980 and 1991.
Genomes. As whole genome sequences became available, again with the pioneering work of Frederick Sanger, the term bioinformatics was re-discovered to refer to the creation of databases such as GenBank in 1982. With the public availability of data tools for their analysis were quickly developed and described in journals such as Nucleic Acids Research which published specialized issues on bioinformatics tools as early as 1982.
Goals.
In order to study how normal cellular activities are altered in different disease states, the biological data must be combined to form a comprehensive picture of these activities. Therefore, the field of bioinformatics has evolved such that the most pressing task now involves the analysis and interpretation of various types of data. This includes nucleotide and amino acid sequences, protein domains, and protein structures. The actual process of analyzing and interpreting data is referred to as computational biology. Important sub-disciplines within bioinformatics and computational biology include:
The primary goal of bioinformatics is to increase the understanding of biological processes. What sets it apart from other approaches, however, is its focus on developing and applying computationally intensive techniques to achieve this goal. Examples include: pattern recognition, data mining, machine learning algorithms, and visualization. Major research efforts in the field include sequence alignment, gene finding, genome assembly, drug design, drug discovery, protein structure alignment, protein structure prediction, prediction of gene expression and protein–protein interactions, genome-wide association studies, and the modeling of evolution.
Bioinformatics now entails the creation and advancement of databases, algorithms, computational and statistical techniques, and theory to solve formal and practical problems arising from the management and analysis of biological data.
Over the past few decades rapid developments in genomic and other molecular research technologies and developments in information technologies have combined to produce a tremendous amount of information related to molecular biology. Bioinformatics is the name given to these mathematical and computing approaches used to glean understanding of biological processes.
Approaches.
Common activities in bioinformatics include mapping and analyzing DNA and protein sequences, aligning DNA and protein sequences to compare them, and creating and viewing 3-D models of protein structures.
There are two fundamental ways of modelling a Biological system (e.g., living cell) both coming under Bioinformatic approaches.
A broad sub-category under bioinformatics is structural bioinformatics.
Sequence analysis.
Since the Phage Φ-X174 was sequenced in 1977, the DNA sequences of thousands of organisms have been decoded and stored in databases. This sequence information is analyzed to determine genes that encode polypeptides (proteins), RNA genes, regulatory sequences, structural motifs, and repetitive sequences. A comparison of genes within a species or between different species can show similarities between protein functions, or relations between species (the use of molecular systematics to construct phylogenetic trees). With the growing amount of data, it long ago became impractical to analyze DNA sequences manually. Today, computer programs such as BLAST are used daily to search sequences from more than 260 000 organisms, containing over 190 billion nucleotides. These programs can compensate for mutations (exchanged, deleted or inserted bases) in the DNA sequence, to identify sequences that are related, but not identical. A variant of this sequence alignment is used in the sequencing process itself. The so-called shotgun sequencing technique (which was used, for example, by The Institute for Genomic Research to sequence the first bacterial genome, "Haemophilus influenzae") does not produce entire chromosomes. Instead it generates the sequences of many thousands of small DNA fragments (ranging from 35 to 900 nucleotides long, depending on the sequencing technology). The ends of these fragments overlap and, when aligned properly by a genome assembly program, can be used to reconstruct the complete genome. Shotgun sequencing yields sequence data quickly, but the task of assembling the fragments can be quite complicated for larger genomes. For a genome as large as the human genome, it may take many days of CPU time on large-memory, multiprocessor computers to assemble the fragments, and the resulting assembly will usually contain numerous gaps that have to be filled in later. Shotgun sequencing is the method of choice for virtually all genomes sequenced today, and genome assembly algorithms are a critical area of bioinformatics research.
Another aspect of bioinformatics in sequence analysis is annotation. This involves computational gene finding to search for protein-coding genes, RNA genes, and other functional sequences within a genome. Not all of the nucleotides within a genome are part of genes. Within the genomes of higher organisms, large parts of the DNA do not serve any obvious purpose. This so-called junk DNA may, however, contain unrecognized functional elements. Bioinformatics helps to bridge the gap between genome and proteome projects — for example, in the use of DNA sequences for protein identification.
Genome annotation.
In the context of genomics, annotation is the process of marking the genes and other biological features in a DNA sequence. The first genome annotation software system was designed in 1995 by Owen White, who was part of the team at The Institute for Genomic Research that sequenced and analyzed the first genome of a free-living organism to be decoded, the bacterium "Haemophilus influenzae". White built a software system to find the genes (fragments of genomic sequence that encode proteins), the transfer RNAs, and to make initial assignments of function to those genes. Most current genome annotation systems work similarly, but the programs available for analysis of genomic DNA, such as the GeneMark program trained and used to find protein-coding genes in "Haemophilus influenzae", are constantly changing and improving.
Computational evolutionary biology.
Evolutionary biology is the study of the origin and descent of species, as well as their change over time. Informatics has assisted evolutionary biologists by enabling researchers to:
Future work endeavours to reconstruct the now more complex tree of life.
The area of research within computer science that uses genetic algorithms is sometimes confused with computational evolutionary biology, but the two areas are not necessarily related.
Comparative genomics.
The core of comparative genome analysis is the establishment of the correspondence between genes (orthology analysis) or other genomic features in different organisms. It is these intergenomic maps that make it possible to trace the evolutionary processes responsible for the divergence of two genomes. A multitude of evolutionary events acting at various organizational levels shape genome evolution. At the lowest level, point mutations affect individual nucleotides. At a higher level, large chromosomal segments undergo duplication, lateral transfer, inversion, transposition, deletion and insertion. Ultimately, whole genomes are involved in processes of hybridization, polyploidization and endosymbiosis, often leading to rapid speciation. The complexity of genome evolution poses many exciting challenges to developers of mathematical models and algorithms, who have recourse to a spectra of algorithmic, statistical and mathematical techniques, ranging from exact, heuristics, fixed parameter and approximation algorithms for problems based on parsimony models to Markov Chain Monte Carlo algorithms for Bayesian analysis of problems based on probabilistic models.
Many of these studies are based on the homology detection and protein families computation.
Genetics of Disease.
With the advent of next-generation sequencing we are obtaining enough sequence data to map the genes of complex diseases such as infertility, breast cancer or Alzheimer's Disease. Genome-wide association studies are essential to pinpoint the mutations for such complex diseases.
Analysis of mutations in cancer.
In cancer, the genomes of affected cells are rearranged in complex or even unpredictable ways. Massive sequencing efforts are used to identify previously unknown point mutations in a variety of genes in cancer. Bioinformaticians continue to produce specialized automated systems to manage the sheer volume of sequence data produced, and they create new algorithms and software to compare the sequencing results to the growing collection of human genome sequences and germline polymorphisms. New physical detection technologies are employed, such as oligonucleotide microarrays to identify chromosomal gains and losses (called comparative genomic hybridization), and single-nucleotide polymorphism arrays to detect known "point mutations". These detection methods simultaneously measure several hundred thousand sites throughout the genome, and when used in high-throughput to measure thousands of samples, generate terabytes of data per experiment. Again the massive amounts and new types of data generate new opportunities for bioinformaticians. The data is often found to contain considerable variability, or noise, and thus Hidden Markov model and change-point analysis methods are being developed to infer real copy number changes.
Another type of data that requires novel informatics development is the analysis of lesions found to be recurrent among many tumors.
Gene and protein expression.
Analysis of gene expression.
The expression of many genes can be determined by measuring mRNA levels with multiple techniques including microarrays, expressed cDNA sequence tag (EST) sequencing, serial analysis of gene expression (SAGE) tag sequencing, massively parallel signature sequencing (MPSS), RNA-Seq, also known as "Whole Transcriptome Shotgun Sequencing" (WTSS), or various applications of multiplexed in-situ hybridization. All of these techniques are extremely noise-prone and/or subject to bias in the biological measurement, and a major research area in computational biology involves developing statistical tools to separate signal from noise in high-throughput gene expression studies. Such studies are often used to determine the genes implicated in a disorder: one might compare microarray data from cancerous epithelial cells to data from non-cancerous cells to determine the transcripts that are up-regulated and down-regulated in a particular population of cancer cells.
Analysis of protein expression.
Protein microarrays and high throughput (HT) mass spectrometry (MS) can provide a snapshot of the proteins present in a biological sample. Bioinformatics is very much involved in making sense of protein microarray and HT MS data; the former approach faces similar problems as with microarrays targeted at mRNA, the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases, and the complicated statistical analysis of samples where multiple, but incomplete peptides from each protein are detected.
Analysis of regulation.
Regulation is the complex orchestration of events starting with an extracellular signal such as a hormone and leading to an increase or decrease in the activity of one or more proteins. Bioinformatics techniques have been applied to explore various steps in this process. For example, promoter analysis involves the identification and study of sequence motifs in the DNA surrounding the coding region of a gene. These motifs influence the extent to which that region is transcribed into mRNA. Expression data can be used to infer gene regulation: one might compare microarray data from a wide variety of states of an organism to form hypotheses about the genes involved in each state. In a single-cell organism, one might compare stages of the cell cycle, along with various stress conditions (heat shock, starvation, etc.). One can then apply clustering algorithms to that expression data to determine which genes are co-expressed. For example, the upstream regions (promoters) of co-expressed genes can be searched for over-represented regulatory elements. Examples of clustering algorithms applied in gene clustering are k-means clustering, self-organizing maps (SOMs), hierarchical clustering, and consensus clustering methods such as the Bi-CoPaM. The later, namely Bi-CoPaM, has been actually proposed to address various issues specific to gene discovery problems such as consistent co-expression of genes over multiple microarray datasets.
Structural bioinformatics.
Prediction of protein structure.
Protein structure prediction is another important application of bioinformatics. The amino acid sequence of a protein, the so-called primary structure, can be easily determined from the sequence on the gene that codes for it. In the vast majority of cases, this primary structure uniquely determines a structure in its native environment. (Of course, there are exceptions, such as the bovine spongiform encephalopathy – a.k.a. Mad Cow Disease – prion.) Knowledge of this structure is vital in understanding the function of the protein. For lack of better terms, structural information is usually classified as one of "secondary", "tertiary" and "quaternary" structure. A viable general solution to such predictions remains an open problem. Most efforts have so far been directed towards heuristics that work most of the time.
One of the key ideas in bioinformatics is the notion of homology. In the genomic branch of bioinformatics, homology is used to predict the function of a gene: if the sequence of gene "A", whose function is known, is homologous to the sequence of gene "B," whose function is unknown, one could infer that B may share A's function. In the structural branch of bioinformatics, homology is used to determine which parts of a protein are important in structure formation and interaction with other proteins. In a technique called homology modeling, this information is used to predict the structure of a protein once the structure of a homologous protein is known. This currently remains the only way to predict protein structures reliably.
One example of this is the similar protein homology between hemoglobin in humans and the hemoglobin in legumes (leghemoglobin). Both serve the same purpose of transporting oxygen in the organism. Though both of these proteins have completely different amino acid sequences, their protein structures are virtually identical, which reflects their near identical purposes.
Other techniques for predicting protein structure include protein threading and "de novo" (from scratch) physics-based modeling.
"See also:" structural motif and structural domain.
Network and systems biology.
Network analysis seeks to understand the relationships within biological networks such as metabolic or protein-protein interaction networks. Although biological networks can be constructed from a single type of molecule or entity (such as genes), network biology often attempts to integrate many different data types, such as proteins, small molecules, gene expression data, and others, which are all connected physically and/or functionally.
Systems biology involves the use of computer simulations of cellular subsystems (such as the networks of metabolites and enzymes which comprise metabolism, signal transduction pathways and gene regulatory networks) to both analyze and visualize the complex connections of these cellular processes. Artificial life or virtual evolution attempts to understand evolutionary processes via the computer simulation of simple (artificial) life forms.
Molecular interaction networks.
Tens of thousands of three-dimensional protein structures have been determined by X-ray crystallography and protein nuclear magnetic resonance spectroscopy (protein NMR) and a central question in structural bioinformatics is whether it is practical to predict possible protein–protein interactions only based on these 3D shapes, without performing protein–protein interaction experiments. A variety of methods have been developed to tackle the protein–protein docking problem, though it seems that there is still much work to be done in this field.
Other interactions encountered in the field include Protein–ligand (including drug) and protein–peptide. Molecular dynamic simulation of movement of atoms about rotatable bonds is the fundamental principle behind computational algorithms, termed docking algorithms, for studying molecular interactions.
Others.
Literature analysis.
The growth in the number of published literature makes it virtually impossible to read every paper, resulting in disjointed sub-fields of research. Literature analysis aims to employ computational and statistical linguistics to mine this growing library of text resources. For example:
The area of research draws from statistics and computational linguistics.
High-throughput image analysis.
Computational technologies are used to accelerate or fully automate the processing, quantification and analysis of large amounts of high-information-content biomedical imagery. Modern image analysis systems augment an observer's ability to make measurements from a large or complex set of images, by improving accuracy, objectivity, or speed. A fully developed analysis system may completely replace the observer. Although these systems are not unique to biomedical imagery, biomedical imaging is becoming more important for both diagnostics and research. Some examples are:
High-throughput single cell data analysis.
Computational techniques are used to analyse high-throughput, low-measurement single cell data, such as that obtained from flow cytometry. These methods typically involve finding populations of cells that are relevant to a particular disease state or experimental condition.
Biodiversity Informatics.
Biodiversity informatics deals with the collection and analysis of biodiversity data, such as taxonomic databases, or microbiome data. Examples of such analyses include phylogenetics, niche modelling, species richness mapping, or species identification tools.
Databases.
Databases are essential for bioinformatics research and applications. There is a huge number of available databases covering almost everything from DNA and protein sequences, molecular structures, to phenotypes and biodiversity. Databases generally fall into one of three types. Some contain data resulting directly from empirical methods such as gene knockouts. Others consist of predicted data, and most contain data from both sources. There are meta-databases that incorporate data compiled from multiple other databases. Some others are specialized, such as those specific to an organism. These databases vary in their format, way of accession and whether they are public or not. Some of the most commonly used databases are listed below. For a more comprehensive list, please check the link at the beginning of the subsection.
Please keep in mind that this is a quick sampling and generally most computation data is supported by wet lab data as well.
Software and tools.
Software tools for bioinformatics range from simple command-line tools, to more complex graphical programs and standalone web-services available from various bioinformatics companies or public institutions.
Open-source bioinformatics software.
Many free and open-source software tools have existed and continued to grow since the 1980s. The combination of a continued need for new algorithms for the analysis of emerging types of biological readouts, the potential for innovative "in silico" experiments, and freely available open code bases have helped to create opportunities for all research groups to contribute to both bioinformatics and the range of open-source software available, regardless of their funding arrangements. The open source tools often act as incubators of ideas, or community-supported plug-ins in commercial applications. They may also provide "de facto" standards and shared object models for assisting with the challenge of bioinformation integration.
The range of open-source software packages includes titles such as Bioconductor, BioPerl, Biopython, BioJava, BioRuby, Bioclipse, EMBOSS, .NET Bio, Taverna workbench, and UGENE. In order to maintain this tradition and create further opportunities, the non-profit Open Bioinformatics Foundation have supported the annual Bioinformatics Open Source Conference (BOSC) since 2000.
Web services in bioinformatics.
SOAP- and REST-based interfaces have been developed for a wide variety of bioinformatics applications allowing an application running on one computer in one part of the world to use algorithms, data and computing resources on servers in other parts of the world. The main advantages derive from the fact that end users do not have to deal with software and database maintenance overheads.
Basic bioinformatics services are classified by the EBI into three categories: SSS (Sequence Search Services), MSA (Multiple Sequence Alignment), and BSA (Biological Sequence Analysis). The availability of these service-oriented bioinformatics resources demonstrate the applicability of web-based bioinformatics solutions, and range from a collection of standalone tools with a common data format under a single, standalone or web-based interface, to integrative, distributed and extensible bioinformatics workflow management systems.
Bioinformatics workflow management systems.
A Bioinformatics workflow management system is a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in a Bioinformatics application. Such systems are designed to
Some of the platforms giving this service: Galaxy, Kepler, Taverna, UGENE, Anduril and Anvaya.
Education platforms.
Software platforms designed to teach bioinformatics concepts and methods include Rosalind and online courses offered through the Swiss Institute of Bioinformatics Training Portal.
Conferences.
There are several large conferences that are concerned with bioinformatics. Some of the most notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB), Research in Computational Molecular Biology (RECOMB) and American Society of Mass Spectrometry (ASMS).

</doc>
<doc id="4216" url="http://en.wikipedia.org/wiki?curid=4216" title="Brian De Palma">
Brian De Palma

Brian Russell De Palma (born September 11, 1940) is an American film director and screenwriter. In a career spanning over 40 years, he is probably best known for his suspense and crime thriller films. He directed successful and popular films such as "Carrie", "Dressed to Kill", "Scarface", "The Untouchables", "Carlito's Way", and "". De Palma is credited with fostering the careers of or outrightly discovering Robert De Niro, Jill Clayburgh, John C. Reilly, John Leguizamo, Andy Garcia and Margot Kidder.
Early life.
De Palma, who is of Italian American ancestry, was born in Newark, New Jersey, the son of Vivienne (née Muti) and Anthony Federico De Palma, an orthopedic surgeon. He was raised in Philadelphia, Pennsylvania and New Hampshire and attended various Protestant and Quaker schools, eventually graduating from Friends' Central School. He won a regional science-fair prize for a project titled "An Analog Computer to Solve Differential Equations".
1960s and early career.
Enrolled at Columbia as a physics student, De Palma became enraptured with the filmmaking process after viewing "Citizen Kane" and "Vertigo". De Palma subsequently enrolled at the newly coed Sarah Lawrence College as a graduate student in their theater department in the early 1960s, becoming one of the first male students among a female population. Once there, influences as various as drama teacher Wilford Leach, the Maysles brothers, Michelangelo Antonioni, Jean-Luc Godard, Andy Warhol and Alfred Hitchcock impressed upon De Palma the many styles and themes that would shape his own cinema in the coming decades. An early association with a young Robert De Niro resulted in "The Wedding Party". The film, which was co-directed with Leach and producer Cynthia Munroe, had been shot in 1963 but remained unreleased until 1969, when De Palma's star had risen sufficiently within the Greenwich Village filmmaking scene. De Niro was unknown at the time; the credits mistakenly display his name as "Robert ." The film is noteworthy for its invocation of silent film techniques and an insistence on the jump-cut for effect. De Palma followed this with various small films for the NAACP and The Treasury Department.
During the 1960s, De Palma began making a living producing documentary films, notably "The Responsive Eye", a 1966 movie about "The Responsive Eye" op-art exhibit curated by William Seitz for Museum of Modern Art in 1965. In an interview with Gelmis from 1969, De Palma described the film as "very good and very successful. It's distributed by Pathe Contemporary and makes lots of money. I shot it in four hours, with synched sound. I had two other guys shooting people's reactions to the paintings, and the paintings themselves."
"Dionysus in 69" (1969) was De Palma's other major documentary from this period. The film records The Performance Group's performance of Euripides' The Bacchae, starring, amongst others, De Palma regular William Finley. The play is noted for breaking traditional barriers between performers and audience. The film's most striking quality is its extensive use of the split-screen. De Palma recalls that he was "floored" by this performance upon first sight, and in 1973 recounts how he "began to try and figure out a way to capture it on film. I came up with the idea of split-screen, to be able to show the actual audience involvement, to trace the life of the audience and that of the play as they merge in and out of each other."
De Palma's most significant features from this decade are "Greetings" (1968) and "Hi, Mom!" (1970). Both films star Robert De Niro and espouse a Leftist revolutionary viewpoint common to their era. "Greetings" was entered into the 19th Berlin International Film Festival, where it won a Silver Bear award. His other major film from this period is the slasher comedy "Murder a la Mod". Each of these films contains experiments in narrative and intertextuality, reflecting De Palma's stated intention to become the "American Godard" while integrating several of the themes which permeated Hitchcock's work.
"Greetings" is about three New Yorkers dealing with the draft. The film is often considered the first to deal explicitly with the draft. The film is noteworthy for its use of various experimental techniques to convey its narrative in ultimately unconventional ways. Footage was sped up, rapid cutting was used to distance the audience from the narrative, and it was difficult to discern with whom the audience must ultimately align. "Greetings" ultimately grossed over $1 million at the box office and cemented De Palma's position as a bankable filmmaker.
After the success of his 1968 breakthrough, De Palma and his producing partner, Charles Hirsch, were given the opportunity by Sigma 3 to make an unofficial sequel of sorts, initially entitled "Son of Greetings", and subsequently released as "Hi, Mom!". While "Greetings" accentuated its varied cast, "Hi, Mom!" focuses on De Niro's character, Jon Rubin, an essential carry-over from the previous film. The film is ultimately significant insofar as it displays the first enunciation of De Palma's style in all its major traits – voyeurism, guilt, and a hyper-consciousness of the medium are all on full display, not just as hallmarks, but built into this formal, material apparatus itself.
These traits come to the fore in "Hi, Mom!"'s "Be Black, Baby" sequence. This sequence parodies cinéma vérité, the dominant documentary tradition of the 1960s, while simultaneously providing the audience with a visceral and disturbingly emotional experience. De Palma describes the sequence as a constant invocation of Brechtian distanciation: “First of all, I am interested in the medium of film itself, and I am constantly standing outside and making people aware that they are always watching a film. At the same time I am evolving it. In "Hi, Mom!" for instance, there is a sequence where you are obviously watching a ridiculous documentary and you are told that and you are aware of it, but it still sucks you in. There is a kind of Brechtian alienation idea here: you are aware of what you are watching at the same time that you are emotionally involved with it.”
"Be Black, Baby" was filmed in black and white stock on 16 mm, in low-light conditions that stress the crudity of the direct cinema aesthetic. It is precisely from this crudity that the film itself gains a credibility of “realism.” In an interview with Michael Bliss, De Palma notes “Black, Baby was rehearsed for almost three weeks... In fact, it's all scripted. But once the thing starts, they just go with the way it's going. I specifically got a very good documentary camera filmmaker (Robert Elfstrom) to just shoot it like a documentary to follow the action." Furthermore, "I wanted to show in "Hi, Mom!" how you can really involve an audience. You take an absurd premise – "Be Black, Baby" – and totally involve them and really frighten them at the same time. It's very Brechtian. You suck 'em in and annihilate 'em. Then you say, "It's just a movie, right? It's not real." It's just like television. You're sucked in all the time, and you're being lied to in a very documentary-like setting. The "Be Black, Baby" section of "Hi, Mom!" is probably the most important piece of film I've ever done."
Transition to Hollywood.
In the 1970s, De Palma went to Hollywood where he worked on bigger budget films. In 1970, De Palma left New York for Hollywood at age thirty to make "Get to Know Your Rabbit", starring Orson Welles and Tommy Smothers. Making the film was a crushing experience for De Palma as Tommy Smothers didn't like a lot of De Palma's ideas.
After several small, studio and independent released films that included stand-outs "Sisters", "Phantom of the Paradise", and "Obsession", a small film based on a novel called "Carrie" was released directed by Brian De Palma. The psychic thriller "Carrie" is seen by some as De Palma's bid for a blockbuster. In fact, the project was small, underfunded by United Artists, and well under the cultural radar during the early months of production, as Stephen King's source novel had yet to climb the bestseller list. De Palma gravitated toward the project and changed crucial plot elements based upon his own predilections, not the saleability of the novel. The cast was young and relatively new, though the stars Sissy Spacek and John Travolta had gained considerable attention for previous work in, respectively, film and episodic sitcoms. "Carrie" became a hit, the first genuine box-office success for De Palma. It garnered Spacek and Piper Laurie Oscar nominations for their performances. Preproduction for the film had coincided with the casting process for George Lucas's "", and many of the actors cast in De Palma's film had been earmarked as contenders for Lucas's movie, and vice-versa. The "shock ending" finale is effective even while it upholds horror-film convention, its suspense sequences are buttressed by teen comedy tropes, and its use of split-screen, split-diopter and slow motion shots tell the story visually rather than through dialogue.
The financial and critical success of "Carrie" allowed De Palma to pursue more personal material. "The Demolished Man" was a novel that had fascinated De Palma since the late 1950s and appealed to his background in mathematics and avant-garde storytelling. Its unconventional unfolding of plot (exemplified in its mathematical layout of dialogue) and its stress on perception have analogs in De Palma's filmmaking. He sought to adapt it on numerous occasions, though the project would carry a substantial price tag, and has yet to appear onscreen (Steven Spielberg's adaptation of Philip K. Dick's "Minority Report" bears striking similarities to De Palma's visual style and some of the themes of "The Demolished Man"). The result of his experience with adapting "The Demolished Man" was "The Fury", a science fiction psychic thriller that starred Kirk Douglas, Carrie Snodgress, John Cassavetes and Amy Irving. The film was admired by Jean-Luc Godard, who featured a clip in his mammoth Histoire(s) du cinéma, and Pauline Kael who championed both "The Fury" and De Palma. The film boasted a larger budget than "Carrie", though the consensus view at the time was that De Palma was repeating himself, with diminishing returns. As a film it retains De Palma's considerable visual flair, but points more toward his work in mainstream entertainments such as "The Untouchables" and "", the thematic complex thrillers for which he is now better known.
For many film-goers, De Palma's gangster films, most notably "Scarface" and "Carlito's Way", pushed the envelope of violence and depravity, and yet greatly vary from one another in both style and content and also illustrate De Palma's evolution as a film-maker. In essence, the excesses of "Scarface" contrast with the more emotional tragedy of "Carlito's Way". Both films feature Al Pacino in what has become a fruitful working relationship. In 1984, he directed the music video of Bruce Springsteen's song "Dancing in the Dark".
Later into the 1990s and 2000s, De Palma did other films. He attempted to do dramas and a few thrillers plus science fiction. Some of these movies ("Mission: Impossible, Carlito's Way") worked and some others ("Mission to Mars, Raising Cain, Snake Eyes, The Bonfire of the Vanities") failed at the box office. Of these films, "The Bonfire of the Vanities" would be De Palma's biggest box office disaster, losing millions. Another later movie from De Palma, "Redacted", unleashed a torrent of controversy over its subject of American involvement in Iraq, and supposed atrocities committed there. It received limited release in the United States.
In 2012, his film "Passion" was selected to compete for the Golden Lion at the 69th Venice International Film Festival.
De Palma today resides in Los Angeles.
Trademarks and style.
Themes.
De Palma's films can fall into two categories, his psychological thrillers ("Sisters", "Body Double", "Obsession", "Dressed to Kill", "Blow Out", "Raising Cain") and his mainly commercial films ("Scarface", "The Untouchables", "Carlito's Way", and "Mission: Impossible"). He has often produced "De Palma" films one after the other before going on to direct a different genre, but would always return to his familiar territory. Because of the subject matter and graphic violence of some of De Palma's films, such as "Dressed to Kill", "Scarface" and "Body Double", they are often at the center of controversy with the Motion Picture Association of America, film critics and the viewing public.
De Palma is known for quoting and referencing other director's work throughout his career. Michelangelo Antonioni's "Blowup" and Francis Ford Coppola's "The Conversation" plots were used for the basis of "Blow Out". "The Untouchables" finale shoot out in the train station is a clear borrow from the Odessa Steps sequence in Sergei Eisenstein's "The Battleship Potemkin". The main plot from "Rear Window" was used for "Body Double", while it also used elements of "Vertigo". "Vertigo" was also the basis for "Obsession". "Dressed to Kill" was a note-for-note homage to Hitchcock's "Psycho", including such moments as the surprise death of the lead actress and the exposition scene by the psychiatrist at the end.
Camera shots.
Film critics have often noted De Palma's penchant for unusual camera angles and compositions throughout his career. He often frames characters against the background using a canted angle shot. Split-screen techniques have been used to show two separate events happening simultaneously. To emphasize the dramatic impact of a certain scene De Palma has employed a 360-degree camera pan. Slow sweeping, panning and tracking shots are often used throughout his films, often through precisely-choreographed long takes lasting for minutes without cutting. Split focus shots, often referred to as "di-opt", are used by De Palma to emphasize the foreground person/object while simultaneously keeping a background person/object in focus. Slow-motion is frequently used in his films to increase suspense.
Personal life.
De Palma previously dated Margot Kidder in the early 1970s. He has been married and divorced three times, to actress Nancy Allen (1979–1983), producer Gale Anne Hurd (1991–1993), and Darnell Gregorio (1995–1997). He has one daughter from his marriage to Gale Anne Hurd, Lolita de Palma, born in 1991, and one daughter from his marriage to Darnell Gregorio, Piper De Palma, born in 1996.
Legacy.
De Palma is often cited as a leading member of the New Hollywood generation of film directors, a distinct pedigree who either emerged from film schools or are overtly cine-literate. His contemporaries include Martin Scorsese, Paul Schrader, John Milius, George Lucas, Francis Ford Coppola, John Carpenter, and Ridley Scott. His artistry in directing and use of cinematography and suspense in several of his films has often been compared to the work of Alfred Hitchcock. Psychologists have been intrigued by De Palma's fascination with pathology, by the aberrant behavior aroused in characters who find themselves manipulated by others.
De Palma has encouraged and fostered the filmmaking careers of directors such as Quentin Tarantino, Mark Romanek and Keith Gordon. Tarantino said – during interview with De Palma, that "Blow Out" is one of his all time favourite films, and that after watching "Scarface" he knew how to make his own film. Terrence Malick credits seeing De Palma's early films on college campus tours as a validation of independent film, and subsequently switched his attention from philosophy to filmmaking.
Critics who frequently admire De Palma's work include Pauline Kael, Roger Ebert and Armond White, among others. Kael wrote in her review of "Blow Out", "At forty, Brian De Palma has more than twenty years of moviemaking behind him, and he has been growing better and better. Each time a new film of his opens, everything he has done before seems to have been preparation for it." In his review of "Femme Fatale", Roger Ebert wrote about the director: "De Palma deserves more honor as a director. Consider also these titles: "Sisters", "Blow Out", "The Fury", "Dressed to Kill", "Carrie", "Scarface", "Wise Guys", "Casualties of War", "Carlito's Way", "Mission: Impossible". Yes, there are a few failures along the way ("Snake Eyes", "Mission to Mars", "The Bonfire of the Vanities"), but look at the range here, and reflect that these movies contain treasure for those who admire the craft as well as the story, who sense the glee with which De Palma manipulates images and characters for the simple joy of being good at it. It's not just that he sometimes works in the style of Hitchcock, but that he has the nerve to." White said in defense of De Palma's "Mission to Mars", "film is a litmus test. It can be said with certainty that any reviewer who pans it does not understand movies, let alone like them."
Criticisms.
De Palma is frequently criticized for his filmmaking style. Julie Salamon has written that "many critics argued that De Palma dressed up his woman-hating wickedness so artfully that the intelligentsia didn't see him for what he was: a perverse misogynist." Feminist writer Jane Caputi responded to De Palma's statement that "I'm always attacked for having an erotic, sexist approach-- chopping up women, putting women in peril. I'm making suspense movies! What else is going to happen to them?" by saying "Things can only 'happen to' women in the femicidal grammar. We also can note with great irony just whom De Palma claims is being attacked."
David Thomson wrote in his entry for De Palma, "There is a self-conscious cunning in De Palma's work, ready to control everything except his own cruelty and indifference."
References.
Notes
Bibliography
Further reading

</doc>
<doc id="4218" url="http://en.wikipedia.org/wiki?curid=4218" title="North American B-25 Mitchell">
North American B-25 Mitchell

The North American B-25 "Mitchell" was an American twin-engined medium bomber manufactured by North American Aviation. It was used by many Allied air forces, in every theater of World War II, as well as many other air forces after the war ended, and saw service across four decades.
The B-25 was named in honor of General Billy Mitchell, a pioneer of U.S. military aviation. By the end of its production, nearly 10,000 B-25s in numerous models had been built. These included a few limited variations, such as the United States Navy's and Marine Corps' PBJ-1 patrol bomber and the United States Army Air Forces' F-10 photo reconnaissance aircraft.
Design and development.
The B-25 was a descendant of the earlier XB-21 (North American-39) project of the mid-1930s. Experience gained in developing that aircraft was eventually used by North American in designing the B-25 (called the NA-40 by the company). One NA-40 was built, with several modifications later being done to test a number of potential improvements. These improvements included Wright R-2600 radial engines, which would become standard on the later B-25.
In 1939, the modified and improved NA-40B was submitted to the United States Army Air Corps for evaluation. This aircraft was originally intended to be an attack bomber for export to the United Kingdom and France, both of which had a pressing requirement for such aircraft in the early stages of World War II. However, those countries changed their minds, opting instead for the also-new Douglas DB-7 (later to be used by the U.S. as the A-20 Havoc). Despite this loss of sales, the NA-40B re-entered the spotlight when the Army Air Corps evaluated it for use as a medium bomber. Unfortunately, the NA-40B was destroyed in a crash on 11 April 1939. Nonetheless, the type was ordered into production, along with the army's other new medium bomber, the Martin B-26 Marauder.
Early production.
To get orders for bombers, North American Aviation designed the XB-21. It lost out to the Douglas B-18A, but persisted and developed a more advanced design, which became the B-25 Mitchell bomber.
An improvement of the NA-40B, dubbed the NA-62, was the basis for the first actual B-25. Due to the pressing need for medium bombers by the army, no experimental or service-test versions were built. Any necessary modifications were made during production runs, or to existing aircraft at field modification centers around the world.
A significant change in the early days of B-25 production was a redesign of the wing. In the first nine aircraft, a constant-dihedral wing was used, in which the wing had a consistent, straight, slight upward angle from the fuselage to the wingtip. This design caused stability problems, and as a result, the dihedral angle was nullified on the outboard wing sections, giving the B-25 its slightly gull wing configuration. Less noticeable changes during this period included an increase in the size of the tail fins and a decrease in their inward cant.
North American was the largest producer of aircraft in the war by far. It was the first time a company had produced trainers, bombers and fighters simultaneously (the AT-6/SNJ Texan, B-25 Mitchell, and the P-51 Mustang). It produced B-25s at both its Inglewood main plant and an additional 6,608 planes at its Kansas City, Kansas plant at Fairfax Airport.
A descendant of the B-25 was the North American XB-28, meant to be a high-altitude version of the B-25. Despite this premise, the actual aircraft bore little resemblance to the Mitchell and in some respects more closely resembled the Martin B-26 Marauder.
Operational history.
Far East.
The majority of B-25s in American service were used in the Pacific. It fought on Papua New Guinea, in Burma and in the island hopping campaign in the central Pacific. It was in the Pacific that the aircraft’s potential as a ground-attack aircraft was discovered and developed. The jungle environment reduced the usefulness of standard-level bombing, and made low-level attack the best tactic. The ever-increasing amount of forward firing guns was a response to this operational environment, making the B-25 a formidable strafing aircraft. 
In Burma, the B-25 was often used to attack Japanese communication links, especially bridges in central Burma. It also helped supply the besieged troops at Imphal in 1944. 
In the Pacific, the B-25 proved itself to be a very capable anti-shipping weapon, sinking many ships. Later in the war, the distances between islands limited the usefulness of the B-25, although it was used against Guam and Tinian. It was also used against Japanese-occupied islands that had been bypassed by the main campaign, as happened in the Marshall Islands.
Middle East and Italy.
The first B-25s arrived in Egypt just in time to take part in the Battle of El Alamein. From there the aircraft took part in the rest of the campaign in North Africa, the invasion of Sicily and the advance up Italy. In Italy the B-25 was used in the ground-attack role, concentrating on attacks against road and rail links in Italy, Austria and the Balkans. The B-25 had a longer range than the Douglas A-20 Havoc and Douglas A-26 Invaders, allowing it to reach further into occupied Europe. The five bombardment groups that used the B-25 in the desert and Italy were the only U.S. units to use the B-25 in Europe.
Europe.
The U.S. Eighth Air Force, based in Britain, concentrated on long-range raids over Germany and occupied Europe. Although it did have a small number of units equipped with twin-engined aircraft, the B-25 was not amongst them. However, the RAF received nearly 900 Mitchells, using them to replace Douglas Bostons, Lockheed Venturas and Vickers Wellington bombers. The Mitchell entered active RAF service on 22 January 1943. At first it was used to bomb strategic targets in occupied Europe. After the D-Day invasion the RAF used its Mitchells to support the armies in Europe, moving several squadrons to forward airbases in France and Belgium.
Operators.
USAAF.
The B-25 first gained fame as the bomber used in the 18 April 1942 Doolittle Raid, in which 16 B-25Bs led by Lieutenant Colonel Jimmy Doolittle attacked mainland Japan, four months after the bombing of Pearl Harbor. The mission gave a much-needed lift in spirits to the Americans, and alarmed the Japanese who had believed their home islands were inviolable by enemy forces. Although the amount of actual damage done was relatively minor, it forced the Japanese to divert troops for the home defense for the remainder of the war. 
The raiders took off from the carrier and successfully bombed Tokyo and four other Japanese cities without loss. Fifteen of the bombers subsequently crash-landed "en route" to recovery fields in Eastern China. These losses were the result of the task force being spotted by a Japanese vessel forcing the bombers to take off early, fuel exhaustion, stormy nighttime conditions with zero visibility, and lack of electronic homing aids at the recovery bases. Only one B-25 bomber landed intact, in Siberia where its five-man crew was interned and the aircraft confiscated. Of the 80 aircrew, 69 survived their historic mission and eventually made it back to American lines.
Following a number of additional modifications, including the addition of Plexiglas windows for the navigator and radio operator, heavier nose armament, and de-icing and anti-icing equipment, the B-25C was released to the Army. This was the second mass-produced version of the Mitchell, the first being the lightly armed B-25B used by the Doolittle Raiders. The B-25C and B-25D differed only in location of manufacture: -Cs at Inglewood, California, -Ds at Kansas City, Kansas. A total of 3,915 B-25Cs and -Ds were built by North American during World War II.
Although the B-25 was originally designed to bomb from medium altitudes in level flight, it was used frequently in the Southwest Pacific theater (SWPA) on treetop-level strafing and parafrag (parachute-retarded fragmentation bombs) missions against Japanese airfields in New Guinea and the Philippines. These heavily armed Mitchells, field-modified at Townsville, Australia, by Major Paul I. "Pappy" Gunn and North American tech rep Jack Fox, were also used on strafing and skip-bombing missions against Japanese shipping trying to resupply their land-based armies. 
Under the leadership of Lieutenant General George C. Kenney, B-25s of the Fifth and Thirteenth Air Forces devastated Japanese targets in the Southwest Pacific theater from 1942 to 1945, and played a significant role in pushing the Japanese back to their home islands. B-25s were also used with devastating effect in the Central Pacific, Alaska, North Africa, Mediterranean and China-Burma-India (CBI) theaters.
Use as a gunship.
Because of the urgent need for hard-hitting strafer aircraft, a version dubbed the B-25G was developed, in which the standard-length transparent nose and the bombardier were replaced by a shorter solid nose containing two fixed .50 in (12.7 mm) machine guns and a 75 mm (2.95 in) M4 cannon, one of the largest weapons fitted to an aircraft, similar to the experimental British Mosquito Mk. XVIII (57 mm gun), and German Ju 88P heavy cannon (up to a 75mm long-barrel "Bordkanone BK 7,5" cannon) carrying aircraft. The B-25G's cannon was manually loaded and serviced by the navigator, who was able to perform these operations without leaving his crew station just behind the pilot. This was possible due to the shorter nose of the G-model and the length of the M4, which allowed the breech to extend into the navigator's compartment.
The B-25G's successor, the B-25H, had even more firepower. The M4 gun was replaced by the lighter T13E1, designed specifically for the aircraft. The 75 mm (2.95 in) gun fired at a muzzle velocity of 2,362 ft/s (about 720 m/s). Due to its low rate of fire (approximately four rounds could be fired in a single strafing run) and relative ineffectiveness against ground targets, as well as substantial recoil, the 75 mm (2.95 in) gun was sometimes removed from both G and H models and replaced with two additional .50 in (12.7 mm) machine guns as a field modification. 
Besides that, the -H normally mounted four fixed forward-firing .50  (12.7 mm) machine guns in the nose, four more fixed ones in forward-firing cheek blisters, two more in the manned dorsal turret, one each in a pair of new waist positions, and a final pair in a new tail gunner's position. Company promotional material bragged the B-25H could "bring to bear 10 machine guns coming and four going, in addition to the 75 mm cannon, a brace of eight rockets and 3,000 lb (1,360 kg) of bombs."
The B-25H also featured a redesigned cockpit area, required by the dorsal turret having been relocated forward to the navigator's compartment – just aft of the cockpit and just ahead of the leading edge wing roots, thus requiring the addition of the waist and tail gun positions – and a heavily modified cockpit designed to be operated by a single pilot, the co-pilot's station and controls deleted, and the seat cut down and used by the navigator/cannoneer, the radio operator being moved to the aft compartment, operating the waist guns. A total of 405 B-25Gs and 1000 B-25Hs were built, the 248 of the latter being used by Navy as PBJ-1H.
The final, and the most built, version of the Mitchell, the B-25J, looked much like the earlier B, C and D, having reverted to the longer, glazed bombardier's nose, but with the -H version's relocated-forward dorsal manned turret. The less-than-successful 75 mm (2.95 in) cannon was deleted. Instead, 800 of this version were built with a solid nose containing eight .50  (12.7 mm) machine guns, while other J-models featured the earlier "greenhouse" style nose containing the bombardier's position.
Regardless of the nose style used, all J-models also included four .50 in (12.7 mm) light-barrel Browning AN/M2 guns in a pair of "fuselage package", flank-mount conformal gun pods each containing two Browning M2s, located directly beneath the pilot's and co-pilot's compartment along the external sides of the cockpit, with the co-pilot's seat and dual flight controls restored to their previous cockpit locations. The solid-nose B-25J variant carried a total of 18 .50 in (12.7 mm) light-barrel AN/M2 Browning M2 machine guns: eight in the nose, four in the flank-mount conformal gun pod packages, two in the dorsal turret, one each in the pair of waist positions, and a pair in the tail – with fourteen of the guns either aimed directly forward, or aimable to fire directly forward for strafing missions. No other main series production bomber of World War II carried as many guns.
The first 555 B-25Js (the B-25J-1-NC production block) were delivered without the fuselage package guns, because it was discovered that muzzle blast from these guns was causing severe stress in the fuselage; this problem was cured with heavier fuselage skin patches. Although later production runs returned these fuselage package guns to the aircraft, they were often removed as a field modification for the same reason. The later B-25J was additionally armed with eight 5 in (130 mm) high-velocity aircraft rockets (HVAR). In all, 4,318 B-25Js were built.
Flight characteristics.
The B-25 was a safe and forgiving aircraft to fly. With an engine out, 60° banking turns into the dead engine were possible, and control could be easily maintained down to 145 mph (230 km/h). However, the pilot had to remember to maintain engine-out directional control at low speeds after takeoff with rudder; if this maneuver was attempted with ailerons, the aircraft would snap out of control. The tricycle landing gear made for excellent visibility while taxiing. The only significant complaint about the B-25 was the extremely high noise level produced by its engines; as a result, many pilots eventually suffered from varying degrees of hearing loss. 
The high noise level was due to design and space restrictions in the engine cowlings which resulted in the exhaust "stacks" protruding directly from the cowling ring and partly covered by a small triangular fairing. This arrangement directed exhaust and noise directly at the pilot and crew compartments. Crew members and operators on the airshow circuit frequently comment that "the B-25 is the fastest way to turn aviation fuel directly into noise". Many B-25s now in civilian ownership have been modified with exhaust rings that direct the exhaust through the outboard bottom section of the cowling.
The Mitchell was an exceptionally sturdy aircraft that could withstand tremendous punishment. One well-known B-25C of the 321st Bomb Group was nicknamed "Patches" because its crew chief painted all the aircraft's flak hole patches with high-visibility zinc chromate primer. By the end of the war, this aircraft had completed over 300 missions, was belly-landed six times and sported over 400 patched holes. The airframe was so bent askew that straight-and-level flight required 8° of left aileron trim and 6° of right rudder, causing the aircraft to "crab" sideways across the sky.
An interesting characteristic of the B-25 was its ability to extend range by using one-quarter wing flap settings. Since the aircraft normally cruised in a slightly nose-high attitude, about 40 gal (150 l) of fuel was below the fuel pickup point and thus unavailable for use. The flaps-down setting gave the aircraft a more level flight attitude, which resulted in this fuel becoming available, thus slightly extending the aircraft's range.
By the time a separate United States Air Force was established in 1947, most B-25s had been consigned to long-term storage. However, a select number continued in service through the late 1940s and 1950s in a variety of training, reconnaissance and support roles. Its principal use during this period was for undergraduate training of multi-engine aircraft pilots slated for reciprocating engine or turboprop cargo, aerial refueling or reconnaissance aircraft. Still others were assigned to units of the Air National Guard in training roles in support of Northrop F-89 Scorpion and Lockheed F-94 Starfire operations. 
TB-25J-25-NC Mitchell, "44-30854", the last B-25 in the USAF inventory, assigned at March AFB, California as of March 1960, was flown to Eglin AFB, Florida, from Turner Air Force Base, Georgia, on 21 May 1960, the last flight by a USAF B-25, and presented by Brig. Gen. A. J. Russell, Commander of SAC's 822d Air Division at Turner AFB, to the Air Proving Ground Center Commander, Brig. Gen. Robert H. Warren, who in turn presented the bomber to Valparaiso, Florida Mayor Randall Roberts on behalf of the Niceville-Valparaiso Chamber of Commerce. Four of the original Tokyo Raiders were present for the ceremony, Col. Davy Jones, Col. Jack Simms, Lt. Col. Joseph Manske, and retired Master Sgt. Edwin W. Horton. It was donated back to the Air Force Armament Museum c. 1974 and marked as Doolittle's "40-2344".
Today, many B-25s are kept in airworthy condition by air museums and collectors.
U.S. Navy and USMC.
The PBJ-1 was a navalized version of the USAAF B-25, the designation standing for Patrol (P) Bomber (B) built by North American Aviation (J), first variant (-1). The PBJ had its origin in a deal cut in mid-1942 between the Navy and the USAAF. As part of the deal, 50 B-25Cs and 152 B-25Ds were transferred to the Navy from the USAAF. The bombers carried Navy bureau numbers (BuNos) beginning with BuNo 34998. The first PBJ-1s arrived in February 1943 and were used by Marine Corps pilots, beginning with Marine Bombing squadron 413 (VMB-413). Many of PBJs were equipped with a search radar with a retractable radome fitted in place of the ventral turret.
Large numbers of B-25H and J variants were delivered to the Navy as PBJ-1H and PBJ-1J respectively. These aircraft joined, but did not necessarily replace, the earlier PBJs. 
The PBJs were operated almost exclusively by the Marine Corps as land-based bombers. To operate them, the U.S. Marine Corps established a number of marine bomber squadrons (VMB), beginning with VMB-413, in March 1943 at MCAS Cherry Point, North Carolina. Eight VMB squadrons were flying PBJs by the end of 1943, forming the initial marine medium bombardment group. Four more squadrons were in the process of formation in late 1945, but had not yet deployed by the time the war ended. 
Operational use of the Marine Corps PBJ-1s began in March 1944. The marine PBJs operated from the Philippines, Saipan, Iwo Jima and Okinawa during the last few months of the Pacific war. Their primary mission was the long range interdiction of enemy shipping that was trying to run the blockade which was strangling Japan. The weapon of choice during these missions was usually the five-inch HVAR rocket, eight of which could be carried on underwing racks. 
Many of the PBJ-1C and D versions carried a rather ugly, bulbous antenna for an APS-3 search radar sticking out of the upper part of the transparent nose. On the PBJ-1H and J, the APS-3 search radar antenna was usually housed inside a ventral or wingtip radome. Some PBJ-1Js had their top turrets removed to save weight, especially towards the end of the war when Japanese fighters had become relatively scarce.
After World War Two, some PBJs were stationed at the navy's rocket laboratory at Inyokern, California, the present-day Naval Air Weapons Station China Lake, to test various air-to-ground rockets and arrangements, including a twin-barrel nose arrangement that could fire ten spin-stabilized five inch rockets in one salvo.
Royal Air Force.
The Royal Air Force (RAF) was an early customer for the B-25 via Lend-Lease. The RAF was the only force to use the B-25 on raids against Europe from bases in the United Kingdom, as the USAAF used the Martin B-26 Marauder and Boeing B-17 Flying Fortress for this purpose instead. 
The first Mitchells were designated Mitchell I by the RAF and were delivered in August 1941, to No 111 Operational Training Unit based in the Bahamas. These bombers were used exclusively for training and familiarization and never achieved operational status. The B-25Cs and Ds were designated Mitchell II, altogether, 167 B-25Cs and 371 B-25Ds were delivered to the RAF. 
A total of 93 Mitchell Is and IIs had been delivered to the RAF by the end of 1942 and served with No. 2 Group RAF, the RAF's tactical medium bomber force. The first RAF operation with the Mitchell II took place on 22 January 1943, when six aircraft from No. 180 Squadron RAF attacked oil installations at Ghent. After the invasion of Europe, all four Mitchell squadrons moved to bases in France and Belgium (Melsbroek) to support Allied ground forces. The British Mitchell squadrons were joined by No. 342 (Lorraine) Squadron of the French Air Force in April 1945. 
No 305 (Polish) Squadron flew Mitchell IIs from September to December 1943 before converting to Mosquitos. In addition to the 2nd Group, the B-25 was used by various second-line RAF units in the UK and abroad. In the Far East, No. 3 PRU, which consisted of Nos. 681 and 684 Squadrons, flew the Mitchell (primarily Mk IIs) on photographic reconnaissance sorties. 
The RAF was allocated 316 B-25Js which entered service as the Mitchell III. Deliveries took place between August 1944 and August 1945. However, only about 240 of these bombers actually reached Britain, with some being diverted to No. 111 OTU in the Bahamas, some crashing during delivery and some being retained in the United States.
Royal Canadian Air Force.
The Royal Canadian Air Force (RCAF) was an important user of the B-25 Mitchell, although most of the RCAF use of the 162 Mitchells delivered was postwar. The first B-25s for the RCAF had originally been diverted to Canada from RAF orders. These included one Mitchell I, 42 Mitchell IIs, and 19 Mitchell IIIs. No 13 (P) Squadron was formed unofficially at RCAF Rockcliffe in May 1944. They operated Mitchell IIs on high-altitude aerial photography sorties. The No. 5 OTU (Operational Training Unit) at Boundary Bay, British Columbia and Abbotsford, British Columbia operated the Mitchell in the training role together with B-24 Liberators for Heavy Conversion as part of the BCATP. The RCAF retained the Mitchell until October 1963. 
No 418 (Auxiliary) Squadron received its first Mitchell IIs in January 1947. It was followed by No 406 (auxiliary) which flew Mitchell IIIs from April 1947 to June 1958. No 418 Operated a mix of IIs and IIIs until March 1958. No 12 Squadron of Air Transport Command also flew Mitchell IIIs along with other types from September 1956 to November 1960. In 1951, the RCAF received an additional 75 B-25Js from USAF stocks to make good attrition and to equip various second line units.
Royal Australian Air Force.
The Australians got Mitchells by the spring of 1944. The joint Australian-Dutch No. 18 (Netherlands East Indies) Squadron RAAF had more than enough Mitchells for one squadron so the surplus went to re-equip the RAAF's No. 2 Squadron, replacing their Beauforts.
Dutch Air Force.
During World War II, the Mitchell served in fairly large numbers with the Air Force of the Dutch government-in-exile. They participated in combat both in the East Indies as well as on the European front. On 30 June 1941, the Netherlands Purchasing Commission, acting on behalf of the Dutch government-in-exile in London, signed a contract with North American Aviation for 162 B-25C aircraft. The bombers were to be delivered to the Netherlands East Indies to help deter any Japanese aggression into the region. 
In February 1942, the British Overseas Airways Corporation (BOAC) agreed to ferry 20 of the Dutch B-25s from Florida to Australia via Africa and India, and an additional ten via the South Pacific route from California. During March, five of the bombers on the Dutch order had reached Bangalore, India and 12 had reached Archerfield in Australia. It was agreed that the B-25s in Australia would be used as the nucleus of a new squadron, designated No. 18. This squadron would be staffed jointly by Australian and Dutch aircrews plus a smattering of aircrews from other nations, but would operate at least initially under Royal Australian Air Force command. 
The B-25s of No. 18 Squadron would be painted with the Dutch national insignia (at this time a rectangular Netherlands flag) and would carry NEIAF serials. Discounting the ten "temporary" B-25s delivered to 18 Squadron in early 1942, a total of 150 Mitchells were taken on strength by the NEIAF, 19 in 1942, 16 in 1943, 87 in 1944, and 28 in 1945. They flew bombing raids against Japanese targets in the East Indies. In 1944, the more capable B-25J Mitchell replaced most of the earlier C and D models. 
In June 1940, No. 320 Squadron RAF had been formed from personnel formerly serving with the Royal Dutch Naval Air Service who had escaped to England after the German occupation of the Netherlands. Equipped with various British aircraft, No. 320 Squadron flew anti-submarine patrols, convoy escort missions, and performed air-sea rescue duties. They acquired the Mitchell II in September 1943, performing operations over Europe against gun emplacements, railway yards, bridges, troops and other tactical targets. They moved to Belgium in October 1944, and transitioned to the Mitchell III in 1945. No. 320 Squadron was disbanded in August 1945. Following the war, B-25s were used in a vain attempt of the Dutch to retain control of Indonesia.
Soviet Air Force.
The U.S. supplied 862 B-25 (of B, D, G, and J types) aircraft to the Soviet Union under lend-lease during the Second World War via the Alaska–Siberia ALSIB ferry route. 
Other damaged aircraft arrived or crashed in the Far East of Russia, and one Doolittle Raid aircraft landed there short of fuel after attacking Japan. It is not known what happened to these latter aircraft. In general, the B-25 was operated as a ground-support and tactical daylight bomber (as similar Douglas A-20 Havocs were used). It saw action in fights from Stalingrad (with B/D models) to the German surrender during May 1945 with (G/J types).
China Air Force.
Well over 100 B-25Cs and Ds were supplied to the Nationalist Chinese during the Second World War. In addition, a total of 131 B-25Js were supplied to China under Lend-Lease. 
The four squadrons of the 1st BG (1st, 2nd, 3rd, and 4th) of the 1st Medium Bomber Group were formed during the War. They formerly operated Russian-built Tupolev SB bombers, then transferred to the B-25. The 1st BG was under the command of CACW (Chinese-American Composite Wing) while operating B-25. Following the end of the war in the Pacific, these four bombardment squadrons were established to fight against the Communist insurgency that was rapidly spreading throughout the country. During the civil war, Chinese Mitchells fought alongside de Havilland Mosquitos. 
In December 1948, the Nationalists were forced to move to the island of Taiwan, taking many of their Mitchells with them. However, some B-25s were left behind and were impressed into service with the air force of the new People's Republic of China.
Brazilian Air Force.
During the war, the Força Aérea Brasileira (FAB) received a few B-25s under Lend-Lease. Brazil declared war against the Axis powers in August 1942 and participated in the war against the U-boats in the southern Atlantic. The last Brazilian B-25 was finally declared surplus in 1970.
Free French.
At least 21 Mitchell IIIs were issued by the Royal Air Force to No 342 Squadron, which was made up primarily of Free French aircrews. 
Following the liberation of France, this squadron was transferred to the newly formed French air force (Armée de l'Air) as GB I/20 Lorraine. These aircraft were operated by GB I/20 after the war, some being converted from bomber configuration into fast VIP transports. They were finally struck off charge in June 1947.
Empire State Building incident.
At 9:40 on Saturday, 28 July 1945, a USAAF B-25D crashed in thick fog into the north side of the Empire State Building between the 79th and 80th floors. Fourteen people died – eleven in the building and the three occupants of the aircraft including the pilot, Colonel William Smith. Betty Lou Oliver, an elevator attendant, survived the impact and a subsequent uncontrolled descent in the elevator.
Partly as a result of this incident, Towers 1 and 2 of the World Trade Center were designed to withstand an aircraft impact. However, this design was based on an impact by a Boeing 707 aircraft in common use in the late 1960s and early 1970s, not the larger Boeing 767, two of which, (American Airlines Flight 11 and United Airlines Flight 175), struck the towers on September 11, 2001, resulting in their subsequent collapse.
Variants.
Trainer variants.
Most models of the B-25 were used at some point as training aircraft.

</doc>
<doc id="4219" url="http://en.wikipedia.org/wiki?curid=4219" title="British Open (disambiguation)">
British Open (disambiguation)

The British Open is the Open Championship men's golf tournament.
British Open may also refer to:

</doc>
<doc id="4224" url="http://en.wikipedia.org/wiki?curid=4224" title="Bobby Charlton">
Bobby Charlton

Sir Robert "Bobby" Charlton CBE (born 11 October 1937) is an English former football player, regarded as one of the greatest midfielders of all time, and an essential member of the England team who won the World Cup and also won the Ballon d'Or in 1966. He played almost all of his club football at Manchester United, where he became renowned for his attacking instincts and passing abilities from midfield and his ferocious long-range shot. He was also well known for his fitness and stamina. His elder brother Jack, who was also in the World Cup-winning team, is a former defender for Leeds United and international manager.
Born in Ashington, Northumberland, Charlton made his debut for the Manchester United first-team in 1956, and over the next two seasons gained a regular place in the team, during which time he survived the Munich air disaster of 1958 after being rescued by Harry Gregg. After helping United to win the Football League in 1965, he won a World Cup medal with England in 1966 and another Football League title with United the following year. In 1968, he captained the Manchester United team that won the European Cup, scoring two goals in the final to help his team be the first English side to win the competition. He has scored more goals for England and United than any other player. Charlton held the record for most appearances for Manchester United (758), before being surpassed by Ryan Giggs.
He was selected for four World Cups (1958, 1962, 1966, and 1970), and helped England to win the competition in 1966. At the time of his retirement from the England team in 1970, he was the nation's most capped player, having turned out 106 times at the highest level. This record has since been eclipsed by Bobby Moore, Peter Shilton, David Beckham, Steven Gerrard and Ashley Cole.
He left Manchester United to become manager of Preston North End for the 1973–74 season. He changed to player-manager the following season. He next accepted a post as a director with Wigan Athletic, then became a member of Manchester United's board of directors in 1984 and remains one as of May 2014.
Early life.
Charlton is related to several professional footballers on his mother's side of the family: his uncles were Jack Milburn (Leeds United and Bradford City), George Milburn (Leeds United and Chesterfield), Jim Milburn (Leeds United and Bradford City) and Stan Milburn (Chesterfield, Leicester City and Rochdale), and legendary Newcastle United and England footballer Jackie Milburn, was his mother's cousin. However, Charlton credits much of the early development of his career to his grandfather Tanner and his mother Cissie. His elder brother, Jack, initially went to work applying to the Police Service before also becoming a professional footballer with Leeds United.
Club career.
On 9 February 1953, then a Bedlington Grammar School pupil, Charlton was spotted playing for East Northumberland schools by Manchester United chief scout Joe Armstrong. Charlton went on to play for England schoolboys and the 15-year-old signed with United on 1 January 1953, along with Wilf McGuinness, also aged 15. Initially his mother was reluctant to let him commit to an insecure football career, so he began an apprenticeship as an electrical engineer; however he went on to turn professional in October 1954.
Charlton became one of the famed Busby Babes, the collection of talented footballers who emerged through the system at Old Trafford in the 1940s, 1950s and 1960s as Matt Busby set about a long-term plan of rebuilding the club after the Second World War. He worked his way through the pecking order of teams, scoring regularly for the youth and reserve sides before he was handed his first team debut against Charlton Athletic in October 1956. At the same time, he was doing his National service with the Royal Army Ordnance Corps in Shrewsbury, where Busby had advised him to apply as it meant he could still play for Manchester United at the weekend. Also doing his army service in Shrewsbury at the same time was his United team-mate Duncan Edwards.
Charlton played 14 times for United in that first season, scoring twice on his debut and managing a total of 12 goals in all competitions, and including a hat-trick in a 5–1 away win over Charlton Athletic in the February. United won the league championship but were denied the 20th century's first "double" when they controversially lost the 1957 FA Cup Final to Aston Villa. Charlton, still only 19, was selected for the game, which saw United goalkeeper Ray Wood carried off with a broken cheekbone after a clash with Villa centre forward Peter McParland. Though Charlton was a candidate to go in goal to replace Wood (in the days before substitutes, and certainly before goalkeeping substitutes), it was teammate Jackie Blanchflower who ended up between the posts.
Charlton was an established player by the time the next season was fully underway, which saw United, as current League champions, become the first English team to compete in the European Cup. Previously, the Football Association had scorned the competition but United made progress, reaching the semi-finals where they lost to holders Real Madrid. Their reputation was further enhanced the next season as they reached the quarter finals to play Red Star Belgrade. In the first leg at home, United won 2–1. The return in Yugoslavia saw Charlton score twice as United stormed 3–0 ahead, although the hosts came back to earn a 3–3 draw. However, United maintained their aggregate lead to reach the last four and were in jubilant mood as they left to catch their flight home, thinking of an important League game against Wolves at the weekend. But the plane, carrying 44 passengers and crew (including the 17-strong Manchester United squad) crashed on take-off after re-fuelling at Munich. Charlton survived with minor injuries, but 23 people (eight of them Manchester United players) died as a result of their injures in the crash. Of the eight other players who survived, two of them were injured so badly that they never played again.
Further success with Manchester United came at last when they beat Leicester City 3–1 in the FA Cup final of 1963, with Charlton finally earning a winners' medal in his third final. Busby's post-Munich rebuilding programme continued to progress with two League championships within three seasons, with United taking the title in 1965 and 1967. A successful (though trophyless) season with Manchester United had seen him take the honours of "Football Writers' Association Footballer of the Year" and "European Footballer Of The Year" into the competition.
In 1968, Manchester United reached the European Cup final, ten seasons after Munich. Even though other clubs had taken part in the competition in the intervening decade, the team which got to this final was still the first English side to do so. On a highly emotional night at Wembley, Charlton scored twice in a 4–1 win after extra time against Benfica and, as United captain, lifted the trophy.
During the early 1970s, Manchester United were no longer competing among the top teams in England, and at several stages were battling against relegation. At times, Charlton was not on speaking terms with United's other superstars George Best and Denis Law, and Best refused to play in Charlton's testimonial match against Celtic, saying that "to do so would be hypocritical". Charlton left Manchester United at the end of the 1972–73 season, having scored 249 goals and set a club record of 758 appearances, a record which Ryan Giggs broke in the 2008 UEFA Champions League Final. His goalscoring record remains intact. 
His last game was against Chelsea at Stamford Bridge on 28 April 1973, and before the game the BBC cameras for "Match of the Day" captured the Chelsea chairman handing Charlton a commemorative cigarette case. The match ended in a 1-0 defeat. His final goal came a month earlier, on 31 March, in a 2-0 win at Southampton, also in the First Division.
International career.
Charlton's emergence as the country's leading young football talent was completed when he was called up to join the England squad for a British Home Championship game against Scotland at Hampden Park on 19 April 1958, just over two months after he had survived the Munich air disaster. 
Charlton was handed his debut as England romped home 4–0, with the new player gaining even more admirers after scoring a magnificent thumping volley dispatched with authority after a cross by the left winger Tom Finney. He scored both goals in his second game as England beat Portugal 2–1 in a friendly at Wembley; and overcame obvious nerves on a return to Belgrade to play his third match against Yugoslavia. England lost that game 5–0 and Charlton played poorly.
1958 World Cup.
He was selected for the squad which competed at the 1958 World Cup in Sweden, but didn't kick a ball, something at which critics expressed surprise and bewilderment, even allowing for his lacklustre performance in Belgrade.
In 1959 he scored a hat-trick as England demolished the US 8–1; and his second England hat-trick came in 1961 in an 8–0 thrashing of Mexico. He also managed to score in every British Home Championship tournament he played in except 1963 in an association with the tournament which lasted from 1958 to 1970 and included 16 goals and ten tournament victories (five shared).
1962 World Cup.
He played in qualifiers for the 1962 World Cup in Chile against Luxembourg and Portugal and was named in the squad for the finals themselves. His goal in the 3–1 group win over Argentina was his 25th for England in just 38 appearances, and he was still only 24 years old, but his individual success could not be replicated by that of the team, which was eliminated in the quarter final by Brazil, who went on to win the tournament.
By now, England were coached by Alf Ramsey who had managed to gain sole control of the recruitment and team selection procedure from the committee-based call-up system which had lasted up to the previous World Cup. Ramsey had already cleared out some of the older players who had been reliant on the loyalty of the committee for their continued selection – it was well known that decorum on the pitch at club level had been just as big a factor in playing for England as ability and form. Luckily for Charlton, he had all three.
A hat-trick in the 8–1 rout of Switzerland in June 1963 took Charlton's England goal tally to 30, equalling the record jointly held by Tom Finney and Nat Lofthouse and Charlton's 31st goal against Wales in October the same year gave him the record alone. Charlton's role was developing from traditional inside-forward to what today would be termed an attacking midfield player, with Ramsey planning to build the team for the 1966 World Cup around him. When England beat the USA 10-0 in a friendly on 27 May 1964, he scored one goal, his 33rd at senior level for England.
His goals became a little less frequent, and indeed Jimmy Greaves, playing purely as a striker, would overtake Charlton's England tally in October 1964. Nevertheless, he was still scoring and creating freely and as the tournament was about to start, he was expected to become one of its stars and galvanise his established reputation as one of the world's best footballers.
1966 World Cup.
England drew the opening game of the tournament 0–0 with Uruguay, and Charlton scored the first goal in the 2–0 win over Mexico. This was followed by an identical scoreline against France, allowing England to qualify for the quarter finals.
England defeated Argentina 1–0 – the game was the only one in which Charlton received a caution – and faced Portugal in the semi finals. This turned out to be one of Charlton's most important games for England.
Charlton opened the scoring with a crisp side-footed finish after a run by Roger Hunt had forced the Portuguese goalkeeper out of his net; his second was a sweetly struck shot after a run and pull-back from Geoff Hurst. Charlton and Hunt were now England's joint-highest scorers in the tournament with three each, and a final against West Germany beckoned.
The final turned out to be one of Charlton's quieter days; he and a young Franz Beckenbauer effectively marked each other out of the game. England won 4–2 after extra time.
Euro 1968.
Charlton's next England game was his 75th as England beat Northern Ireland; 2 caps later and he had become England's second most-capped player, behind the veteran Billy Wright, who was approaching his 100th appearance when Charlton was starting out and ended with 105 caps.
Weeks later he scored his 45th England goal in a friendly against Sweden, breaking the record of 44 set the previous year by Jimmy Greaves. He was then in the England team which made it to the semi-finals of the 1968 European Championships where they were knocked out by Yugoslavia in Florence. During the match Charlton struck a Yugoslav post. England defeated the Soviet Union 2–0 in the third place match.
In 1969, Charlton was awarded the OBE for services to football. More milestones followed as he won his 100th England cap on 21 April 1970 against Northern Ireland, and was made captain by Ramsey for the occasion. Inevitably, he scored. This was his 48th goal for his country – his 49th and final goal would follow a month later in a 4–0 win over Colombia during a warm-up tour for the 1970 World Cup, designed to get the players adapted to altitude conditions. Charlton's inevitable selection by Ramsey for the tournament made him the first – and still, to date, only – England player to feature in four World Cup squads.
1970 World Cup.
Shortly before the World Cup Charlton was involved in the Bogotá Bracelet incident in which he and Bobby Moore were accused of stealing a bracelet from a jewellery store. Moore was later arrested and detained for four days before being granted a conditional release, while Charlton was not arrested.
England began the tournament with two victories in the group stages, plus a memorable defeat against Brazil. Charlton played in all three, though was substituted for Alan Ball in the final game of the group against Czechoslovakia. Ramsey, confident of victory and progress to the quarter final, wanted Charlton to rest.
England duly reached the last eight where they again faced West Germany. Charlton controlled the midfield and suppressed Franz Beckenbauer's runs from deep as England coasted to a 2–0 lead. Beckenbauer pulled a goal back for the Germans and Ramsey replaced the ageing and tired Charlton with Colin Bell who further tested the German keeper Maier and also provided a great cross for Geoff Hurst who uncharacteristically squandered the chance. West Germany, who had a habit of coming back from behind, eventually scored twice – a back header from Uwe Seeler made it 2–2. In extra-time, Geoff Hurst had a goal mysteriously ruled out after which Gerd Müller's goal won the match 2-3. England were out and, after a record 106 caps and 49 goals, Charlton decided to end his international career at the age of 32. On the flight home from Mexico, he asked Ramsey not to consider him again. His brother Jack, two years his senior but 71 caps his junior, did likewise.
Despite popular opinion the substitution did not change the game as Franz Beckenbauer had scored before Charlton left the field, hence Charlton had failed to cancel out the German. Charlton himself conceded that the substitution did not affect the game in a BBC documentary. His caps record lasted until 1973 when Bobby Moore overtook him, and Charlton currently lies sixth in the all-time England appearances list behind Ashley Cole Moore, Steven Gerrard, David Beckham and Peter Shilton, whose own England career began in the first game after Charlton's had ended. As of May 2014, Charlton's goalscoring record still stands.
Munich air disaster.
The aeroplane which took the United players and staff home from Zemun Airport needed to stop in Munich to refuel. This was carried out in worsening weather, and by the time the refuelling was complete and the call was made for the passengers to re-board the aircraft, the wintry showers had taken hold and snow had settled heavily on the runway and around the airport. There were two aborted take-offs which led to concern on board, and the passengers were advised by a stewardess to disembark again while a minor technical error was fixed.
The team was back in the airport terminal barely ten minutes when the call to reconvene on the plane came, and a number of passengers began to feel nervous. Charlton and teammate Dennis Viollet swapped places with Tommy Taylor and David Pegg, who had decided they would be safer at the back of the plane.
The plane clipped the fence at the end of the runway on its next take-off attempt and a wing tore through a nearby house, setting it alight. The wing and part of the tail came off and hit a tree and a wooden hut, the plane spinning along the snow until coming to a halt. It had been cut in half.
Charlton, strapped into his seat, had fallen out of the cabin and when United goalkeeper Harry Gregg (who had somehow got through a hole in the plane unscathed and begun a one-man rescue mission) found him, he thought he was dead. That said, he grabbed both Charlton and Viollet by their trouser waistbands and dragged them away from the plane in constant fear that it would explode. Gregg returned to the plane to try to help the appallingly injured Busby and Blanchflower, and when he turned around again, he was relieved to see that Charlton and Viollet, both of whom he had presumed to be dead, had got out of their detached seats and were looking into the wreckage.
Charlton suffered cuts to his head and severe shock and was in hospital for a week. Seven of his teammates had perished at the scene, including Taylor and Pegg, with whom he and Viollet had swapped seats prior to the fatal take-off attempt. Club captain Roger Byrne was also killed, along with Mark Jones, Billy Whelan, Eddie Colman and Geoff Bent. Duncan Edwards died a fortnight later from the injuries he had sustained. In total, the crash claimed 23 lives. Initially, ice on the wings was blamed, but a later inquiry declared that slush on the runway had made a safe take-off almost impossible.
Charlton was the first injured survivor to leave hospital. Harry Gregg and Bill Foulkes were not hospitalized since they escaped uninjured. He arrived back in England on 14 February 1958, eight days after the crash. As he convalesced with family in Ashington, he spent some time kicking a ball around with local youths, and a famous photograph of him was taken. He was still only 20 years old, yet now there was an expectation that he would help with the rebuilding of the club as Busby's aides tried to piece together what remained of the season. He returned to playing in an FA cup tie against West Bromwich Albion on 1 March; the game was a draw and United won the replay 1–0.
Not unexpectedly, United went out of the European Cup to Milan in the semi-finals to a 5–2 aggregate defeat and fell behind in the League. Yet somehow they reached their second consecutive FA Cup final, and the big day at Wembley coincided with Busby's return to work. However, his words could not inspire a side which was playing on a nation's goodwill and sentiment, and Nat Lofthouse scored twice to give Bolton Wanderers side a 2–0 win.
Management career and directorships.
Charlton became the manager of Preston North End in 1973, signing his former United and England team-mate Nobby Stiles as player-coach. His first season ended in relegation and although he began playing again he left Preston early in the 1975–76 season after a disagreement with the board over the transfer of John Bird to Newcastle United. He was awarded the CBE that year and began a casual association with the BBC for punditry on matches which continued for many years. In early 1976, he scored once in 3 league appearances for Waterford United.
He joined Wigan Athletic as a director, and was briefly caretaker manager there in 1983. He then spent some time playing in South Africa. He also built up several businesses in areas such as travel, jewellery and hampers, and ran soccer schools in the UK, the US, Canada, Australia and China. In 1984, he was invited to become member of the board of directors at Manchester United, partly because of his football knowledge and partly because it was felt that the club needed a "name" on the board after the resignation of Sir Matt Busby. He remains a director of Manchester United as of 2014 and his continued presence was a factor in placating many fans opposed to the club's takeover by Malcolm Glazer.
Personal life and retirement.
He met his wife, Norma Ball, at an ice rink in Manchester in 1959 and they married in 1961. They have two daughters – Suzanne and Andrea. Suzanne was a weather forecaster for the BBC during the 1990s. They now have grandchildren, including Suzanne's son Robert, who is named in honour of his grandfather.
In 2007, while publicising his forthcoming autobiography, Charlton revealed that he had a long-running feud with his brother, Jack. They have rarely spoken since a falling-out between his wife Norma and his mother Cissie (who died on 25 March 1996 at the age of 83). Charlton did not see his mother after 1992 as a result of the feud.
Jack presented him with his BBC Sports Personality of the Year Lifetime Achievement Award on 14 December 2008. He said that he was 'knocked out' as he was presented the award by his brother. He received a standing ovation as he stood waiting for his prize.
Charlton helped to promote Manchester's bids for the 1996 and 2000 Olympic Games and the 2002 Commonwealth Games, England's bid for the 2006 FIFA World Cup and London's successful bid for the 2012 Summer Olympics. He received a knighthood in 1994 and was an Inaugural Inductee to the English Football Hall of Fame in 2002. On accepting his award he commented "I'm really proud to be included in the National Football Museum's Hall of Fame. It's a great honour. If you look at the names included I have to say I couldn't argue with them. They are all great players and people I would love to have played with." He is also the (honorary) president of the National Football Museum, an organisation about which he said "I can't think of a better Museum anywhere in the world." On 14 December 2008 Charlton was awarded the prestigious BBC Sports Personality of the Year Lifetime Achievement Award.
On 2 March 2009, Charlton was given the freedom of the city of Manchester, stating "I'm just so proud, it's fantastic. It's a great city. I have always been very proud of it." 
Charlton is involved in a number of charitable activities including fund raising for cancer hospitals. Charlton became involved in the cause of land mine clearance after visits to Bosnia and Cambodia and supports the Mines Advisory Group as well as founding his own charity Find a Better Way which funds research into improved civilian landmine clearance.
In January 2011 Charlton was voted the 4th greatest Manchester United player of all time by the readers of Inside United and ManUtd.com, behind Ryan Giggs (who topped the poll), Eric Cantona and George Best.
He is a member of the Laureus World Sports Academy. On 6 February 2012 Sir Bobby Charlton was taken to hospital after falling ill, and subsequently had a gallstone removed. This prevented him from collecting a Lifetime Achievement award at the Laureus World Sports Awards.
In the 2011 film "United", centred on the successes of the Busby Babes and the decimation of the team in the Munich crash, Charlton was portrayed by actor Jack O'Connell.

</doc>
<doc id="4227" url="http://en.wikipedia.org/wiki?curid=4227" title="Barry Lyndon">
Barry Lyndon

Barry Lyndon is a 1975 British-American period drama film written, produced and directed by Stanley Kubrick, based on the 1844 novel "The Luck of Barry Lyndon" by William Makepeace Thackeray. It stars Ryan O'Neal, Marisa Berenson, Patrick Magee and Hardy Krüger. The film recounts the exploits of a fictional 18th-century Irish adventurer. Most of the exteriors were shot on location in Ireland. At the 1975 Academy Awards, the film won four Oscars in production categories.
The film, which had a modest commercial success and a mixed critical reception on initial release, is now regarded as one of Kubrick's finest films. In numerous polls, such as "Village Voice" (1999), "Sight and Sound" (2002), and "Time" magazine (2005), it has been rated one of the greatest films ever made. Director Martin Scorsese has cited "Barry Lyndon" as his favorite Kubrick film. Quotations from its script have also appeared in such disparate works as Ridley Scott's "The Duellists", Scorsese's "The Age of Innocence", Wes Anderson's "Rushmore" and Lars von Trier's "Dogville".
Plot.
Act I.
An omniscient (though unreliable) narrator (voiced by Michael Hordern) states that in 1750s Ireland, the father of Redmond Barry (Ryan O'Neal) is killed in a duel over a disputed horse sale. The widow (Marie Kean), disdaining offers of marriage, devotes herself to her only son.
As a teenager, Barry falls in love with his older cousin, Nora Brady (Gay Hamilton). Though she seduces him, she later drops Barry (who has no money) for the well-off English Captain John Quin (Leonard Rossiter). Nora and her family plan to relieve their poverty with an advantageous marriage, but Barry refuses to accept the situation and shoots Quin in a duel.
Barry flees to Dublin, but en route is robbed of purse and equipment by Captain Feeney (Arthur O'Sullivan), an infamous highwayman. Broke, Barry joins the British army, whereupon he reunites with Captain Grogan (Godfrey Quigley), a family friend, who informs him that, in fact, he did not kill Quin — Barry's dueling pistol was loaded with tow. The duel was staged by Nora's family to get rid of Barry so that their family finances would be secured through the marriage of Nora and Quin.
Barry's regiment is sent to Germany to fight in the Seven Years' War, where Captain Grogan is fatally wounded by the French in a skirmish at the Battle of Minden. Barry deserts the army, stealing an officer courier's uniform, horse, and identification papers. En route to neutral Holland he encounters the Prussian Captain Potzdorf (Hardy Krüger), who, seeing through his disguise, offers him the choice of being turned back over to the British where he will be shot as a deserter, or enlisting in the Prussian army. Barry enlists in his second army and later receives a special commendation from Frederick the Great for saving Potzdorf's life in a battle.
After the war ends in 1763, Barry is employed by Captain Potzdorf's uncle in the Prussian Ministry of Police to become the servant of the Chevalier de Balibari (Patrick Magee), a professional gambler. The Prussians suspect he is a spy and send Barry as an undercover agent to verify this. Barry reveals himself to the Chevalier right away and they become confederates cheating at cards.
After he and the Chevalier cheat the Prince of Tübingen at the cardtable, the Prince accuses the Chevalier (without proof) and refuses to pay his debt unless the Chevalier demands satisfaction. When Barry relays this to his Prussian handlers, they (still suspecting that the Chevalier is a spy) are wary of allowing another meeting between the Chevalier and the Prince. So, the Prussians arrange for the Chevalier to be expelled from the country. Barry conveys this plan to the Chevalier, who flees in the night. The next morning, Barry, under disguise as the Chevalier, is escorted from Prussian territory by Prussian officers.
For the next few years, Barry and the Chevalier travel the spas and parlors of Europe, profiting from their gambling with Barry enforcing reluctant debtors with a duel. Seeing that his life is going nowhere, Barry decides to marry into wealth. At a gambling table in Belgium, he encounters the beautiful and wealthy Countess of Lyndon (Marisa Berenson). He seduces and later marries her after the death of her elderly husband, Sir Charles Lyndon (Frank Middlemass).
Act II.
In 1773, Barry takes the Countess' last name in marriage and settles in England to enjoy her wealth, still with no money of his own. Lord Bullingdon (Dominic Savage), Lady Lyndon's 10-year-old son by Sir Charles, does not approve of the marriage and quickly comes to hate Barry, aware that Barry is merely a "common opportunist" and does not love his mother. The Countess bears Barry a son, Bryan Patrick, but the marriage is unhappy: Barry is openly unfaithful and enjoys spending his wife's money in self-indulgent spending sprees while keeping his wife in dull seclusion.
Some years later, Barry's mother comes to live with him at the Lyndon estate. She warns her son that his position is precarious: If Lady Lyndon were to die, all her wealth would go to her first-born son Lord Bullingdon (now a young man, played by Leon Vitali), leaving Barry penniless. Barry's mother advises him to obtain a noble title to protect himself. To further this goal, he cultivates the acquaintance of the influential Lord Wendover (André Morell) and begins to expend even larger sums of money to ingratiate himself to high society. All this effort is wasted, however, during a birthday party for Lady Lyndon, where Lord Bullingdon announces his hatred of his stepfather and his intention to leave the family estate for as long as his mother remains married to Barry. Angered, Barry assaults Bullingdon before the guests. This public display of cruelty loses Barry all the powerful friends he has worked so hard to make and he is shunned socially. Bullingdon makes good on his announcement and leaves the estate and England itself for parts unknown.
In contrast to his mistreatment of his stepson, Barry proves a compassionate and doting father to Bryan, with whom he spends all his time after Bullingdon's departure. He cannot refuse his son anything, and succumbs to Bryan's insistence on receiving a full-grown horse for his ninth birthday. The spoiled Bryan disobeys his parents' direct instructions that Bryan ride the horse only in the presence of his father, and is thrown by the horse. Bryan dies a few days later from his injuries.
The grief-stricken Barry turns to alcohol, while Lady Lyndon seeks solace in religion, assisted by the Reverend Samuel Runt (Murray Melvin), who had been tutor first to Lord Bullingdon and then to Bryan. Left in charge of the families' affairs while Barry and Lady Lyndon grieve, Barry's mother dismisses the Reverend, both because the family no longer needs (nor can afford, due to Barry's spending debts) a tutor and for fear that his influence worsens Lady Lyndon's condition. Plunging even deeper into grief, Lady Lyndon later attempts suicide (though she ingests only enough poison to make herself ill). The Reverend and the family's accountant and emissary Graham (Philip Stone) then seek out Lord Bullingdon. Upon hearing of these events, Lord Bullingdon returns to England where he finds Barry in a local tavern getting drunk and mourning the loss of his son rather than being with Lady Lyndon. Bullingdon demands "satisfaction" (a challenge to a duel) for Barry's public assault.
The duel with pistols is held in an abandoned chapel. A coin-toss gives Bullingdon the right of first fire, but his pistol misfires as it is being cocked. Barry, reluctant to shoot Bullingdon, magnanimously fires into the ground, but the unmoved Bullingdon refuses to let the duel end. In the second round, Bullingdon shoots Barry in his left leg. At a nearby inn, a surgeon informs Barry that the leg will need to be amputated below the knee if he is to survive.
While Barry is recovering, Bullingdon takes control of the estate. He sends a very nervous Graham to the inn with a proposition: Bullingdon will grant Barry an annuity of 500 guineas per year for life on the conditions that he leave England forever and end his marriage to Lady Lyndon. Otherwise, with his credit and bank accounts exhausted, Barry's creditors and bill collectors will assuredly see that he is jailed. Defeated, Barry accepts. The narrator states that Barry goes first to Ireland with his mother, then to the European continent to resume his former profession of gambler (though without his former success) and that he never sees Lady Lyndon again. The final scene (set in December 1789) shows the middle-aged Lady Lyndon signing Barry's annuity cheque as Bullingdon looks on.
Cast.
Critic Tim Robey suggests, that the film "makes you realise that the most undervalued aspect of Kubrick's genius could well be his way with actors." He adds that the supporting cast is a "glittering procession of cameos, not from star names but from vital character players."
The cast featured Leon Vitali as the older Lord Bullingdon, who would then become Kubrick's personal assistant, working as the casting director on his following films, and supervising film-to-video transfers for Kubrick. Their relationship lasted until Kubrick's death. The film's cinematographer, John Alcott, appears at the men's club in the non-speaking role of the man asleep in a chair near the title character when Lord Bullingdon challenges Barry to a duel. Kubrick's daughter Vivian also appears (in an uncredited role) as a guest at Bryan's birthday party.
Kubrick stalwarts Patrick Magee (who had played the handicapped writer in "A Clockwork Orange") and Philip Stone (who had played Alex's father in "A Clockwork Orange", and would go on to play the dead caretaker Grady in "The Shining") are featured as the Chevalier du Balibari and as Graham, respectively.
Production.
Development.
After "", Kubrick made plans for a film about Napoleon Bonaparte. During pre-production, however, Sergei Bondarchuk and Dino De Laurentiis' "Waterloo" was released and subsequently failed at the box office. As a result, Kubrick's financiers pulled their funding for the film and he turned his attention to his next film, "A Clockwork Orange". Subsequently, Kubrick showed an interest in Thackeray's "Vanity Fair" but dropped the project when a serialised version for television was produced. He told an interviewer, "At one time, "Vanity Fair" interested me as a possible film but, in the end, I decided the story could not be successfully compressed into the relatively short time-span of a feature film...as soon as I read "Barry Lyndon" I became very excited about it."
Having garnered Oscar nominations for "Dr. Strangelove", "2001: A Space Odyssey" and "A Clockwork Orange", Kubrick's reputation in the early 1970s was that of "a perfectionist auteur who loomed larger over his movies than any concept or star." His studio—Warner Bros.—was therefore "eager to bankroll" his next project, which Kubrick kept "shrouded in secrecy" from the press partly due to the furor surrounding the controversially violent "A Clockwork Orange" (particularly in the UK) and partly due to his "long-standing paranoia about the tabloid press."
Having felt compelled to set aside his plans for a film about Napoleon Bonaparte, Kubrick set his sights on Thackeray's 1844 "satirical picaresque about the fortune-hunting of an Irish rogue," "Barry Lyndon", the setting of which allowed Kubrick to take advantage of the copious period research he had done for the now-aborted "Napoleon". At the time, Kubrick merely announced only that his next film would star Ryan O'Neal (deemed "a seemingly un-Kubricky choice of leading man") and Marisa Berenson, a former "Vogue" and "Time" magazine cover model, and be shot largely in Ireland. So heightened was the secrecy surrounding the film that "Even Berenson, when Kubrick first approached her, was told only that it was to be an 18th-century costume piece she was instructed to keep out of the sun in the months before production, to achieve the period-specific pallor he required."
Principal photography.
Principal photography took 300 days, from spring 1973 through early 1974, with a break for Christmas.
Many of the film's exteriors were shot in Ireland, playing "itself, England, and Prussia during the Seven Years' War." Drawing inspiration from "the landscapes of Watteau and Gainsborough," Kubrick and cinematographer Alcott also relied on the "scrupulously researched art direction" of Ken Adam and Roy Walker. Alcott, Adam and Walker would be among those who would win Oscars for their "amazing work" on the film.
Several of the interior scenes were filmed in Powerscourt House, a famous 18th-century mansion in County Wicklow, Republic of Ireland. The house was destroyed in an accidental fire several months after filming (November 1974), so the film serves as a record of the lost interiors, particularly the "Saloon" which was used for more than one scene. The Wicklow Mountains are visible, for example, through the window of the Saloon during a scene set in Berlin. Other locations included Kells Priory (the English Redcoat encampment) Blenheim Palace, Castle Howard (exteriors of the Lyndon estate), Corsham Court (various interiors and the music room scene), Petworth House (chapel, and so on.), Stourhead (lake and temple), Longleat, and Wilton House (interior and exterior) in England, Dunrobin Castle (exterior and garden as Spa) in Scotland, Dublin Castle in Ireland (the chevalier's home), Ludwigsburg Palace near Stuttgart and Frederick the Great's Neues Palais at Potsdam near Berlin (suggesting Berlin's main street Unter den Linden as construction in Potsdam had just begun in 1763). Some exterior shots were also filmed at Waterford Castle (now a luxury hotel and golf course) and Little Island, Waterford. Moorstown Castle in Tipperary also featured. Several scenes were filmed at Castletown House outside Carrick-on-Suir, Co.Tipperary, and at Youghal, Co. Cork.
Cinematography.
The film—as with "almost every Kubrick film"—is a "showcase for major innovation in technique." While "2001: A Space Odyssey" had featured "revolutionary effects," and "The Shining" would later feature heavy use of the Steadicam, "Barry Lyndon" saw a considerable number of sequences shot "without recourse to electric light." Cinematography was overseen by director of photography John Alcott (who won an Oscar for his work), and is particularly noted for the technical innovations that made some of its most spectacular images possible. To achieve photography without electric lighting "[for the many densely furnished interior scenes... meant shooting by candlelight," which is known to be difficult in still photography, "let alone with moving images."
Kubrick was "determined not to reproduce the set-bound, artificially lit look of other costume dramas from that time." After "tinkerwith different combinations of lenses and film stock," the production got hold of three super-fast 50mm lenses (Carl Zeiss Planar 50mm f/0.7) developed by Zeiss for use by NASA in the Apollo moon landings," which Kubrick had discovered in his search for low-light solutions. These super-fast lenses "with their huge aperture (the film actually features the lowest f-stop in film history) and fixed focal length" were problematic to mount, and were extensively modified into three versions by Cinema Products Corp. for Kubrick so to gain a wider angle of view, with input from optics expert Richard Vetter of Todd-AO. This allowed Kubrick and Alcott to shoot scenes lit with actual candles to an average lighting volume of only three candela, "recreating the huddle and glow of a pre-electrical age." In addition, Kubrick had the entire film push-developed by one stop.
Although Kubrick's express desire was to avoid electric lighting where possible, most shots were achieved with conventional lenses and lighting, but were lit to deliberately mimic natural light rather than for compositional reasons. In addition to potentially seeming more realistic, these methods also gave a particular period look to the film which has often been likened to 18th-century paintings (which were, of course, depicting a world devoid of electric lighting), in particular owing "a lot to William Hogarth, with whom Thackeray had always been fascinated."
According to critic Tim Robey, the film has a "stately, painterly, often determinedly static quality." For example, to help light some interior scenes, lights were placed outside and aimed through the windows, which were covered in a diffuse material to scatter the light evenly through the room rather than being placed inside for maximum use as most conventional films do. A sign of this method occurs in the scene where Barry duels Lord Bullingdon. Though it appears to be lit entirely with natural light, one can see that the light coming in through the cross-shaped windows in the abandoned chapel appears blue in color, while the main lighting of the scene coming in from the side is not. This is because the light through the cross-shaped windows is daylight from the sun, which when recorded on the film stock used by Kubrick showed up as blue-tinted compared to the incandescent electric light coming in from the side.
Despite such slight tinting effects, this method of lighting not only gave the look of natural daylight coming in through the windows, but it also protected the historic locations from the damage caused by mounting the lights on walls or ceilings and the heat from the lights. This helped the film "fit... perfectly with Kubrick's gilded-cage aesthetic - the film is consciously a museum piece, its characters pinned to the frame like butterflies."
Music.
The film's period setting allowed Kubrick to indulge his penchant for classical music, and the film score uses pieces by Johann Sebastian Bach (an arrangement of the Concerto for violin and oboe in C minor), Antonio Vivaldi (Cello Concerto in E-Minor, a transcription of the Cello Sonata in E Minor RV 40), Giovanni Paisiello, Wolfgang Amadeus Mozart, and Franz Schubert (German Dance No. 1 in C major, Piano Trio in E-Flat, Opus 100 and Impromptu No. 1 in C minor), as well as the Hohenfriedberger March. The piece most associated with the film, however, is the main title music: George Frideric Handel's stately "Sarabande" from the Suite in D minor HWV 437. Originally for solo harpsichord, the versions for the main and end titles are performed very romantically with orchestral strings, harpsichord, and timpani. It is used at various points in the film, in various arrangements, to indicate the implacable working of impersonal fate.
The score also includes Irish folk music, including Seán Ó Riada's song "Women of Ireland", arranged by Paddy Moloney and performed by The Chieftains.
Reception.
The film "was not the commercial success Warner Bros. had been hoping for" within the United States, although it fared better in Europe. This mixed reaction saw the film (in the words of one retrospective review) "greeted, on its release, with dutiful admiration – but not love. Critics... rail against the perceived coldness of Kubrick's style, the film's self-conscious artistry and slow pace. Audiences, on the whole, rather agreed..." This "air of disappointment" factored into Kubrick's decision to next film Stephen King's "The Shining" – a project that would not only please him artistically, but also be more likely to succeed financially. Still, several other critics, including Gene Siskel, praised the film's technical quality and strong narrative, and Siskel himself counted it as one of the five best films of the year.
In recent years, the film has gained a more positive reaction. As of March 2012 it holds a 94% "Certified Fresh" rating on Rotten Tomatoes based on 49 reviews. Roger Ebert added the film to his 'Great Movies' list on September 9, 2009, writing, "It defies us to care, it asks us to remain only observers of its stately elegance", and it "must be one of the most beautiful films ever made."
Awards.
In 1976, at the 48th Academy Awards, the film won four awards, for Best Art Direction (Ken Adam, Roy Walker, Vernon Dixon), Best Cinematography (John Alcott), Best Costume Design (Milena Canonero, Ulla-Britt Söderlund) and Best Musical Score (Leonard Rosenman, "for his arrangements of Schubert and Handel".) Kubrick was nominated three times, for Best Director, Best Picture, and Best Adapted Screenplay.
Kubrick won the British Academy of Film and Television Arts Award for Best Direction. John Alcott won for Best Cinematography. "Barry Lyndon" was also nominated for Best Film, Art Direction, and Costume Design.
Source novel.
Stanley Kubrick based his original screenplay on William Makepeace Thackeray's "The Luck of Barry Lyndon" (republished as the novel "Memoirs of Barry Lyndon, Esq.)," a picaresque tale written and published in serial form in 1844. The serial, which is told in the first person and "edited" by the fictional George Savage FitzBoodle, concerns a member of the Irish gentry trying to become a member of the English aristocracy.
The source novel is written by Lyndon while imprisoned looking back on his life. Lyndon is a notable example of the literary device of the unreliable narrator – throughout the novel the reader is constantly asked to question the veracity of the events described by him. Although later editions dropped the frame device of FitzBoodle's (Thackeray's pseudonym) editions, it is crucial in unmasking Lyndon's narcissism through occasional notes inserted at the bottom of the page noting information that is contradictory or inconsistent in relation to what Lyndon writes elsewhere. As Andrew Sanders argues in his introduction for the Oxford Classics edition, these annotations were relevant to the novel as an ingenious narrative device as Thackeray constantly invites the reader to question Lyndon's version of the events.
Kubrick however felt that using a first-person narrative would not be useful in a film adaptation:
"I believe Thackeray used Redmond Barry to tell his own story in a deliberately distorted way because it made it more interesting. Instead of the omniscient author, Thackeray used the imperfect observer, or perhaps it would be more accurate to say the dishonest observer, thus allowing the reader to judge for himself, with little difficulty, the probable truth in Redmond Barry's view of his life. This technique worked extremely well in the novel but, of course, in a film you have objective reality in front of you all of the time, so the effect of Thackeray's first-person story-teller could not be repeated on the screen. It might have worked as comedy by the juxtaposition of Barry's version of the truth with the reality on the screen, but I don't think that Barry Lyndon should have been done as a comedy."
As in the case of most literary adaptations, Kubrick shortens or in some cases omits characters who were significant in the novel. The time period constituting his escape from the Prussian army to his marriage is given greater detail in the novel than the film.
It is also interesting to note that the film ends much before the novel's ending. At the end of the film, Barry Lyndon survives with an amputated leg from a duel (an incident absent in the novel) and returns to his gambling lifestyle with lesser success while Lady Lyndon pays the debts accumulated during her marriage to Barry, including the sum promised to Redmond in return for leaving the country. Though these events occur in the novel as well, Thackeray also writes that upon Lady Lyndon's death, the sum promised to Barry is cancelled and he becomes destitute eventually winding up in prison for his confidence schemes. It is at this place where Barry writes his memoirs, which end noting that he has to 'eke out a miserable existence, quite unworthy of the famous and fashionable Barry Lyndon'.
At this point Fitz-Boodle writes an epilogue of sorts about Barry's final days, where his only visitor is his mother. He dies after spending 19 years in prison.
Thackeray based the novel on the life and exploits of the Irish rakehell and fortunehunter Andrew Robinson Stoney, who married (and subsequently was divorced by) Mary Eleanor Bowes, the Countess of Strathmore, who became known as "The Unhappy Countess" because of the tempestuous liaison.
The revised version, which is the novel generally known as "Barry Lyndon", was shorter and tighter than the original serialization, and dropped the FitzBoodle, Ed. device. It generally is considered the first "novel without a hero" or novel with an antihero in the English language. Upon its publication in 1856, it was entitled by Thackeray's publisher "The Memoirs of Barry Lyndon, Esq. Of The Kingdom Of Ireland Containing An Account of His Extraordinary Adventures; Misfortunes; His Sufferings In The Service Of His Late Prussian Majesty; His Visits To Many Courts of Europe; His Marriage and Splendid Establishments in England And Ireland; And The Many Cruel Persecutions, Conspiracies And Slanders Of Which He Has Been A Victim".
"Barry Lyndon" departs from its source novel in several ways. In Thackeray's writings, events are related in the first person by Barry himself. A comic tone pervades the work, as Barry proves both a raconteur and an unreliable narrator. Kubrick's film, by contrast, presents the story objectively. Though the film contains voice-over (by actor Michael Hordern), the comments expressed are not Barry's, but those of an omniscient, although not entirely impartial, narrator. This change in perspective alters the tone of the story; Thackeray tells a jaunty, humorous tale, but Kubrick's telling is essentially tragic, albeit with a satirical tone.
Kubrick also changed the plot. The novel does not include a final duel. By adding this episode, Kubrick establishes dueling as the film's central motif: the film begins with a duel where Barry's father is shot dead, and duels recur throughout the film. Also, in Thackeray's novel, the Chevalier de Balibari (played by Patrick Magee in the film) is Barry's long-lost uncle ("Balibari" being a gentrified version of "Bally Barry," the family's home), and by marrying into the Lyndons, Barry intends to regain his family fortune (his ancestors were dispossessed by the Lyndons). In the film, Kubrick eliminated these familial connections from the story.

</doc>
<doc id="4230" url="http://en.wikipedia.org/wiki?curid=4230" title="Cell (biology)">
Cell (biology)

The cell (from Latin "cella", meaning "small room") is the basic structural, functional and biological unit of all known living organisms. Cells are the smallest unit of life that can replicate independently, and are often called the "building blocks of life". The study of cells is called cell biology. 
Cells consist of a protoplasm enclosed within a membrane, which contains many biomolecules such as proteins and nucleic acids. The Alberts text discusses how the "cellular building blocks" move to shape developing embryos. It is also common to describe small molecules such as amino acids as "molecular building blocks".</ref> Organisms can be classified as unicellular (consisting of a single cell; including most bacteria) or multicellular (including plants and animals). While the number of cells in plants and animals varies from species to species, humans contain about 100 trillion (1014) cells. Most plant and animal cells are visible only under the microscope, with dimensions between 1 and 100 micrometres.
The cell was discovered by Robert Hooke in 1665. The cell theory, first developed in 1839 by Matthias Jakob Schleiden and Theodor Schwann, states that all organisms are composed of one or more cells, that all cells come from preexisting cells, that vital functions of an organism occur within cells, and that all cells contain the hereditary information necessary for regulating cell functions and for transmitting information to the next generation of cells. Cells emerged on Earth at least 3.5 billion years ago.
Anatomy.
There are two types of cells, eukaryotes, which contain a nucleus, and prokaryotes, which do not. Prokaryotic cells are usually single-celled organisms, while eukaryotic cells can be either single-celled or part of multicellular organisms.
Prokaryotic cells.
Prokaryotic cells were the first form of life on Earth. They are simpler and smaller than eukaryotic cells, and lack membrane-bound organelles such as the nucleus. Prokaryotes include two of the domains of life, bacteria and archaea. The DNA of a prokaryotic cell consists of a single chromosome that is in direct contact with the cytoplasm. The nuclear region in the cytoplasm is called the nucleoid.
A prokaryotic cell has three architectural regions:
Eukaryotic cells.
Plants, animals, fungi, slime moulds, protozoa, and algae are all eukaryotic. These cells are about fifteen times wider than a typical prokaryote and can be as much as a thousand times greater in volume. The main distinguishing feature of eukaryotes as compared to prokaryotes is compartmentalization: the presence of membrane-bound compartments in which specific metabolic activities take place. Most important among these is a cell nucleus, a membrane-delineated compartment that houses the eukaryotic cell's DNA. This nucleus gives the eukaryote its name, which means "true nucleus". Other differences include:
Subcellular components.
All cells, whether prokaryotic or eukaryotic, have a membrane that envelops the cell, regulates what moves in and out (selectively permeable), and maintains the electric potential of the cell. Inside the membrane, a salty cytoplasm takes up most of the cell volume. All cells (except red blood cells which lack a cell nucleus and most organelles to accommodate maximum space for hemoglobin) possess DNA, the hereditary material of genes, and RNA, containing the information necessary to build various proteins such as enzymes, the cell's primary machinery. There are also other kinds of biomolecules in cells. This article lists these primary components of the cell, then briefly describes their function.
Membrane.
The cell membrane, or plasma membrane, surrounds the cytoplasm of a cell. In animals, the plasma membrane is the outer boundary of the cell, while in plants and prokaryotes it is usually covered by a cell wall. This membrane serves to separate and protect a cell from its surrounding environment and is made mostly from a double layer of phospholipids, which are amphiphilic (partly hydrophobic and partly hydrophilic). Hence, the layer is called a phospholipid bilayer, or sometimes a fluid mosaic membrane. Embedded within this membrane is a variety of protein molecules that act as channels and pumps that move different molecules into and out of the cell. The membrane is said to be 'semi-permeable', in that it can either let a substance (molecule or ion) pass through freely, pass through to a limited extent or not pass through at all. Cell surface membranes also contain receptor proteins that allow cells to detect external signaling molecules such as hormones.
Cytoskeleton.
The cytoskeleton acts to organize and maintain the cell's shape; anchors organelles in place; helps during endocytosis, the uptake of external materials by a cell, and cytokinesis, the separation of daughter cells after cell division; and moves parts of the cell in processes of growth and mobility. The eukaryotic cytoskeleton is composed of microfilaments, intermediate filaments and microtubules. There are a great number of proteins associated with them, each controlling a cell's structure by directing, bundling, and aligning filaments. The prokaryotic cytoskeleton is less well-studied but is involved in the maintenance of cell shape, polarity and cytokinesis.
Genetic material.
Two different kinds of genetic material exist: deoxyribonucleic acid (DNA) and ribonucleic acid (RNA). Cells use DNA for their long-term information storage. The biological information contained in an organism is encoded in its DNA sequence. RNA is used for information transport (e.g., mRNA) and enzymatic functions (e.g., ribosomal RNA). Transfer RNA (tRNA) molecules are used to add amino acids during protein translation.
Prokaryotic genetic material is organized in a simple circular DNA molecule (the bacterial chromosome) in the nucleoid region of the cytoplasm. Eukaryotic genetic material is divided into different, linear molecules called chromosomes inside a discrete nucleus, usually with additional genetic material in some organelles like mitochondria and chloroplasts (see endosymbiotic theory).
A human cell has genetic material contained in the cell nucleus (the nuclear genome) and in the mitochondria (the mitochondrial genome). In humans the nuclear genome is divided into 46 linear DNA molecules called chromosomes, including 22 homologous chromosome pairs and a pair of sex chromosomes. The mitochondrial genome is a circular DNA molecule distinct from the nuclear DNA. Although the mitochondrial DNA is very small compared to nuclear chromosomes, it codes for 13 proteins involved in mitochondrial energy production and specific tRNAs.
Foreign genetic material (most commonly DNA) can also be artificially introduced into the cell by a process called transfection. This can be transient, if the DNA is not inserted into the cell's genome, or stable, if it is. Certain viruses also insert their genetic material into the genome.
Organelles.
Organelles are parts of the cell which are adapted and/or specialized for carrying out one or more vital functions, analogous to the organs of the human body (such as the heart, lung, and kidney, with each organ performing a different function). Both eukaryotic and prokaryotic cells have organelles, but prokaryotic organelles are generally simpler and are not membrane-bound.
There are several types of organelles in a cell. Some (such as the nucleus and golgi apparatus) are typically solitary, while others (such as mitochondria, chloroplasts, peroxisomes and lysosomes) can be numerous (hundreds to thousands). The cytosol is the gelatinous fluid that fills the cell and surrounds the organelles.
Structures outside the cell membrane.
Many cells also have structures which exist wholly or partially outside the cell membrane. These structures are notable because they are not protected from the external environment by the impermeable cell membrane. In order to assemble these structures, their components must be carried across the cell membrane by export processes.
Cell wall.
Many types of prokaryotic and eukaryotic cells have a cell wall. The cell wall acts to protect the cell mechanically and chemically from its environment, and is an additional layer of protection to the cell membrane. Different types of cell have cell walls made up of different materials; plant cell walls are primarily made up of pectin, fungi cell walls are made up of chitin and bacteria cell walls are made up of peptidoglycan.
Prokaryotic.
Capsule.
A gelatinous capsule is present in some bacteria outside the cell membrane and cell wall. The capsule may be polysaccharide as in pneumococci, meningococci or polypeptide as "Bacillus anthracis" or hyaluronic acid as in streptococci. (See Bacterial capsule.)
Capsules are not marked by normal staining protocols and can be detected by India ink or methyl blue; which allows for higher contrast between the cells for observation.
Flagella.
Flagella are organelles for cellular mobility. The bacterial flagellum stretches from cytoplasm through the cell membrane(s) and extrudes through the cell wall. They are long and thick thread-like appendages, protein in nature. Are most commonly found in bacteria cells but are found in animal cells as well.
Fimbriae (pili).
They are short and thin hair-like filaments, formed of protein called pilin (antigenic). Fimbriae are responsible for attachment of bacteria to specific receptors of human cell (adherence). There are special types of pili called (sex pili) involved in conjunction. (See Pilus.)
Cellular processes.
Growth and metabolism.
Between successive cell divisions, cells grow through the functioning of cellular metabolism. Cell metabolism is the process by which individual cells process nutrient molecules. Metabolism has two distinct divisions: catabolism, in which the cell breaks down complex molecules to produce energy and reducing power, and anabolism, in which the cell uses energy and reducing power to construct complex molecules and perform other biological functions.
Complex sugars consumed by the organism can be broken down into a less chemically complex sugar molecule called glucose. Once inside the cell, glucose is broken down to make adenosine triphosphate (ATP), a form of energy, through two different pathways.
The first pathway, glycolysis, requires no oxygen and is referred to as anaerobic metabolism. Each reaction produces ATP and NADH, which are used in cellular functions, as well as two pyruvate molecules that derived from the original glucose molecule. In prokaryotes, all energy is produced by glycolysis.
The second pathway, called the Krebs cycle or citric acid cycle, is performed only by eukaryotes and involves further breakdown of the pyruvate produced in glycolysis. It occurs inside the mitochondria and generates much more energy than glycolysis, mostly through oxidative phosphorylation.
Replication.
Cell division involves a single cell (called a "mother cell") dividing into two daughter cells. This leads to growth in multicellular organisms (the growth of tissue) and to procreation (vegetative reproduction) in unicellular organisms. Prokaryotic cells divide by binary fission, while eukaryotic cells usually undergo a process of nuclear division, called mitosis, followed by division of the cell, called cytokinesis. A diploid cell may also undergo meiosis to produce haploid cells, usually four. Haploid cells serve as gametes in multicellular organisms, fusing to form new diploid cells.
DNA replication, or the process of duplicating a cell's genome, always happens when a cell divides through mitosis or binary fission. This occurs during the S phase of the cell cycle.
In meiosis, the DNA is replicated only once, while the cell divides twice. DNA replication only occurs before meiosis I. DNA replication does not occur when the cells divide the second time, in meiosis II. Replication, like all cellular activities, requires specialized proteins for carrying out the job.
Protein synthesis.
Cells are capable of synthesizing new proteins, which are essential for the modulation and maintenance of cellular activities. This process involves the formation of new protein molecules from amino acid building blocks based on information encoded in DNA/RNA. Protein synthesis generally consists of two major steps: transcription and translation.
Transcription is the process where genetic information in DNA is used to produce a complementary RNA strand. This RNA strand is then processed to give messenger RNA (mRNA), which is free to migrate through the cell. mRNA molecules bind to protein-RNA complexes called ribosomes located in the cytosol, where they are translated into polypeptide sequences. The ribosome mediates the formation of a polypeptide sequence based on the mRNA sequence. The mRNA sequence directly relates to the polypeptide sequence by binding to transfer RNA (tRNA) adapter molecules in binding pockets within the ribosome. The new polypeptide then folds into a functional three-dimensional protein molecule.
Movement or motility.
Unicellular organisms can move in order to find food or escape predators. Common mechanisms of motion include flagella and cilia. 
In multicellular organisms, cells can move during processes such as wound healing, the immune response and cancer metastasis. For example, in wound healing in animals, white blood cells move to the wound site to kill the microorganisms that cause infection. Cell motility involves many receptors, crosslinking, bundling, binding, adhesion, motor and other proteins. The process is divided into three steps – protrusion of the leading edge of the cell, adhesion of the leading edge and de-adhesion at the cell body and rear, and cytoskeletal contraction to pull the cell forward. Each step is driven by physical forces generated by unique segments of the cytoskeleton.
Multicellularity.
Cell specialization.
Multicellular organisms are organisms that consist of more than one cell, in contrast to single-celled organisms.
In complex multicellular organisms, cells specialize into different cell types that are adapted to particular functions. In mammals, major cell types include skin cells, muscle cells, neurons, blood cells, fibroblasts, stem cells, and others. Cell types differ both in appearance and function, yet are genetically identical. Cells are able to be of the same genotype but different cell type due to the differential regulation of the genes they contain.
Most distinct cell types arise from a single totipotent cell, called a zygote, that differentiates into hundreds of different cell types during the course of development. Differentiation of cells is driven by different environmental cues (such as cell–cell interaction) and intrinsic differences (such as those caused by the uneven distribution of molecules during division).
Origin of multicellularity.
Multicellularity has evolved independently at least 25 times, including in some prokaryotes, like cyanobacteria, myxobacteria, actinomycetes, "Magnetoglobus multicellularis" or "Methanosarcina". However, complex multicellular organisms evolved only in six eukaryotic groups: animals, fungi, brown algae, red algae, green algae, and plants. It evolved repeatedly for plants (Chloroplastida), once or twice for animals, once for brown algae, and perhaps several times for fungi, slime molds, and red algae. Multicellularity may have evolved from colonies of interdependent organisms, from cellularization, or from organisms in symbiotic relationships. 
The first evidence of multicellularity is from cyanobacteria-like organisms that lived between 3 and 3.5 billion years ago. Other early fossils of multicellular organisms include the contested Grypania spiralis and the fossils of the black shales of the Palaeoproterozoic Francevillian Group Fossil B Formation in Gabon.
The evolution of multicellularity from unicellular ancestors has been replicated in the laboratory, in evolution experiments using predation as the selective pressure.
Origins.
The origin of cells has to do with the origin of life, which began the history of life on Earth.
Origin of the first cell.
There are several theories about the origin of small molecules that led to life on the early Earth. They may have been carried to Earth on meteorites (see Murchison meteorite), created at deep-sea vents, or synthesized by lightning in a reducing atmosphere (see Miller–Urey experiment). There is little experimental data defining what the first self-replicating forms were. RNA is thought to be the earliest self-replicating molecule, as it is capable of both storing genetic information and catalyzing chemical reactions (see RNA world hypothesis), but some other entity with the potential to self-replicate could have preceded RNA, such as clay or peptide nucleic acid.
Cells emerged at least 3.5 billion years ago. The current belief is that these cells were heterotrophs. The early cell membranes were probably more simple and permeable than modern ones, with only a single fatty acid chain per lipid. Lipids are known to spontaneously form bilayered vesicles in water, and could have preceded RNA, but the first cell membranes could also have been produced by catalytic RNA, or even have required structural proteins before they could form.
Origin of eukaryotic cells.
The eukaryotic cell seems to have evolved from a symbiotic community of prokaryotic cells. DNA-bearing organelles like the mitochondria and the chloroplasts are descended from ancient symbiotic oxygen-breathing proteobacteria and cyanobacteria, respectively, which were endosymbiosed by an ancestral archaean prokaryote.
There is still considerable debate about whether organelles like the hydrogenosome predated the origin of mitochondria, or vice versa: see the hydrogen hypothesis for the origin of eukaryotic cells.

</doc>
<doc id="4231" url="http://en.wikipedia.org/wiki?curid=4231" title="Buffy the Vampire Slayer (film)">
Buffy the Vampire Slayer (film)

Buffy the Vampire Slayer is a 1992 American action/comedy horror film about a Valley girl cheerleader named Buffy who learns that it is her fate to hunt vampires. The film starred Kristy Swanson, Donald Sutherland, Paul Reubens, Rutger Hauer, Luke Perry and Hilary Swank. It was a moderate success at the box office, but received mixed reception from critics. The film was taken in a different direction from the one that its writer, Joss Whedon, intended, but several years later he was able to create the darker and acclaimed TV series of the same name.
Plot.
High school senior Buffy Summers (Kristy Swanson) is introduced as a stereotypical, shallow cheerleader at Hemery High School in Los Angeles. She is a carefree popular girl whose main concerns are shopping and spending time with her airhead friends and her boyfriend, Jeffrey. While at school one day, she is approached by a man named Merrick Jamison-Smythe (Donald Sutherland). He informs her that she is The Slayer, or chosen one, and he is a Watcher whose duty it is to guide and train her. She initially rebukes his claims, but finally becomes convinced that he is right when he is able to describe a recurring dream of hers in detail. In addition, Buffy is exhibiting uncanny abilities not known to her, including heightened agility, senses, and endurance, yet she repeatedly tries Merrick's patience with her frivolous nature and sharp-tongued remarks.
Meanwhile Oliver Pike (Luke Perry), and best friend Benny (David Arquette), who resented Buffy and her friends due to differing social circles, are out drinking when they are attacked by vampires. Benny is turned but Oliver is saved by Merrick. As a vampire, Benny visits Oliver and tries to get him to join him. Later, when Oliver and his boss are discussing Benny, Oliver tells him to run if he sees him.
After several successful outings, Buffy is drawn into conflict with a local vampire king named Lothos (Rutger Hauer), who has killed a number of past Slayers. During an encounter with Lothos' main minion Amilyn (Paul Reubens) and his gang of vampires, Buffy, Oliver, and Merrick fight against them in the forest as Amilyn loses his arm. Amilyn flees the fight to talk to Lothos who now realizes Buffy is the slayer. After this encounter, Buffy and Oliver start a friendship, which eventually becomes romantic and Oliver becomes Buffy's partner in fighting the undead.
During a basketball game, Buffy and Oliver find out that one of the players is a minion of Lothos. After a quick chase to a parade float storage yard, Buffy finally confronts Lothos, shortly after she and Oliver take down his gang. Lothos puts Buffy in a hypnotic trance, which is broken due to Merrick's intervention. Lothos turns on Merrick and impales him with the stake he attempted to use on him. Lothos leaves, saying that Buffy is not ready. As Merrick dies, he tells Buffy to do things her own way rather than live by the rules of others. Because of her new life, responsibilities, and heartbreak, Buffy becomes emotionally shocked and has a falling out with her friends as she outgrows their immature, selfish behavior, and starts dropping her Slayer responsibilities.
At the senior dance, Buffy meets up with Oliver and as they start to dance and eventually kiss, Lothos leads the remainder of his minions to the school and attacks the students and the attending faculty. Buffy confronts the vampires outside while Oliver fights the vampiric Benny. After overpowering the vampires, she confronts Lothos inside the school and stabs Amilyn. Lothos hypnotises Buffy again but she uses a cross and hairspray to create a makeshift flame-thrower and burns Lothos before heading back into the gym. Buffy sees everybody recover from the attack, but Lothos emerges again getting into a fight with Buffy, who then stakes him.
The film ends with Buffy and Oliver leaving the dance on a motorcycle, and a news crew interviewing the students and the principal about the attack during the credits.
Continuity with the television show.
Many of the details given in the film differ from the continuity of the later television series. For example, Buffy's history is dissimilar, and both the vampires' and Slayer's abilities are depicted differently. The vampires in the film die like humans, while in the TV show they turn to dust. Joss Whedon has expressed his disapproval with the movie's interpretation of the script, stating, "I finally sat down and had written it and somebody had made it into a movie, and I felt like — well, that's not quite her. It's a start, but it's not quite the girl."
According to the "Official Buffy Watcher's Guide", Whedon wrote the pilot to the TV series as a sequel to his original script, which is why the show makes references to events that did not occur in the film. In 1999, Dark Horse Comics released a graphic novel adaptation of Whedon's original script under the title, "The Origin". Whedon stated: "The "Origin" comic, though I have issues with it, CAN pretty much be accepted as canonical. They did a cool job of combining the movie script with the series, that was nice, and using the series Merrick and not a certain OTHER thespian who shall remain hated."
Box office.
The film debuted at #5 at the North American box office and eventually grossed $16,624,456 against a $7 million production budget.
Home releases.
The film was released on VHS and Laserdisc in the U.S. in 1992 by Fox Video and re-released in 1995 under the "Twentieth Century Fox Selections" banner. It was released on DVD in the US in 2001 and on BluRay in 2011.
Soundtrack.
The soundtrack was released on July 28, 1992.
The soundtrack does not include every song played in the film, which also included "In the Wind" by War Babies and "Inner Mind" by Eon.
Remake.
On May 25, 2009, "The Hollywood Reporter" reported that Roy Lee and Doug Davison of Vertigo Entertainment were working with Fran Rubel Kuzui and Kaz Kuzui on a re-envisioning or relaunch of the "Buffy" film for the big screen. The film would not be a sequel or prequel to the existing film or television franchise and Joss Whedon would have no involvement in the project. None of the characters, cast, or crew from the television series would be featured. Television series executive producer Marti Noxon later reflected that this story might have been produced by the studio in order to frighten Whedon into taking the reins of the project. On November 22, 2010, "The Hollywood Reporter" confirmed that Warner Bros. had picked up the movie rights to the remake. The film was set for release sometime in 2012. 20th Century Fox, which usually holds rights to the more successful "Buffy"/"Angel" television franchise, will retain merchandising and some distribution rights.
The idea of the remake caused wrath among fans of the TV series, since Whedon is not involved and the project does not have any connection with the show and will not conform to the continuity maintained with the "Buffy the Vampire Slayer Season Eight" and "Season Nine" comic book titles. Not only the fandom, but the main cast members of both "Buffy" and "Angel" series, expressed disagreement with the report on Twitter and in recent interviews. Sarah Michelle Gellar said, "I think it's a horrible idea. To try to do a "Buffy" without Joss Whedon... to be incredibly non-eloquent: that's the dumbest idea I've ever heard." Proposed shooting locations included Black Wood and other areas in rural England, due to budgetary constraints and the potential setting as being outside of the city, an unusual change for the franchise.
In December 2011, more than a year after the official reboot announcement, the "Los Angeles Times" site reported that Whit Anderson, the writer picked for the new "Buffy" movie, had her script rejected by the producers behind the project, and that a new writer was being sought. Sources also stated that "If you're going to bring it back, you have to do it right. came in with some great ideas and she had reinvented some of the lore and it was pretty cool but in the end there just wasn't enough on the page."

</doc>
<doc id="4232" url="http://en.wikipedia.org/wiki?curid=4232" title="Barter">
Barter

Barter is a system of exchange by which goods or services are directly exchanged for other goods or services without using a medium of exchange, such as money. It is distinguishable from gift economies in that the reciprocal exchange is immediate and not delayed in time. It is usually bilateral, but may be multilateral (i.e., mediated through barter organizations) and usually exists parallel to monetary systems in most developed countries, though to a very limited extent. Barter usually replaces money as the method of exchange in times of monetary crisis, such as when the currency may be either unstable (e.g., hyperinflation or deflationary spiral) or simply unavailable for conducting commerce.
David Graeber argues that the inefficiency of barter in archaic society has been used by economists since Adam Smith to explain the emergence of money, the economy, and hence the discipline of economics itself. "Economists of the contemporary orthodoxy... propose an evolutionary development of economies which places barter, as a 'natural' human characteristic, at the most primitive stage, to be superseded by monetary exchange as soon as people become aware of the latter's greater efficiency." However, extensive investigation by anthropologists like Graeber has since then established that "No example of a barter economy, pure and simple, has ever been described, let alone the emergence from it of money; all available ethnography suggests that there never has been such a thing. But there are economies today which are nevertheless "dominated" by barter."
Since the 1830s, direct barter in western market economies has been aided by exchanges which frequently utilize alternative currencies based on the labour theory of value, and designed to prevent profit taking by intermediators. Examples include the Owenite socialists, the Cincinnati Time store, and more recently Ithaca HOURS (Time banking) and the LETS system.
Economic theory.
Adam Smith on the origin of money.
Adam Smith, the father of modern economics, sought to demonstrate that markets (and economies) pre-existed the state, and hence should be free of government regulation. He argued (against conventional wisdom) that money was not the creation of governments. Markets emerged, in his view, out of the division of labour, by which individuals began to specialize in specific crafts and hence had to depend on others for subsistence goods. These goods were first exchanged by barter. Specialization depended on trade, but was hindered by the "double coincidence of wants" which barter requires, i.e., for the exchange to occur, each participant must want what the other has. To complete this hypothetical history, craftsmen would stockpile one particular good, be it salt or metal, that they thought no one would refuse. This is the origin of money according to Smith. Money, as a universally desired medium of exchange, allows each half of the transaction to be separated.
Barter is characterized in Adam Smith's ""The Wealth of Nations"" by a disparaging vocabulary: "higgling, haggling, swapping, dickering." It has also been characterized as negative reciprocity, or "selfish profiteering."
Anthropologists have argued, in contrast, "that when something resembling barter "does" occur in stateless societies it is almost always between strangers, people who would otherwise be enemies." Barter occurred between strangers, not fellow villagers, and hence cannot be used to naturalisticly explain the origin of money without the state. Since most people engaged in trade knew each other, exchange was fostered through the extension of credit. Marcel Mauss, author of 'The Gift', argued that the first economic contracts were to "not" act in one's economic self-interest, and that before money, exchange was fostered through the processes of reciprocity and redistribution, not barter. Everyday exchange relations in such societies are characterized by generalized reciprocity, or a non-calculative familial "communism" where each takes according to their needs, and gives as they have.
Limitations.
Barter's limits are usually explained in terms of its inefficiencies in easing exchange in comparison to the functions of money:
History.
Silent trade.
Other anthropologists have questioned whether barter is typically between "total" strangers, a form of barter known as "silent trade". Silent trade, also called silent barter, dumb barter ("dumb" here used in its old meaning of "mute"), or depot trade, is a method by which traders who cannot speak each other's language can trade without talking. However, Benjamin Orlove has shown that while barter occurs through "silent trade" (between strangers), it also occurs in commercial markets as well. "Because barter is a difficult way of conducting trade, it will occur only where there are strong institutional constraints on the use of money or where the barter symbolically denotes a special social relationship and is used in well-defined conditions. To sum up, multipurpose money in markets is like lubrication for machines - necessary for the most efficient function, but not necessary for the existence of the market itself."
In his analysis of barter between coastal and inland villages in the Trobriand Islands, Keith Hart highlighted the difference between highly ceremonial gift exchange between community leaders, and the barter that occurs between individual households. The haggling that takes place between strangers is possible because of the larger temporary political order established by the gift exchanges of leaders. From this he concludes that barter is "an atomized interaction predicated upon the presence of society" (i.e. that social order established by gift exchange), and not typical between complete strangers.
Times of monetary crisis.
As Orlove noted, barter may occur in commercial economies, usually during periods of monetary crisis. During such a crisis, currency may be in short supply, or highly devalued through hyperinflation. In such cases, money ceases to be the universal medium of exchange or standard of value. Money may be in such short supply that it becomes an item of barter itself rather than the means of exchange. Barter may also occur when people cannot afford to keep money (as when hyperinflation quickly devalues it).
Exchanges.
Economic historian Karl Polanyi has argued that where barter is widespread, and cash supplies limited, barter is aided by the use of credit, brokerage, and money as a unit of account (i.e. used to price items). All of these strategies are found in ancient economies including Ptolemaic Egypt. They are also the basis for more recent barter exchange systems.
While one-to-one bartering is practiced between individuals and businesses on an informal basis, organized barter exchanges have developed to conduct third party bartering which helps overcome some of the limitations of barter. A barter exchange operates as a broker and bank in which each participating member has an account that is debited when purchases are made, and credited when sales are made.
Modern barter and trade has evolved considerably to become an effective method of increasing sales, conserving cash, moving inventory, and making use of excess production capacity for businesses around the world. Businesses in a barter earn trade credits (instead of cash) that are deposited into their account. They then have the ability to purchase goods and services from other members utilizing their trade credits – they are not obligated to purchase from those whom they sold to, and vice-versa. The exchange plays an important role because they provide the record-keeping, brokering expertise and monthly statements to each member. Commercial exchanges make money by charging a commission on each transaction either all on the buy side, all on the sell side, or a combination of both. Transaction fees typically run between 8 and 15%.
Utopian socialism.
The Owenite socialists in Britain and the United States in the 1830s were the first to attempt to organize barter exchanges. Owenism developed a "theory of equitable exchange" as a critique of the exploitative wage relationship between capitalist and labourer, by which all profit accrued to the capitalist. To counteract the uneven playing field between employers and employed, they proposed "schemes of labour notes based on labour time, thus institutionalizing Owen's demand that human labour, not money, be made the standard of value." This alternate currency eliminated price variability between markets, as well as the role of merchants who bought low and sold high. The system arose in a period where paper currency was an innovation. Paper currency was an I.O.U. circulated by a bank (a promise to pay, not a payment in itself). Both merchants and an unstable paper currency created difficulties for direct producers.
An alternate currency, denominated in labour time, would prevent profit taking by middlemen; all goods exchanged would be priced only in terms of the amount of labour that went into them as expressed in the maxim 'Cost the limit of price'. It became the basis of exchanges in London, and in America, where the idea was implemented at the New Harmony communal settlement by Josiah Warren in 1826, and in his Cincinnati 'Time store' in 1827. Warren ideas were adopted by other Owenites and currency reformers, even though the labour exchanges were relatively short lived.
In England, about 30 to 40 cooperative societies sent their surplus goods to an "exchange bazaar" for direct barter in London, which later adopted a similar labour note. The British Association for Promoting Cooperative Knowledge established an "equitable labour exchange" in 1830. This was expanded as the National Equitable Labour Exchange in 1832 on Grays Inn Road in London. These efforts became the basis of the British cooperative movement of the 1840s. In 1848, the socialist and first self-designated anarchist Pierre-Joseph Proudhon postulated a system of "time chits". In 1875, Karl Marx wrote of "Labor Certificates" ("Arbeitszertifikaten") in his Critique of the Gotha Program of a "certificate from society that labourer has furnished such and such an amount of labour", which can be used to draw "from the social stock of means of consumption as much as costs the same amount of labour."
Twentieth century experiments.
The first exchange system was the Swiss WIR Bank. It was founded in 1934 as a result of currency shortages after the stock market crash of 1929. "WIR" is both an abbreviation of Wirtschaftsring and the word for "we" in German, reminding participants that the economic circle is also a community.
In Spain (particularly the Catalonia region) there is a growing number of exchange markets. These barter markets or swap meets work without money. Participants bring things they do not need and exchange them for the unwanted goods of another participant. Swapping among three parties often helps satisfy tastes when trying to get around the rule that money is not allowed.
Michael Linton originated the term "local exchange trading system" (LETS) in 1983 and for a time ran the Comox Valley LETSystems in Courtenay, British Columbia. LETS networks use interest-free local credit so direct swaps do not need to be made. For instance, a member may earn credit by doing childcare for one person and spend it later on carpentry with another person in the same network. In LETS, unlike other local currencies, no scrip is issued, but rather transactions are recorded in a central location open to all members. As credit is issued by the network members, for the benefit of the members themselves, LETS are considered mutual credit systems.
Modern developments.
According to the International Reciprocal Trade Association, the industry trade body, more than 450,000 businesses transacted $10 billion globally in 2008 – and officials expect trade volume to grow by 15% in 2009.
It is estimated that over 450,000 businesses in the United States were involved in barter exchange activities in 2010. There are approximately 400 commercial and corporate barter companies serving all parts of the world. There are many opportunities for entrepreneurs to start a barter exchange. Several major cities in the U.S. and Canada do not currently have a local barter exchange. There are two industry groups in the United States, the National Association of Trade Exchanges (NATE) and the International Reciprocal Trade Association (IRTA). Both offer training and promote high ethical standards among their members. Moreover, each has created its own currency through which its member barter companies can trade. NATE's currency is the known as the BANC and IRTA's currency is called Universal Currency (UC). In Canada, the largest barter exchange is Tradebank, founded in 1987. In the United States, the largest barter exchange and corporate trade group is International Monetary Systems, founded in 1985, now with representation in various countries. In Australia and New Zealand the largest barter exchange is Bartercard, founded in 1991, with offices in the United Kingdom,United States, Cyprus,UAE and Thailand.
Corporate barter focuses on larger transactions, which is different from a traditional, retail oriented barter exchange. Corporate barter exchanges typically use media and advertising as leverage for their larger transactions. It entails the use of a currency unit called a "trade-credit". The trade-credit must not only be known and guaranteed, but also be valued in an amount the media and advertising could have been purchased for had the "client" bought it themselves (contract to eliminate ambiguity and risk).
Soviet bilateral trade is occasionally called "barter trade", because although the purchases were denominated in U.S. dollars, the transactions were credited to an international clearing account, avoiding the use of hard cash.
Tax implications.
In the United States, Karl Hess used bartering to make it harder for the IRS to seize his wages and as a form of tax resistance. Hess explained how he turned to barter in an op-ed for "The New York Times" in 1975. However the IRS now requires barter exchanges to be reported as per the Tax Equity and Fiscal Responsibility Act of 1982. Barter exchanges are considered taxable revenue by the IRS and must be reported on a 1099-B form. According to the IRS, "The fair market value of goods and services exchanged must be included in the income of both parties."
Other countries though do not have the reporting requirement that the U.S. does concerning proceeds from barter transactions, but taxation is handled the same way as a cash transaction. If one barters for a profit, one pays the appropriate tax; if one generates a loss in the transaction, they have a loss. Bartering for business is also taxed accordingly as business income or business expense. Many barter exchanges require that one register as a business.

</doc>
<doc id="4233" url="http://en.wikipedia.org/wiki?curid=4233" title="Berthe Morisot">
Berthe Morisot

Berthe Morisot (; January 14, 1841 – March 2, 1895) was a painter and a member of the circle of painters in Paris who became known as the Impressionists. She was described by Gustave Geffroy in 1894 as one of "les trois grandes dames" of Impressionism alongside Marie Bracquemond and Mary Cassatt.
In 1864, she exhibited for the first time in the highly esteemed Salon de Paris. Sponsored by the government, and judged by Academicians, the Salon was the official, annual exhibition of the Académie des beaux-arts in Paris. Her work was selected for exhibition in six subsequent Salons until, in 1874, she joined the ""rejected"" Impressionists in the first of their own exhibitions, which included Paul Cézanne, Edgar Degas, Claude Monet, Camille Pissarro, Pierre-Auguste Renoir, and Alfred Sisley. It was held at the studio of the photographer Nadar.
She became the sister-in-law of her friend and colleague, Édouard Manet, when she married his brother, Eugène.
Education.
Morisot was born in Bourges, Cher, France, into a successful bourgeois family. According to family tradition, the family had included one of the most prolific Rococo painters of the ancien régime, Fragonard, whose handling of color and expressive, confident brushwork influenced later painters. Both Berthe and her sister, Edma Morisot, chose to become painters.
Berthe Morisot's family moved to Paris when she was a child. Once Berthe settled on pursuing art, her family did not impede her career. She registered as a copyist at the Louvre. By age twenty, she had met and befriended the important, and pivotal, landscape painter of the Barbizon School, Camille Corot, who excelled in figure painting as well. The older artist instructed Berthe and her sister in painting and introduced them to other artists and teachers. Under Corot's influence, Morisot took up the plein air method of working.
Manet and impressionism.
Morisot's first appearance in the Salon de Paris came at the age of twenty-three in 1864, with the acceptance of two landscape paintings. She continued to show regularly in the Salon, to generally favorable reviews, until 1873, the year before the first Impressionist exhibition.
Meanwhile, in 1868 Morisot became acquainted with Édouard Manet. He took a special interest in Morisot, as is evident from his warm portrayal of her in several paintings, including a striking portrait study of Morisot in a black veil, while in mourning for her father's death (displayed at the top of the article). Correspondence between them bespeaks affection. He once gave her an easel as a Christmas present. He also interfered in one of her Salon submissions when he was engaged to transport it. Manet mistook one of Morisot's self-criticisms as an invitation to add his corrections, which he did, much to Morisot's dismay.
Although traditionally Manet has been related as the master and Morisot as the follower, there is evidence that their relationship was a reciprocating one. Morisot had developed her own distinctive artistic style. Records of paintings show Manet's appreciation of certain stylistic and compositional decisions that Morisot originated. He incorporated some of these characteristics into his own work.
It was Morisot who persuaded Manet to attempt plein air painting, which she had been practicing since having been introduced to it by Corot.
She also drew Manet into the circle of painters who soon became known as the Impressionists. In 1874, Morisot married Manet's brother, Eugene, and they had one daughter, Julie. Julie Manet became the subject for many of her mother's paintings and a book of her memoirs "Growing Up with the Impressionists: The Diary of Julie Manet," was published in 1987.
Subjects.
Morisot painted what she experienced on a daily basis. Her paintings reflect the 19th-century cultural restrictions of her class and gender. She avoided urban and street scenes as well as the nude figure and, like her fellow female Impressionist Mary Cassatt, focused on domestic life and portraits in which she could use family and personal friends as models. Paintings like "The Cradle" (1872), in which she depicted current trends for nursery furniture, reflect her sensitivity to fashion and advertising, both of which would have been apparent to her female audience. Her works also include landscapes, portraits, garden settings and boating scenes.
Death.
Berthe Morisot died on March 2, 1895, in Paris, of pneumonia contracted while attending to her daughter Julie's similar illness. She was interred in the Cimetière de Passy.
In popular culture.
She was portrayed by actress Marine Delterme in the eponymous 2012 French biographical TV film directed by Caroline Champetier.
Art market.
At a Christie's auction in February 2013, "After Lunch" (1881), a portrait of a young redhead in a straw hat and purple dress, sold for $10.9 million, roughly three times its high estimate. The painting set a record at the time as the most expensive work ever sold by a female artist at auction at the time, topping an earlier record set with $10.7 million having been paid for a sculpture by Louise Bourgeois in 2012.

</doc>
<doc id="4237" url="http://en.wikipedia.org/wiki?curid=4237" title="Barnard College">
Barnard College

Barnard College is a private women's liberal arts college and a member of the Seven Sisters. Founded in 1889, it has been affiliated with Columbia University since 1900. Barnard's campus stretches along Broadway between 116th and 120th Streets in the Morningside Heights neighborhood in the borough of Manhattan, in New York City. It is directly adjacent to Columbia's campus and near several other academic institutions and has been used by Barnard since 1898.
History.
Barnard College was founded to provide an undergraduate education for women comparable to that of Columbia University and other Ivy League schools, most of which admitted only men for undergraduate study into the 1960s. The college was named after Frederick Augustus Porter Barnard, an American educator and mathematician, who served as the president of the then-Columbia College from 1864 to 1889. He advocated equal educational privileges for men and women, preferably in a coeducational setting, and began proposing in 1879 that Columbia admit women. The board of trustees repeatedly rejected Barnard's suggestion, but in 1883 agreed to create a detailed syllabus of study for women. While they could not attend Columbia classes, those who passed examinations based on the syllabus would receive a degree. The first such woman graduate received her bachelor's degree in 1887. A former student of the program, Annie Nathan Meyer, and other prominent New York women persuaded the board in 1889 to create a women's college connected to Columbia.
Barnard College's original 1889 home was a rented brownstone at 343 Madison Avenue, where a faculty of six offered instruction to 14 students in the School of Arts, as well as to 22 "specials", who lacked the entrance requirements in Greek and so enrolled in science. When Columbia University announced in 1892 its impending move to Morningside Heights, Barnard built a new campus on 119th-120th Streets with gifts from Mary E. Brinckerhoff, Elizabeth Milbank Anderson and Martha T. Fiske. Milbank, Brinckerhoff, and Fiske Halls, built in 1897–1898, were listed on the National Register of Historic Places in 2003. Ella Weed supervised the college in its first four years; Emily James Smith succeeded her as Barnard's first dean. As the college grew it needed additional space, and in 1903 it received the three blocks south of 119th Street from Anderson who had purchased a former portion of the Bloomingdale Asylum site from the New York Hospital. By the mid-20th century Barnard had succeeded in its original goal of providing an elite education to women. Between 1920 and 1974, only the much larger Hunter College and University of California, Berkeley produced more women graduates who later received doctorate degrees.
Students' Hall, now known as Barnard Hall, was built in 1916. Brooks and Hewitt Halls were built in 1906–1907 and 1926–1927, respectively. They were listed on the National Register of Historic Places in 2003.
Relationship with Columbia University.
The relationship between Barnard College and Columbia University is complicated. The college's front gates state "Barnard College of Columbia University". Barnard describes itself as "both an independently incorporated educational institution and an official college of Columbia University", and advises students to state "Barnard College, Columbia University" or "Barnard College of Columbia University" on résumés. Columbia describes Barnard as an affiliated institution that is a faculty of the university or is "in partnership with" it. An academic journal describes Barnard as a former affiliate that became a school within the university. Facebook includes Barnard students and alumnae within the Columbia interest group. All Barnard faculty are granted tenure by the college and Columbia, and Barnard graduates receive Columbia University diplomas signed by both the Barnard and Columbia presidents.
Smith and Columbia president Seth Low worked to open Columbia classes to Barnard students. By 1900 they could attend Columbia classes in philosophy, political science, and several scientific fields. That year Barnard formalized an affiliation with the university which made available to its students the instruction and facilities of Columbia. Many top women attended the college; Franz Boas, who taught at both Columbia and Barnard in the early 1900s, was among those faculty members who reportedly found Barnard students superior to their male Columbia counterparts. From 1955 Columbia and Barnard students could register for the other school's classes with the permission of the instructor; from 1973 no permission was needed.
Columbia president William J. McGill predicted in 1970 that Barnard College and Columbia College would merge within five years, and Columbia's financial difficulties during the 1970s increased its desire to merge, but Barnard resisted doing so because of the university's large debt. After a decade of failed negotiations for a merger with Barnard akin to the one between Harvard College and Radcliffe College, Columbia College instead began admitting women in 1983. Applications to Columbia rose 56% that year, making admission more selective, and nine Barnard students transferred to Columbia. Eight students admitted to both Columbia and Barnard chose Barnard, while 78 chose Columbia.
The Columbia-Barnard affiliation continued, however, despite Columbia College's decision. Barnard pays Columbia about $5 million a year under the terms of the "interoperate relationship", which the two schools renegotiate every 15 years. Despite the affiliation Barnard is legally and financially separate from Columbia, with an independent faculty and board of trustees. It is responsible for its own separate admissions, health, security, guidance and placement services, and has its own alumnae association. Nonetheless, Barnard students participate in the academic, social, athletic and extracurricular life of the broader University community on a reciprocal basis. The affiliation permits the two schools to share some academic resources; for example, only Barnard has an urban studies department, and only Columbia has a computer science department. Most Columbia classes are open to Barnard students and vice versa. Barnard students and faculty are represented in the University Senate, and student organizations such as the "Columbia Daily Spectator" are open to all students. Barnard students play on Columbia athletics teams, and Barnard uses Columbia email, telephone and network services.
Admissions.
Admissions to Barnard is considered most selective by "U.S. News & World Report". It is the most selective women's college in the nation; in 2008, Barnard had the lowest acceptance rate of the five Seven Sisters that remain single-sex in admissions.
The class of 2017's admission rate was 20.5%, a new record low. The class of 2016 set the admission rate at a 21%, with 5,440 applications received.
For the class of 2015, 5,154 applications were received, setting the admission rate at 24.9%. 
For the class of 2014, the admit rate was 27.8%, with 4,618 applications received. 
For the class of 2013, 90.3% ranked in first or second decile at their high school (of the 35.0% ranked by their schools). The average GPA of the class of 2013 was 94.6 on a 100-pt. scale and 3.84 on a 4.0 scale.
For the class of 2012, the admission rate was 28.5% of the 4,273 applications received. The early-decision admission rate was 47.7%, out of 392 applications. The median SAT Combined was 2060, with median subscores of 660 in Math, 690 in Critical Reading, and 700 in Writing. The Median ACT score was 30. Of the women in the class of 2012, 89.4% ranked in first or second decile at their high school (of the 41.3% ranked by their schools). The average GPA of the class of 2012 was 94.3 on a 100-point scale and 3.88 on a 4.0 scale.
For the class of 2011, Barnard College admitted 28.7% of those who applied. The median ACT score was 30, while the median combined SAT score was 2100.
Academic Ranking.
Barnard was most recently ranked 26th in the U.S. News & World Report Rankings. The ranking came under widespread criticism, as it only accounted for institution-specific resources. Greg Brown, chief operating officer at Barnard, said, "I believe that our ranking is lower than it should be, primarily because the methodology simply can't account for the Barnard-Columbia relationship. Because the Columbia relationship doesn't fit neatly into any of the survey categories, it is essentially ignored. Rankings are inherently limited in this way."
In 1998, then president Judith Shapiro compared the ranking service to the "equivalent of "Sport's Illustrated" swimsuit issue." According to Shapiro's letter, "Such a ranking system certainly does more harm than good in terms of educating the public." On June 19, 2007, following a meeting of the Annapolis Group, which represents over 100 liberal arts colleges, Barnard announced that it would no longer participate in the U.S. News annual survey, and that they would fashion their own way to collect and report common data.
Barnard Library.
Barnard's Wollman Library is located in Adele Lehman Hall. Its collection includes over 300,000 volumes which support the undergraduate curriculum. It also houses an archival collection of official and student publications, photographs, letters and other material that documents Barnard's history from its founding in 1889 to the present day. Barnard's rare books collections include the Overbury Collection, the personal library of Nobel prize-winning poet Gabriela Mistral, and a small collection of other rare books. The Overbury Collection consists of 3,300 items, including special and first edition books as well as manuscript materials by and about American women authors. Alumnae Books is a collection of books donated by Barnard alumnae authors. Conflicting accounts list either Richard B. Snow or Philip M. Chu as the architect of Lehman Hall... as well as of the Amherst College library and one of the libraries at Princeton University. The building opened in 1959.
Barnard Library Zine Collection.
Birthed from a proposal by longtime zinester Jenna Freedman, Barnard collects zines in an effort to document the third-wave feminism and Riot Grrrl culture. The Zine Collection complements Barnard's women's studies research holdings because it gives room to voices of girls and women otherwise under or not at all represented in the book stacks. According to its Library collection development policy, Barnard's zines are "written by New York City and other urban women with an emphasis on zines by women of color. (In this case the word "woman" includes anyone who identifies as female and some who don't believe in binary gender.) The zines are personal and political publications on activism, anarchism, body image, third wave feminism, gender, parenting, queer community, riotgrrrl, sexual assault, and other topics."
Barnard's collection documents movements and trends in feminist thought through the personal work of artists, writers, and activists. Currently, the Barnard Zine Collection has over 4,000 items, including zines about race, gender, sexuality, childbirth, motherhood, politics, and relationships. Barnard attempts to collect two copies of each zine, one of which circulates with the second copy archived for preservation. To facilitate circulation, Barnard zines are cataloged in CLIO (the Columbia/Barnard OPAC) and OCLC's Worldcat.
Culture and student life.
Student organizations.
Every Barnard student is part of the Student Government Association (SGA), which elects a representative student government. SGA aims to facilitate the expression of opinions on matters that directly affect the Barnard community. Members of the Executive Board and the Representative Council of SGA promote these goals through active communication between students, faculty, and administration. The Executive Board includes the President of SGA, Vice President, Vice President for Campus Life, Vice President for Communications,and Vice President of Finance. Members of the Representative Council include the Senior Representative to the Board of Trustees, Junior Representative to the Board of Trustees, University Senator, Representative for Campus Policy, Representative for Academic Affairs, Representative for Diversity, Representative for Student Services, Representative for Student Interests, Representative for College Relations, Representative for Arts and Culture, Representative for Campus Affairs, and Representative for Information and Technology. In addition to these members the President and Vice President of each Class Council also sit on the Representative Council.
Student groups include theatre and vocal music groups, language clubs, literary magazines, a freeform radio station called WBAR, a biweekly magazine called the "Barnard Bulletin", community service groups, and others. Barnard students can also join extracurricular activities or organizations at Columbia University, while Columbia University students are allowed in most, but not all, Barnard organizations.
Barnard's McIntosh Activities Council (commonly known as McAC), named after the first President of Barnard, Millicent McIntosh, organizes various community focused events on campus, such as Big Sub and Midnight Breakfast. McAC is made up of five sub-committees which are the Multi-Cultural committee, the Time-Out committee, the Network committee, the Community committee, and the Action committee. Each committee has a different focus, such as hosting and publicizing multi-cultural events (Multi-Cultural), having regular study breaks and relaxation events (Time-Out), giving students opportunities to be involved with Alumnae and various professionals (Network), planning events that bring the entire student body together (Community), and planning community service events that give back to the surrounding community (Action).
In 2011, Barnard's SGA and McAC will work together to bring back the Greek Games, an old but quite famous Barnard tradition.
Barnard College officially banned sororities in 1913, but Barnard students continue to participate in Columbia's five National Panhellenic Conference sororities—Alpha Chi Omega, Alpha Omicron Pi, Delta Gamma, Kappa Alpha Theta, and Sigma Delta Tau—and the National Pan-Hellenic Council Sororities- Alpha Kappa Alpha (Lambda chapter) and Delta Sigma Theta (Rho chapter) as well as other sororities in the Multicultural Greek Council. Two National Panhellenic Conference organizations were founded at Barnard College. The Alpha Omicron Pi Fraternity, founded on January 2, 1897, left campus during the 1913 ban but returned to establish its Alpha chapter in 2013. The Alpha Epsilon Phi, founded on October 24, 1909, is no longer on campus. As of 2010, Barnard does not fully recognize the National Panhellenic Conference sororities at Columbia, but it does provide some funding to account for Barnard students living in Columbia housing through these organizations.
Athletics.
Barnard athletes compete in the Ivy League (NCAA Division I) through the Columbia/Barnard Athletic Consortium, which was established in 1983. Through this arrangement, Barnard is the only women's college offering Division I athletics. There are 15 intercollegiate teams, and students also compete at the intramural and club levels.
From 1975–1983, before the establishment of the Columbia/Barnard Athletic Consortium, Barnard students competed as the "Barnard Bears". Prior to 1975, students referred to themselves as the "Barnard honeybears". 
Seven Sisters—student collaborations.
Established within the Barnard Student Government Association (SGA), The Seven Sisters Governing Board represents Barnard College as part of the Seven Sisters Coalition, which is a group of representatives from student councils of the historic Seven Sisters colleges. The reps on the coordinating board of Seven Sisters Coalition are rotating every year to hold the annual Seven Sisters Conference in a serious but informal setting. The first Seven Sisters Conference was hosted by SGA student representatives at Barnard College in 2009. In fall 2013, the conference was hosted by Vassar college during the first weekend of November. The major topic focused on inner college collaborations and differences in student government structures among Seven Sisters Colleges. The Seven Sisters Coordinating Board of Barnard brought six Barnard student representatives to attend the Fall Semester conference, which was hosted at Vassar College in the past fall semester. Based on the Coalition Coordinating Board Constitution established in February 2013, Students delegates were initiating projects in the aspects of public relations,alumni outreach and website management to promote the presence and development of the seven sisters culture. Meanwhile, The Barnard delegates engaged in discussions about the various structures of the student governments among the historic seven sisters colleges.
Sustainability.
Barnard College has issued a statement affirming its commitment to environmental sustainability, a major part of which is the goal of reducing its greenhouse gas emissions by 30% by 2017. Student EcoReps work as a resource on environmental issues for students in Barnard's residence halls, while the student-run Earth Coalition works on outreach initiatives such as local park clean-ups, tutoring elementary school students in environmental education, and sponsoring environmental forums. Barnard earned a "C-" for its sustainability efforts on the College Sustainability Report Card 2009 published by the Sustainable Endowments Institute. Its highest marks were in Student Involvement and Food and Recycling, receiving a "B" in both categories.
Nine Ways of Knowing.
Nine Ways of Knowing are liberal arts requirements. Students must take one year of one laboratory science, study a single foreign language for four semesters, and complete one 3-credit course in each of the following categories: reason and value, social analysis, historical studies, cultures in comparison, quantitative and deductive reasoning, literature, and visual and performing arts. The use of AP or IB credit to fulfill these requirements is very limited, but Nine Ways of Knowing courses may overlap with major or minor requirements. In addition to the Nine Ways of Knowing, students must complete a first-year seminar, a first-year English course, and one semester of physical education.
Controversies.
In the spring of 1960 Columbia University President Grayson Kirk complained to the President of Barnard that Barnard students were wearing inappropriate clothing. The garments in question were pants and Bermuda shorts. The administration forced the Student Council to institute a dress code. Students would be allowed to wear shorts and pants only at Barnard and only if the shorts were no more than two inches above the knee and the pants were not tight. Barnard women crossing the street to enter the Columbia campus wearing shorts or pants were required to cover themselves with a long coat similar to a jilbab.
In March 1968, "The New York Times" ran an article on students who cohabited, identifying one of the persons they interviewed as a student at Barnard College from New Hampshire named "Susan". Barnard officials searched their records for women from New Hampshire and were able to determine that "Susan" was the pseudonym of a student (Linda LeClair) who was living with her boyfriend, a student at Columbia University. She was called before Barnard's student-faculty administration judicial committee, where she faced the possibility of expulsion. A student protest included a petition signed by 300 other Barnard women, admitting that they too had broken the regulations against cohabitating. The judicial committee reached a compromise and the student was allowed to remain in school, but was denied use of the college cafeteria and barred from all social activities. The student briefly became a focus of intense national attention. She eventually dropped out of Barnard.
In October 2011, the Barnard administration issued a controversial policy which mandated that every student must pay full-time tuition as of Fall 2012, regardless of how many credits were taken. Students, families and faculty alike responded with a petition on Change.org and a protest from students.

</doc>
<doc id="4240" url="http://en.wikipedia.org/wiki?curid=4240" title="Order of Saint Benedict">
Order of Saint Benedict

The Order of Saint Benedict (OSB; Latin: "Ordo Sancti Benedicti"), also knownin reference to the colour of its members' habitsas the Black Monks, is a Roman Catholic religious order of independent monastic communities that observe the Rule of Saint Benedict. Each community (monastery, priory or abbey) within the order maintains its own autonomy, while the order itself represents their mutual interests. The terms "Order of Saint Benedict" and "Benedictine Order" are, however, also used to refer to Benedictine communities "in toto", sometimes giving the incorrect impression that there exists a generalate or motherhouse with jurisdiction over them.
Internationally, the order is governed by the Benedictine Confederation, a body, established in 1883 by Pope Leo XIII's Brief "Summum semper", whose head is known as the Abbot Primate. Individuals whose communities are members of the order generally add the initials "OSB" after their names.
Historical development.
The monastery at Subiaco in Italy, established by Saint Benedict of Nursia circa 529, was the first of the dozen monasteries he founded. There is no evidence, however, that he intended to found an order and the Rule of St Benedict presupposes the autonomy of each community. As most monasteries founded during the Middle Ages adopted the Rule of St Benedict, it became a standard form of Western monasticism despite the absence of a Benedictine order.
Today, Benedictine monasticism is fundamentally different from other Western religious orders insofar as its individual communities are not part of a religious order with "Generalates" and "Superiors General". Rather, in modern times, the various autonomous houses have formed themselves loosely into congregations (for example, Cassinese, English, Solesmes, Subiaco, Camaldolese, Sylvestrines) that in turn are represented in the Benedictine Confederation that came into existence through Pope Leo XIII's Apostolic Brief "Summum semper" on July 12, 1883. This organization facilitates dialogue of Benedictine communities with each other and the relationship between Benedictine communities and other religious orders and the church at large.
The Rule of Saint Benedict is also used by a number of religious orders that began as reforms of the Benedictine tradition such as the Cistercians and Trappists although none of these groups are part of the Benedictine Confederation.
The largest number of Benedictines are Roman Catholics, but there are also some within the Anglican Communion and occasionally within other Christian denominations as well, for example, within the Lutheran Church.
England.
In the English Reformation, all monasteries were dissolved and their lands confiscated by the Crown, forcing their Catholic members to flee into exile on the Continent. During the 19th century they were able to return to England, including to Selby Abbey in Yorkshire, one of the few great monastic churches to survive the Dissolution.
St. Mildred's Priory, on the Isle of Thanet, Kent, was built in 1027 on the site of an abbey founded in 670 by the daughter of the first Christian King of Kent. Currently the priory is home to a community of Benedictine nuns. Four of the most notable English abbeys are the Basilica of St Gregory the Great at Downside, commonly known as Downside Abbey, Ealing Abbey in Ealing, West London, St. Lawrence's in Yorkshire (Ampleforth Abbey), and Worth Abbey. Prinknash Abbey, used by Henry VIII as a hunting lodge, was officially returned to the Benedictines four hundred years later, in 1928. During the next few years, so-called Prinknash Park was used as a home until it was returned to the order.
Since the Oxford Movement, there has also been a modest flourishing of Benedictine monasticism in the Anglican Church and Protestant Churches. Anglican Benedictine Abbots are invited guests of the Benedictine Abbot Primate in Rome at Abbatial gatherings at Sant'Anselmo. There are an estimated 2,400 celibate Anglican Religious (1,080 men and 1,320 women) in the Anglican Communion as a whole, some of whom have adopted the Rule of St. Benedict. For a full list of all historic Benedictine houses in England and Wales, see below.
France.
Monasticism had been introduced into the region of modern France during the Roman era by Saint Martin of Tours, who founded the first monastery in Western Europe. The Rule of St. Benedict was promoted by various rulers of France, especially the House of Capet. Figures such as Benedict of Aniane were authorized by the Emperor Louis the Pious and his successors to promote its adoption by monasteries throughout the Holy Roman Empire. It expanded throughout the next millennium, growing through periods of revival and decay over the centuries. Monasteries were among the institutions of the Catholic Church swept away during the French Revolution.
Monasteries were again allowed to form in the 19th century under the Bourbon Restoration. Later that century, under the Third French Republic, laws were enacted preventing religious teaching. The original intent was to allow secular schools. Thus in 1880 and 1882, Benedictine teaching monks were effectively exiled; this was not completed until 1901.
Benedictine vow and life.
Section 17 in chapter 58 of the Rule of Saint Benedict states the solemn promise candidates for reception into a Benedictine community are required to make: a promise of stability (i.e. to remain in the same community), "conversatio morum" (an idiomatic Latin phrase suggesting "conversion of manners"; see below) and obedience (to the community's superior, seen as holding the place of Christ within it). This solemn commitment tends to be referred to as the "Benedictine vow" and is the Benedictine antecedent and equivalent of the evangelical counsels professed by candidates for reception into a religious order.
Much scholarship over the last fifty years has been dedicated to the translation and interpretation of ""conversatio morum"". The older translation "conversion of life" has generally been replaced with phrases such as "to a monastic manner of life", drawing from the Vulgate's use of "conversatio" as a translation of "citizenship" or "homeland" in Philippians 3:20. Some scholars have claimed that the vow formula of the Rule is best translated as "to live in this place as a monk, in obedience to its rule and abbot."
Benedictine abbots and abbesses have full jurisdiction of their abbey and thus absolute authority over the monks or nuns who are resident. This authority includes the power to assign duties, to decide which books may or may not be read, to regulate comings and goings, and to punish and to excommunicate, in the sense of an enforced isolation from the monastic community.
A tight communal timetablethe horariumis meant to ensure that the time given by God is not wasted but used in God's service, whether for prayer, work, meals, spiritual reading or sleep.
Although Benedictines do not take a vow of silence, hours of strict silence are set, and at other time silence is maintained as much as is practically possible. Social conversations tend to be limited to communal recreation times. But such details, like the many other details of the daily routine of a Benedictine house that the Rule of St Benedict leaves to the discretion of the superior, are set out in its customary.
In the Roman Catholic Church, according to the norms of the 1983 Code of Canon Law, a Benedictine abbey is a "religious institute" and its members are therefore members of the consecrated life. While Canon Law 588 §1 explains that Benedictine monks are "neither clerical nor lay", they can, however, be ordained. Benedictine Oblates endeavor to embrace the spirit of the Benedictine vow in their own life in the world.

</doc>
<doc id="4241" url="http://en.wikipedia.org/wiki?curid=4241" title="Bayezid I">
Bayezid I

Bayezid I (; ; nicknamed "Yıldırım" (Ottoman Turkish: ییلدیرم), "The Thunderbolt"; 1354 – 8 March 1403) was the Sultan of the Ottoman Empire from 1389 to 1402. He was the son of Murad I and Valide Sultan Gülçiçek Hatun.
Biography.
Bayezid ascended to the throne following the death of his father Murad I, who was killed by Serbian knight Miloš Obilić during (15 June), or immediately after (16 June), the Battle of Kosovo in 1389, by which Serbia became a vassal of the Ottoman Empire. Immediately after obtaining the throne, he had his younger brother strangled to avoid a plot. In 1390, Bayezid took as a wife Princess Olivera Despina, the daughter of Prince Lazar of Serbia, who also lost his life in Kosovo. Bayezid recognized Stefan Lazarević, the son of Lazar, as the new Serbian leader (later despot), with considerable autonomy.
From 1389 to 1395 he conquered Bulgaria and northern Greece. In 1394 he crossed the River Danube to attack Wallachia, ruled at that time by Mircea the Elder. The Ottomans were superior in number, but on 10 October 1394 (or 17 May 1395), in the Battle of Rovine, on forested and swampy terrain, the Wallachians won the fierce battle and prevented Bayezid's army from advancing beyond the Danube.
Meanwhile, he begin the reunification of the Turkish Anatolia, conquering the beyliks of Aydin, Saruhan in 1390, the beyliks of Mentese, Germiyan and Kastamonu in 1391; and finally the great emirate of Karaman and the ex-emirate of Burhan-ad-Din in Tokat, Sivas and Kayseri(1397–98). Next he occuped the cities of Malatya and Elbistan, in a war with the mamluk sultan of Egypt.
In 1394, Bayezid laid siege to Constantinople, the capital of the Byzantine Empire. Anadoluhisarı fortress was built between 1393 and 1394 as part of preparations for the Second Ottoman Siege of Constantinople, which took place in 1395. On the urgings of the Byzantine emperor Manuel II Palaeologus a new crusade was organized to defeat him. This proved unsuccessful: in 1396 the Christian allies, under the leadership of the King of Hungary and future Holy Roman Emperor (in 1433) Sigismund, were defeated in the Battle of Nicopolis. Bayezid built the magnificent Ulu Cami in Bursa, to celebrate this victory.
Thus, the siege of Constantinople continued, lasting until 1402. The beleaguered Byzantines had their reprieve when Bayezid fought the Timurid in the East. At this time, the empire of Bayezid included Thrace (except Constantinople), Macedonia, Bulgaria, and parts of Serbia in Europe. In Asia, his domains extended to the Taurus Mountains. His army was considered one of the best in the Islamic world. In 1400, the Central Asian warlord Timur succeeded in rousing the local Turkic beyliks that had been vassals of the Ottomans to join him in his attack on Bayezid, who was also considered one of the most powerful rulers in the Muslim world during that period. In the fateful Battle of Ankara, on 20 July 1402, Bayezid was captured by Timur and the Ottoman army was defeated. Many writers claim that Bayezid was mistreated by the Timurids. However, writers and historians from Timur's own court reported that Bayezid was treated well, and that Timur even mourned his death. One of Bayezid's sons, Mustafa Çelebi, was captured with him and held captive in Samarkand until 1405.
Four of Bayezid's sons, specifically Süleyman Çelebi, İsa Çelebi, Mehmed Çelebi, and Musa Çelebi, however, escaped from the battlefield and later started a civil war for the Ottoman throne known as the Ottoman Interregnum. After Mehmed's victory, his coronation as Mehmed I, and the death of all four but Mehmed, Bayezid's other son Mustafa Çelebi emerged from hiding and began two failed rebellions against his brother Mehmed and, after Mehmed's death, his nephew Murat II.
Legacy.
A commando battalion in the Pakistan Army is named Yaldaram Battalion after him.
Yildirim Beyazit University, a state university in Turkey, is also named after him.
Marriages and progeny.
His mother was Valide Sultan Gülçiçek Hatun who was of ethnic Greek descent.
In fiction.
The defeat of Bayezid became a popular subject for later Western writers, composers, and painters. They embellished the legend that he was taken by Timur to Samarkand with a cast of characters to create an oriental fantasy that has maintained its appeal. Christopher Marlowe's play "Tamburlane the Great" was first performed in London in 1587, three years after the formal opening of English-Ottoman trade relations when William Harborne sailed for Constantinople as an agent of the Levant Company. In 1648, the play "Le Gran Tamerlan et Bejezet" by Jean Magnon appeared in London, and in 1725, Handel's "Tamerlano" was first performed and published in London; Vivaldi's version of the story, "Bajazet", was written in 1735. Magnon had given Bayezid an intriguing wife and daughter; the Handel and Vivaldi renditions included, as well as Tamerlane and Bayezid and his daughter, a prince of Byzantium and a princess of Trebizond (Trabzon) in a passionate love story. A cycle of paintings in Schloss Eggenberg, near Graz in Austria, translated the theme to a different medium; this was completed in the 1670s shortly before the Ottoman army attacked the Habsburgs in central Europe. Bayezid (spelled Bayazid) is a central character in the Robert E. Howard story "Lord of Samarcand."
External links.
47–48

</doc>
<doc id="4242" url="http://en.wikipedia.org/wiki?curid=4242" title="Bayezid II">
Bayezid II

Bayezid II or Sultân Bayezid-î Velî (December 3, 1447 – May 26, 1512) (Ottoman Turkish: بايزيد ثانى "Bāyezīd-i sānī", Turkish:"II. Bayezid" or "II. Beyazıt") was the eldest son and successor of Mehmed II, ruling as Sultan of the Ottoman Empire from 1481 to 1512. During his reign, Bayezid II consolidated the Ottoman Empire and thwarted a Safavid rebellion soon before abdicating his throne to his son, Selim I. He is most notable for evacuating Jews from Spain after the proclamation of the Alhambra Decree and resettling them throughout the Ottoman Empire.
Early life.
Bayezid II was born in Dimetoka Palace (now Didymoteicho) in Thrace as the son of Mehmed II (1432–81) and Valide Sultan Emine Gülbahar Hatun, who died in 1492. Bayezid II married Ayşe Hatun, who was the mother of his eldest son Şehzade Ahmet, as well as Bayezid II's heir and successor, Selim I.
Fight for the throne.
Bayezid II's overriding concern was the quarrel with his brother Cem, who claimed the throne and sought military backing from the Mamluks in Egypt. Having been defeated by his brother's armies, Cem sought protection from the Knights of St. John in Rhodes. Eventually, the Knights handed Cem over to Pope Innocent VIII (1484–1492). The Pope thought of using Cem as a tool to drive the Turks out of Europe, but, as the papal crusade failed to come to fruition, Cem was left to languish and die in a Neapolitan prison.
Reign.
Bayezid II ascended the Ottoman throne in 1481. Like his father, Bayezid II was a patron of western and eastern culture and unlike many other Sultans, worked hard to ensure a smooth running of domestic politics, which earned him the epithet of "the Just". Throughout his reign, Bayezid II engaged in numerous campaigns to conquer the Venetian possessions in Morea, accurately defining this region as the key to future Ottoman naval power in the Eastern Mediterranean. The last of these wars ended in 1501 with Bayezid II in control of the whole Peloponnese. Rebellions in the east, such as that of the Qizilbash, plagued much of Bayezid II's reign and were often backed by the Shah of Persia, Ismail, who was eager to promote Shi'ism to undermine the authority of the Ottoman state. Ottoman authority in Anatolia was indeed seriously threatened during this period, and at one point Bayezid II's grand vizier, Ali Pasha, was killed in battle against rebels.
Jewish and Muslim immigration.
In July 1492, the new state of Spain expelled its Jewish and Muslim populations as part of the Spanish Inquisition. Bayezid II sent out the Ottoman Navy under the command of Admiral Kemal Reis to Spain in 1492 in order to evacuate them safely to Ottoman lands. He sent out proclamations throughout the empire that the refugees were to be welcomed. He granted the refugees the permission to settle in the Ottoman Empire and become Ottoman citizens. He ridiculed the conduct of Ferdinand II of Aragon and Isabella I of Castile in expelling a class of people so useful to their subjects. "You venture to call Ferdinand a wise ruler," he said to his courtiers — "he who has impoverished his own country and enriched mine!" Bayezid addressed a firman to all the governors of his European provinces, ordering them not only to refrain from repelling the Spanish refugees, but to give them a friendly and welcome reception. He threatened with death all those who treated the Jews harshly or refused them admission into the empire. Moses Capsali, who probably helped to arouse the sultan's friendship for the Jews, was most energetic in his assistance to the exiles. He made a tour of the communities, and was instrumental in imposing a tax upon the rich, to ransom the Jewish victims of the persecutions then prevalent.
The Muslims and Jews of al-Andalus (Iberia) contributed much to the rising power of the Ottoman Empire by introducing new ideas, methods and craftsmanship. The first printing press in Constantinople was established by the Sephardic Jews in 1493. It is reported that under Bayezid's reign, Jews enjoyed a period of cultural flourishing, with the presence of such scholars as the Talmudist and scientist Mordecai Comtino; astronomer and poet Solomon ben Elijah Sharbiṭ ha-Zahab; Shabbethai ben Malkiel Cohen, and the liturgical poet Menahem Tamar.
Succession.
On September 14, 1509, Constantinople was devastated by an earthquake. Bayezid II's final years saw a succession battle between his sons Selim I and Ahmet. Ahmet unexpectedly captured Karaman, an Ottoman city, and began marching to Constantinople to exploit his triumph. Fearing for his safety, Selim staged a revolt in Thrace but was defeated by Bayezid and forced to flee back to the Crimean Peninsula. Bayezid II developed fears that Ahmet might in turn kill him to gain the throne and refused to allow his son to enter Constantinople.
Selim returned from Crimea and, with support from the Janissaries, forced his father to abdicate the throne on April 25, 1512. Beyazid departed for retirement in his native Demotika, but he died on May 26, 1512 at Büyükçekmece before reaching his destination, and only a month after his abdication. He was buried next to the Bayezid Mosque in Istanbul.

</doc>
<doc id="4243" url="http://en.wikipedia.org/wiki?curid=4243" title="Boxing">
Boxing

Boxing (pugilism, prize fighting, the sweet science or in Greek pygmachia) is a combat sport in which two people engage in a contest of strength, speed, reflexes, endurance, and will, by throwing punches with gloved hands against each other.
Amateur boxing is both an Olympic and Commonwealth sport and is a common fixture in most of the major international games—it also has its own World Championships. Boxing is supervised by a referee over a series of one- to three-minute intervals called rounds. The result is decided when an opponent is deemed incapable to continue by a referee, is disqualified for breaking a rule, resigns by throwing in a towel, or is pronounced the winner or loser based on the judges' scorecards at the end of the contest.
The origin of boxing may be its acceptance by the ancient Greeks as an Olympic game in BCE 688. Boxing evolved from 16th- and 18th-century prizefights, largely in Great Britain, to the forerunner of modern boxing in the mid-19th century, again initially in Great Britain and later in the United States.
History.
Early history.
"See also Ancient Greek boxing
The earliest known depiction of boxing comes from a Sumerian relief from the 3rd millennium BC. Later depictions from the 2nd millennium BC are found in reliefs from the Mesopotamian nations of Assyria and Babylonia, and in Hittite art from Asia Minor. The earliest evidence for fist fighting with any kind of gloves can be found on Minoan Crete (c. 1500–900 BC), and on Sardinia, if we consider the boxing statues of Prama mountains (c. 2000–1000 BC).
Boxing was a popular spectator sport in Ancient Rome. In order for the fighters to protect themselves against their opponents they wrapped leather thongs around their fists. Eventually harder leather was used and the thong soon became a weapon. The Romans even introduced metal studs to the thongs to make the cestus which then led to a more sinister weapon called the myrmex (‘limb piercer’). Fighting events were held at Roman Amphitheatres. The Roman form of boxing was often a fight until death to please the spectators who gathered at such events. However, especially in later times, purchased slaves and trained combat performers were valuable commodities, and their lives were not given up without due consideration. Often slaves were used against one another in a circle marked on the floor. This is where the term ring came from. In 393 AD, during the Roman gladiator period, boxing was abolished due to excessive brutality. It was not until the late 17th century that boxing re-surfaced in London.
Modern boxing.
Broughton's rules (1743).
Records of Classical boxing activity disappeared after the fall of the Western Roman Empire when the wearing of weapons became common once again and interest in fighting with the fists waned. However, there are detailed records of various fist-fighting sports that were maintained in different cities and provinces of Italy between the 12th and 17th centuries. There was also a sport in ancient Rus called Kulachniy Boy or "Fist Fighting".
As the wearing of swords became less common, there was renewed interest in fencing with the fists. The sport would later resurface in England during the early 16th century in the form of bare-knuckle boxing sometimes referred to as prizefighting. The first documented account of a bare-knuckle fight in England appeared in 1681 in the "London Protestant Mercury", and the first English bare-knuckle champion was James Figg in 1719. This is also the time when the word "boxing" first came to be used. It should be noted, that this earliest form of modern boxing was very different. Contests in Mr. Figg's time, in addition to fist fighting, also contained fencing and cudgeling. On 6 January 1681, the first recorded boxing match took place in Britain when Christopher Monck, 2nd Duke of Albemarle (and later Lieutenant Governor of Jamaica) engineered a bout between his butler and his butcher with the latter winning the prize.
Early fighting had no written rules. There were no weight divisions or round limits, and no referee. In general, it was extremely chaotic. The first boxing rules, called the Broughton's rules, were introduced by champion Jack Broughton in 1743 to protect fighters in the ring where deaths sometimes occurred. Under these rules, if a man went down and could not continue after a count of 30 seconds, the fight was over. Hitting a downed fighter and grasping below the waist were prohibited. Broughton also invented and encouraged the use of "mufflers", a form of padded gloves, which were used in training and exhibitions. The first paper on boxing was published in the early 1700s by a successful Cornish Wrestler from Bunnyip, Cornwall, named Sir Thomas Parkyns, who was also a Physics student of Sir Isaac Newton. The paper was actually a single page in his extensive Wrestling & Fencing manual that entailed a system of headbutting, punching, eye gouging, chokes, and hard throws not common in modern Boxing. Parkyns added the techniques described in his paper to his own fighting style.
These rules did allow the fighters an advantage not enjoyed by today's boxers; they permitted the fighter to drop to one knee to begin a 30-second count at any time. Thus a fighter realizing he was in trouble had an opportunity to recover. However, this was considered "unmanly" and was frequently disallowed by additional rules negotiated by the Seconds of the Boxers. Intentionally going down in modern boxing will cause the recovering fighter to lose points in the scoring system. Furthermore, as the contestants did not have heavy leather gloves and wristwraps to protect their hands, they used different punching technique to preserve their hands because the head was a common target to hit full out as almost all period manuals have powerful straight punches with the whole body behind them to the face (including forehead) as the basic blows.
London Prize Ring rules (1838).
In 1838, the London Prize Ring rules were codified. Later revised in 1853, they stipulated the following:
Marquess of Queensberry rules (1867).
In 1867, the Marquess of Queensberry rules were drafted by John Chambers for amateur championships held at Lillie Bridge in London for Lightweights, Middleweights and Heavyweights. The rules were published under the patronage of the Marquess of Queensberry, whose name has always been associated with them.
There were twelve rules in all, and they specified that fights should be "a fair stand-up boxing match" in a 24-foot-square or similar ring. Rounds were three minutes with one-minute rest intervals between rounds. Each fighter was given a ten-second count if he were knocked down, and wrestling was banned.
The introduction of gloves of "fair-size" also changed the nature of the bouts. An average pair of boxing gloves resembles a bloated pair of mittens and are laced up around the wrists.
The gloves can be used to block an opponent's blows. As a result of their introduction, bouts became longer and more strategic with greater importance attached to defensive maneuvers such as slipping, bobbing, countering and angling. Because less defensive emphasis was placed on the use of the forearms and more on the gloves, the classical forearms outwards, torso leaning back stance of the bare knuckle boxer was modified to a more modern stance in which the torso is tilted forward and the hands are held closer to the face.
Modern.
Through the late nineteenth century, the martial art of boxing or prizefighting was primarily a sport of dubious legitimacy. Outlawed in England and much of the United States, prizefights were often held at gambling venues and broken up by police. Brawling and wrestling tactics continued, and riots at prizefights were common occurrences. Still, throughout this period, there arose some notable bare knuckle champions who developed fairly sophisticated fighting tactics.
The English case of "R v. Coney" in 1882 found that a bare-knuckle fight was an assault occasioning actual bodily harm, despite the consent of the participants. This marked the end of widespread public bare-knuckle contests in England.
The first world heavyweight champion under the Queensberry Rules was "Gentleman Jim" Corbett, who defeated John L. Sullivan in 1892 at the Pelican Athletic Club in New Orleans.
The first instance of film censorship in the United States occurred in 1897 when several states banned the showing of prize fighting films from the state of Nevada, where it was legal at the time.
Throughout the early twentieth century, boxers struggled to achieve legitimacy. They were aided by the influence of promoters like Tex Rickard and the popularity of great champions such as John L. Sullivan.
Rules.
The "Marquess of Queensberry rules" have been the general rules governing modern boxing since their publication in 1867.
A boxing match typically consists of a determined number of three-minute rounds, a total of up to 12 rounds (formerly 15). A minute is typically spent between each round with the fighters in their assigned corners receiving advice and attention from their coach and staff. The fight is controlled by a referee who works within the ring to judge and control the conduct of the fighters, rule on their ability to fight safely, count knocked-down fighters, and rule on fouls.
Up to three judges are typically present at ringside to score the bout and assign points to the boxers, based on punches that connect, defense, knockdowns, and other, more subjective, measures. Because of the open-ended style of boxing judging, many fights have controversial results, in which one or both fighters believe they have been "robbed" or unfairly denied a victory. Each fighter has an assigned corner of the ring, where his or her coach, as well as one or more "seconds" may administer to the fighter at the beginning of the fight and between rounds. Each boxer enters into the ring from their assigned corners at the beginning of each round and must cease fighting and return to their corner at the signaled end of each round.
A bout in which the predetermined number of rounds passes is decided by the judges, and is said to "go the distance". The fighter with the higher score at the end of the fight is ruled the winner. With three judges, unanimous and split decisions are possible, as are draws. A boxer may win the bout before a decision is reached through a knock-out ; such bouts are said to have ended "inside the distance". If a fighter is knocked down during the fight, determined by whether the boxer touches the canvas floor of the ring with any part of their body other than the feet as a result of the opponent's punch and not a slip, as determined by the referee, the referee begins counting until the fighter returns to his or her feet and can continue.
Should the referee count to ten, then the knocked-down boxer is ruled "knocked out" (whether unconscious or not) and the other boxer is ruled the winner by knock-out (KO). A "technical knock-out" (TKO) is possible as well, and is ruled by the referee, fight doctor, or a fighter's corner if a fighter is unable to safely continue to fight, based upon injuries or being judged unable to effectively defend themselves. Many jurisdictions and sanctioning agencies also have a "three-knockdown rule", in which three knockdowns in a given round result in a TKO. A TKO is considered a knockout in a fighter's record. A "standing eight" count rule may also be in effect. This gives the referee the right to step in and administer a count of eight to a fighter that he feels may be in danger, even if no knockdown has taken place. After counting the referee will observe the fighter, and decide if he is fit to continue. For scoring purposes, a standing eight count is treated as a knockdown.
In general, boxers are prohibited from hitting below the belt, holding, tripping, pushing, biting, or spitting. The boxer's shorts are raised so the opponent is not allowed to hit to the groin area with intent to cause pain or injury. Failure to abide by the former may result in a foul. They also are prohibited from kicking, head-butting, or hitting with any part of the arm other than the knuckles of a closed fist (including hitting with the elbow, shoulder or forearm, as well as with open gloves, the wrist, the inside, back or side of the hand). They are prohibited as well from hitting the back, back of the neck or head (called a "rabbit-punch") or the kidneys. They are prohibited from holding the ropes for support when punching, holding an opponent while punching, or ducking below the belt of their opponent (dropping below the waist of your opponent, no matter the distance between).
If a "clinch" – a defensive move in which a boxer wraps his or her opponents arms and holds on to create a pause – is broken by the referee, each fighter must take a full step back before punching again (alternatively, the referee may direct the fighters to "punch out" of the clinch). When a boxer is knocked down, the other boxer must immediately cease fighting and move to the furthest neutral corner of the ring until the referee has either ruled a knockout or called for the fight to continue.
Violations of these rules may be ruled "fouls" by the referee, who may issue warnings, deduct points, or disqualify an offending boxer, causing an automatic loss, depending on the seriousness and intentionality of the foul. An intentional foul that causes injury that prevents a fight from continuing usually causes the boxer who committed it to be disqualified. A fighter who suffers an accidental low-blow may be given up to five minutes to recover, after which they may be ruled knocked out if they are unable to continue. Accidental fouls that cause injury ending a bout may lead to a "no contest" result, or else cause the fight to go to a decision if enough rounds (typically four or more, or at least three in a four-round fight) have passed.
Unheard of these days, but common during the early 20th Century in North America, a "newspaper decision (NWS)" might be made after a no decision bout had ended. A "no decision" bout occurred when, by law or by pre-arrangement of the fighters, if both boxers were still standing at the fight's conclusion and there was no knockout, no official decision was rendered and neither boxer was declared the winner. But this did not prevent the pool of ringside newspaper reporters from declaring a consensus result among themselves and printing a newspaper decision in their publications. Officially, however, a "no decision" bout resulted in neither boxer winning or losing. Boxing historians sometimes use these unofficial newspaper decisions in compiling fight records for illustrative purposes only. Often, media outlets covering a match will personally score the match, and post their scores as an independent sentence in their report.
Professional vs. amateur boxing.
Throughout the 17th through 19th centuries, boxing bouts were motivated by money, as the fighters competed for prize money, promoters controlled the gate, and spectators bet on the result. The modern Olympic movement revived interest in amateur sports, and amateur boxing became an Olympic sport in 1908. In their current form, Olympic and other amateur bouts are typically limited to three or four rounds, scoring is computed by points based on the number of clean blows landed, regardless of impact, and fighters wear protective headgear, reducing the number of injuries, knockdowns, and knockouts. Currently scoring blows in amateur boxing are subjectively counted by ringside judges, but the Australian Institute for Sport has demonstrated a prototype of an Automated Boxing Scoring System, which introduces scoring objectivity, improves safety, and arguably makes the sport more interesting to spectators. Professional boxing remains by far the most popular form of the sport globally, though amateur boxing is dominant in Cuba and some former Soviet republics. For most fighters, an amateur career, especially at the Olympics, serves to develop skills and gain experience in preparation for a professional career.
Amateur boxing.
Amateur boxing may be found at the collegiate level, at the Olympic Games and Commonwealth Games, and in many other venues sanctioned by amateur boxing associations. Amateur boxing has a point scoring system that measures the number of clean blows landed rather than physical damage. Bouts consist of three rounds of three minutes in the Olympic and Commonwealth Games, and three rounds of three minutes in a national ABA (Amateur Boxing Association) bout, each with a one-minute interval between rounds.
Competitors wear protective headgear and gloves with a white strip or circle across the knuckle. A punch is considered a scoring punch only when the boxers connect with the white portion of the gloves. Each punch that lands cleanly on the head or torso with sufficient force is awarded a point. A referee monitors the fight to ensure that competitors use only legal blows. A belt worn over the torso represents the lower limit of punches – any boxer repeatedly landing low blows below the belt is disqualified. Referees also ensure that the boxers don't use holding tactics to prevent the opponent from swinging. If this occurs, the referee separates the opponents and orders them to continue boxing. Repeated holding can result in a boxer being penalized or ultimately disqualified. Referees will stop the bout if a boxer is seriously injured, if one boxer is significantly dominating the other or if the score is severely imbalanced. Amateur bouts which end this way may be noted as "RSC" (referee stopped contest) with notations for an outclassed opponent (RSCO), outscored opponent (RSCOS), injury (RSCI) or head injury (RSCH).
Professional boxing.
Professional bouts are usually much longer than amateur bouts, typically ranging from ten to twelve rounds, though four round fights are common for less experienced fighters or club fighters. There are also some two- and three-round professional bouts, especially in Australia. Through the early twentieth century, it was common for fights to have unlimited rounds, ending only when one fighter quit, benefiting high-energy fighters like Jack Dempsey. Fifteen rounds remained the internationally recognized limit for championship fights for most of the twentieth century until the early 1980s, when the death of boxer Duk Koo Kim eventually prompted the World Boxing Council and other organizations sanctioning professional boxing to reduce the limit to twelve rounds.
Headgear is not permitted in professional bouts, and boxers are generally allowed to take much more damage before a fight is halted. At any time, however, the referee may stop the contest if he believes that one participant cannot defend himself due to injury. In that case, the other participant is awarded a technical knockout win. A technical knockout would also be awarded if a fighter lands a punch that opens a cut on the opponent, and the opponent is later deemed not fit to continue by a doctor because of the cut. For this reason, fighters often employ cutmen, whose job is to treat cuts between rounds so that the boxer is able to continue despite the cut. If a boxer simply quits fighting, or if his corner stops the fight, then the winning boxer is also awarded a technical knockout victory. In contrast with amateur boxing, professional male boxers have to be bare chested.
Boxing styles.
Definition of style.
"Style" is often defined as the strategic approach a fighter takes during a bout. No two fighters' styles are alike, as it is determined by that individual's physical and mental attributes. There are three main styles in boxing: out-fighter ("boxer"), brawler (or "slugger"), and In-fighter ("swarmer"). These styles may be divided into several special subgroups, such as counter puncher, etc. The main philosophy of the styles is, that each style has an advantage over one, but disadvantage over the other one. It follows the rock-paper-scissors scenario - boxer beats brawler, swarmer beats boxer, and brawler beats swarmer.
Boxer/out-fighter.
A classic "boxer" or stylist (also known as an "out-fighter") seeks to maintain distance between himself and his opponent, fighting with faster, longer range punches, most notably the jab, and gradually wearing his opponent down. Due to this reliance on weaker punches, out-fighters tend to win by point decisions rather than by knockout, though some out-fighters have notable knockout records. They are often regarded as the best boxing strategists due to their ability to control the pace of the fight and lead their opponent, methodically wearing him down and exhibiting more skill and finesse than a brawler. Out-fighters need reach, hand speed, reflexes, and footwork.
Notable out-fighters include Muhammad Ali, Larry Holmes, Joe Calzaghe, Floyd Mayweather Jr., Wilfredo Gómez, 
Salvador Sanchez, Cecilia Brækhus, Gene Tunney, Ezzard Charles, Willie Pep, Meldrick Taylor, Ricardo Lopez, Roy Jones, Jr., and Sugar Ray Leonard. This style was also used by fictional boxer Apollo Creed.
Boxer-puncher.
A boxer-puncher is a well-rounded boxer who is able to fight at close range with a combination of technique and power, often with the ability to knock opponents out with a combination and in some instances a single shot. Their movement and tactics are similar to that of an out-fighter (although they are generally not as mobile as an out-fighter), but instead of winning by decision, they tend to wear their opponents down using combinations and then move in to score the knockout. A boxer must be well rounded to be effective using this style.
Notable boxer-punchers include Manny Pacquiao, Wladimir Klitschko, Lennox Lewis, Joe Louis, Wilfredo Gómez, Oscar de la Hoya, Archie Moore, Miguel Cotto, Nonito Donaire, Sam Langford, Henry Armstrong, Sugar Ray Robinson, Tony Zale, Carlos Monzón, Alexis Argüello, Erik Morales, Terry Norris, Marco Antonio Barrera, Naseem Hamed, Thomas Hearns and Victor Ortiz.
Counter puncher.
Counter punchers are slippery, defensive style fighters who often rely on their opponent's mistakes in order to gain the advantage, whether it be on the score cards or more preferably a knockout. They use their well-rounded defense to avoid or block shots and then immediately catch the opponent off guard with a well placed and timed punch. A fight with a skilled counter-puncher can turn into a war of attrition, where each shot landed is a battle in itself. Thus, fighting against counter punchers requires constant feinting and the ability to avoid telegraphing ones attacks. To be truly successful using this style they must have good reflexes, a high level of prediction and awareness, pinpoint accuracy and speed, both in striking and in footwork.
Notable counter punchers include Vitali Klitschko, Floyd Mayweather, Jr., Evander Holyfield, Max Schmeling, Chris Byrd, Jim Corbett, Jack Johnson, Bernard Hopkins, Laszlo Papp, Jerry Quarry, Anselmo Moreno, James Toney, Marvin Hagler, Juan Manuel Márquez, Humberto Soto, Roger Mayweather, Pernell Whitaker and Sergio Gabriel Martinez.
Counter punchers usually wear their opponents down by causing them to miss their punches. The more the opponent misses, the faster they'll tire, and the psychological effects of being unable to land a hit will start to sink in. The counter puncher often tries to outplay their opponent entirely, not just in a physical sense, but also in a mental and emotional sense. This style can be incredibly difficult, especially against seasoned fighters, but winning a fight without getting hit is often worth the pay-off. They usually try to stay away from the center of the ring, in order to outmaneuver and chip away at their opponents. A large advantage in counter-hitting is the forward momentum of the attacker, which drives them further into your return strike. As such, knockouts are more common than one would expect from a defensive style.
Brawler/slugger.
A brawler is a fighter who generally lacks finesse and footwork in the ring, but makes up for it through sheer punching power. Mainly Irish, Irish-American, Puerto Rican, Mexican, and Mexican-American boxers popularized this style. Many brawlers tend to lack mobility, preferring a less mobile, more stable platform and have difficulty pursuing fighters who are fast on their feet. They may also have a tendency to ignore combination punching in favour of continuous beat-downs with one hand and by throwing slower, more powerful single punches (such as hooks and uppercuts). Their slowness and predictable punching pattern (single punches with obvious leads) often leaves them open to counter punches, so successful brawlers must be able to absorb substantial amounts of punishment. However not all brawler/slugger fighters are not mobile, some can move around and switch styles if needed but still have the brawler/slugger style such as Wilfredo Gómez, Prince Naseem Hamed and Danny García.
A brawler's most important assets are power and chin (the ability to absorb punishment while remaining able to continue boxing). Examples of this style include George Foreman, Danny García, Wilfredo Gómez, Sonny Liston, John L. Sullivan, Max Baer, Prince Naseem Hamed, Ray Mancini, David Tua, Arturo Gatti, Micky Ward, Michael Katsidis, James Kirkland, Marcos Maidana, Jake Lamotta, Manny Pacquiao, and Ireland's John Duddy. This style of boxing was also used by fictional boxers Rocky Balboa and James "Clubber" Lang.
Brawlers tend to be more predictable and easy to hit but usually fare well enough against other fighting styles because they train to take punches very well. They often have a higher chance than other fighting styles to score a knockout against their opponents because they focus on landing big, powerful hits, instead of smaller, faster attacks. Oftentimes they place focus on training on their upper body instead of their entire body, to increase power and endurance. They also aim to intimidate their opponents because of their power, stature and ability to take a punch.
Swarmer/in-fighter.
In-fighters/swarmers (sometimes called "pressure fighters") attempt to stay close to an opponent, throwing intense flurries and combinations of hooks and uppercuts. A successful in-fighter often needs a good "chin" because swarming usually involves being hit with many jabs before they can maneuver inside where they are more effective. In-fighters operate best at close range because they are generally shorter and have less reach than their opponents and thus are more effective at a short distance where the longer arms of their opponents make punching awkward. However, several fighters tall for their division have been relatively adept at in-fighting as well as out-fighting.
The essence of a swarmer is non-stop aggression. Many short in-fighters utilize their stature to their advantage, employing a bob-and-weave defense by bending at the waist to slip underneath or to the sides of incoming punches. Unlike blocking, causing an opponent to miss a punch disrupts his balance, permits forward movement past the opponent's extended arm and keeps the hands free to counter. A distinct advantage that in-fighters have is when throwing uppercuts where they can channel their entire bodyweight behind the punch; Mike Tyson was famous for throwing devastating uppercuts. Marvin Hagler was known for his hard "chin", punching power, body attack and the stalking of his opponents. Some in-fighters, like Mike Tyson, have been known for being notoriously hard to hit. The key to a swarmer is aggression, endurance, chin, and bobbing-and-weaving.
Notable in-fighters include Julio César Chávez, Miguel Cotto, Joe Frazier, Danny García, Mike Tyson, Manny Pacquiao, Saúl Álvarez, Rocky Marciano, Jack Dempsey, Wayne McCullough, Harry Greb, David Tua and Ricky Hatton.
Combinations of styles.
All fighters have primary skills with which they feel most comfortable, but truly elite fighters are often able to incorporate auxiliary styles when presented with a particular challenge. For example, an out-fighter will sometimes plant his feet and counter punch, or a slugger may have the stamina to pressure fight with his power punches.
Style matchups.
There is a generally accepted rule of thumb about the success each of these boxing styles has against the others. In general, an in-fighter has an advantage over an out-fighter, an out-fighter has an advantage over a brawler, and a brawler has an advantage over an in-fighter; these form a cycle with each style being stronger relative to one, and weaker relative to another, with none dominating, as in rock-paper-scissors. Naturally, many other factors, such as the skill level and training of the combatants, determine the outcome of a fight, but the widely held belief in this relationship among the styles is embodied in the cliché amongst boxing fans and writers that "styles make fights."
Brawlers tend to overcome swarmers or in-fighters because, in trying to get close to the slugger, the in-fighter will invariably have to walk straight into the guns of the much harder-hitting brawler, so, unless the former has a very good chin and the latter's stamina is poor, the brawler's superior power will carry the day. A famous example of this type of match-up advantage would be George Foreman's knockout victory over Joe Frazier in their original bout "The Sunshine Showdown".
Although in-fighters struggle against heavy sluggers, they typically enjoy more success against out-fighters or boxers. Out-fighters prefer a slower fight, with some distance between themselves and the opponent. The in-fighter tries to close that gap and unleash furious flurries. On the inside, the out-fighter loses a lot of his combat effectiveness, because he cannot throw the hard punches. The in-fighter is generally successful in this case, due to his intensity in advancing on his opponent and his good agility, which makes him difficult to evade. For example, the swarming Joe Frazier, though easily dominated by the slugger George Foreman, was able to create many more problems for the boxer Muhammad Ali in their three fights. Joe Louis, after retirement, admitted that he hated being crowded, and that swarmers like untied/undefeated champ Rocky Marciano would have caused him style problems even in his prime.
The boxer or out-fighter tends to be most successful against a brawler, whose slow speed (both hand and foot) and poor technique makes him an easy target to hit for the faster out-fighter. The out-fighter's main concern is to stay alert, as the brawler only needs to land one good punch to finish the fight. If the out-fighter can avoid those power punches, he can often wear the brawler down with fast jabs, tiring him out. If he is successful enough, he may even apply extra pressure in the later rounds in an attempt to achieve a knockout. Most classic boxers, such as Muhammad Ali, enjoyed their best successes against sluggers.
An example of a style matchup was the historical fight of Julio César Chávez, a swarmer or in-fighter, against Meldrick Taylor, the boxer or out-fighter (see Julio César Chávez vs. Meldrick Taylor). The match was nicknamed "Thunder Meets Lightning" as an allusion to punching power of Chávez and blinding speed of Taylor. Chávez was the epitome of the "Mexican" style of boxing. Taylor's hand and foot speed and boxing abilities gave him the early advantage, allowing him to begin building a large lead on points. Chávez remained relentless in his pursuit of Taylor and due to his greater punching power Chávez slowly punished Taylor. Coming into the later rounds, Taylor was bleeding from the mouth, his entire face was swollen, the bones around his eye socket had been broken, he had swallowed a considerable amount of his own blood, and as he grew tired, Taylor was increasingly forced into exchanging blows with Chávez, which only gave Chávez a greater chance to cause damage. While there was little doubt that Taylor had solidly won the first three quarters of the fight, the question at hand was whether he would survive the final quarter. Going into the final round, Taylor held a secure lead on the scorecards of two of the three judges. Chávez would have to knock Taylor out to claim a victory, whereas Taylor merely needed to stay away from the Mexican legend. However, Taylor did not stay away, but continued to trade blows with Chávez. As he did so, Taylor showed signs of extreme exhaustion, and every tick of the clock brought Taylor closer to victory unless Chávez could knock him out.
With about a minute left in the round, Chávez hit Taylor squarely with several hard punches and stayed on the attack, continuing to hit Taylor with well-placed shots. Finally, with about 25 seconds to go, Chávez landed a hard right hand that caused Taylor to stagger forward towards a corner, forcing Chávez back ahead of him. Suddenly Chávez stepped around Taylor, positioning him so that Taylor was trapped in the corner, with no way to escape from Chávez' desperate final flurry. Chávez then nailed Taylor with a tremendous right hand that dropped the younger man. By using the ring ropes to pull himself up, Taylor managed to return to his feet and was given the mandatory 8-count. Referee Richard Steele asked Taylor twice if he was able to continue fighting, but Taylor failed to answer. Steele then concluded that Taylor was unfit to continue and signaled that he was ending the fight, resulting in a TKO victory for Chávez with only two seconds to go in the bout.
Equipment.
Since boxing involves forceful, repetitive punching, precautions must be taken to prevent damage to bones in the hand. Most trainers do not allow boxers to train and spar without wrist wraps and boxing gloves. Hand wraps are used to secure the bones in the hand, and the gloves are used to protect the hands from blunt injury, allowing boxers to throw punches with more force than if they did not utilize them. Gloves have been required in competition since the late nineteenth century, though modern boxing gloves are much heavier than those worn by early twentieth-century fighters. Prior to a bout, both boxers agree upon the weight of gloves to be used in the bout, with the understanding that lighter gloves allow heavy punchers to inflict more damage. The brand of gloves can also affect the impact of punches, so this too is usually stipulated before a bout.
A mouth guard is important to protect the teeth and gums from injury, and to cushion the jaw, resulting in a decreased chance of knockout. Both fighters must wear soft soled shoes to reduce the damage from accidental (or intentional) stepping on feet. While older boxing boots more commonly resembled those of a professional wrestler, modern boxing shoes and boots tend to be quite similar to their amateur wrestling counterparts.
Boxers practice their skills on two basic types of punching bags. A small, tear-drop-shaped "speed bag" is used to hone reflexes and repetitive punching skills, while a large cylindrical "heavy bag" filled with sand, a synthetic substitute, or water is used to practice power punching and body blows. In addition to these distinctive pieces of equipment, boxers also utilize sport-nonspecific training equipment to build strength, speed, agility, and stamina. Common training equipment includes free weights, rowing machines, jump rope, and medicine balls.
Boxing matches typically take place in a boxing ring, a raised platform surrounded by ropes attached to posts rising in each corner. The term "ring" has come to be used as a metaphor for many aspects of prize fighting in general.
Technique.
Stance.
The modern boxing stance differs substantially from the typical boxing stances of the 19th and early 20th centuries. The modern stance has a more upright vertical-armed guard, as opposed to the more horizontal, knuckles-facing-forward guard adopted by early 20th century hook users such as Jack Johnson.
In a fully upright stance, the boxer stands with the legs shoulder-width apart and the rear foot a half-step in front of the lead man. Right-handed or orthodox boxers lead with the left foot and fist (for most penetration power). Both feet are parallel, and the right heel is off the ground. The lead (left) fist is held vertically about six inches in front of the face at eye level. The rear (right) fist is held beside the chin and the elbow tucked against the ribcage to protect the body. The chin is tucked into the chest to avoid punches to the jaw which commonly cause knock-outs and is often kept slightly offcenter. Wrists are slightly bent to avoid damage when punching and the elbows are kept tucked in to protect the ribcage. Some boxers fight from a crouch, leaning forward and keeping their feet closer together. The stance described is considered the "textbook" stance and fighters are encouraged to change it around once it's been mastered as a base. Case in point, many fast fighters have their hands down and have almost exaggerated footwork, while brawlers or bully fighters tend to slowly stalk their opponents.
Left-handed or southpaw fighters use a mirror image of the orthodox stance, which can create problems for orthodox fighters unaccustomed to receiving jabs, hooks, or crosses from the opposite side. The southpaw stance, conversely, is vulnerable to a straight right hand.
North American fighters tend to favor a more balanced stance, facing the opponent almost squarely, while many European fighters stand with their torso turned more to the side. The positioning of the hands may also vary, as some fighters prefer to have both hands raised in front of the face, risking exposure to body shots.
Modern boxers can sometimes be seen tapping their cheeks or foreheads with their fists in order to remind themselves to keep their hands up (which becomes difficult during long bouts). Boxers are taught to push off with their feet in order to move effectively. Forward motion involves lifting the lead leg and pushing with the rear leg. Rearward motion involves lifting the rear leg and pushing with the lead leg. During lateral motion the leg in the direction of the movement moves first while the opposite leg provides the force needed to move the body.
Punches.
There are four basic punches in boxing: the jab, cross, hook and uppercut. Any punch other than a jab is considered a power punch. If a boxer is right-handed (orthodox), his left hand is the lead hand and his right hand is the rear hand. For a left-handed boxer or southpaw, the hand positions are reversed. For clarity, the following discussion will assume a right-handed boxer.
These different punch types can be thrown in rapid succession to form combinations or "combos". The most common is the jab and cross combination, nicknamed the "one-two combo". This is usually an effective combination, because the jab blocks the opponent's view of the cross, making it easier to land cleanly and forcefully.
A large, swinging circular punch starting from a cocked-back position with the arm at a longer extension than the hook and all of the fighter's weight behind it is sometimes referred to as a "roundhouse", "haymaker", or sucker-punch. Relying on body weight and centripetal force within a wide arc, the roundhouse can be a powerful blow, but it is often a wild and uncontrolled punch that leaves the fighter delivering it off balance and with an open guard.
Wide, looping punches have the further disadvantage of taking more time to deliver, giving the opponent ample warning to react and counter. For this reason, the haymaker or roundhouse is not a conventional punch, and is regarded by trainers as a mark of poor technique or desperation. Sometimes it has been used, because of its immense potential power, to finish off an already staggering opponent who seems unable or unlikely to take advantage of the poor position it leaves the puncher in.
Another unconventional punch is the rarely used bolo punch, in which the opponent swings an arm out several times in a wide arc, usually as a distraction, before delivering with either that or the other arm.
An illegal punch to the back of the head or neck is known as a rabbit punch.
Defense.
There are several basic maneuvers a boxer can use in order to evade or block punches, depicted and discussed below.
Philly Shell or Shoulder roll defense -This is actually a variation of the cross-arm defense. The lead arm (left for an orthodox fighter and right for a southpaw) is placed across the torso usually somewhere in between the belly button and chest and the lead hand rests on the opposite side of the fighter's torso. The back hand is placed on the side of the face (right side for orthodox fighters and left side for southpaws). The lead shoulder is brought in tight against the side of the face (left side for orthodox fighters and right side for southpaws). This style is used by fighters who like to counterpunch.
To execute this guard a fighter must be very athletic and experienced. This style is so effective for counterpunching because it allows fighters to slip punches by rotating and dipping their upper body and causing blows to glance off the fighter. After the punch glances off, the fighter's back hand is in perfect position to hit their out-of-position opponent. The shoulder lean is used in this stance. To execute the shoulder lean a fighter rotates and ducks (to the right for orthodox fighters and to the left for southpaws) when their opponents punch is coming towards them and then rotates back towards their opponent while their opponent is bringing their hand back.
The fighter will throw a punch with their back hand as they are rotating towards their undefended opponent. The weakness to this style is that when a fighter is stationary and not rotating they are open to be hit so a fighter must be athletic and well conditioned to effectively execute this style. To beat this style, fighters like to jab their opponents shoulder causing the shoulder and arm to be in pain and to demobilize that arm. Fighters that used this defense include Sugar Ray Robinson, Ken Norton (also used this defense), Pernell Whitaker, James Toney, and Floyd Mayweather Jr.. Floyd Mayweather Jr., is considered to be the master of this technique.
Less common strategies.
Floyd Mayweather, Jr. employed the use of a check hook against Ricky Hatton, which sent Hatton flying head first into the corner post and being knocked down. Hatton managed to get himself to his feet after the knockdown but was clearly dazed and it was only a matter of moments before Mayweather landed a flurry of punches which sent Hatton crashing to the canvas, giving Mayweather a TKO victory in the 10th round and handing Hatton his first defeat.
Ring corner.
In boxing, each fighter is given a corner of the ring where he rests in between rounds and where his trainers stand. Typically, three men stand in the corner besides the boxer himself; these are the trainer, the assistant trainer and the cutman. The trainer and assistant typically give advice to the boxer on what he is doing wrong as well as encouraging him if he is losing. The cutman is a cutaneous doctor responsible for keeping the boxer's face and eyes free of cuts and blood. This is of particular importance because many fights are stopped because of cuts that threaten the boxer's eyes.
In addition, the corner is responsible for stopping the fight if they feel their fighter is in grave danger of permanent injury. The corner will occasionally throw in a white towel to signify a boxer's surrender (the idiomatic phrase "to throw in the towel", meaning to give up, derives from this practice). This can be seen in the fight between Diego Corrales and Floyd Mayweather. In that fight, Corrales' corner surrendered despite Corrales' steadfast refusal.
Medical concerns.
Knocking a person unconscious or even causing concussion may cause permanent brain damage. There is no clear division between the force required to knock a person out and the force likely to kill a person. Since 1980, more than 200 amateur boxers, professional boxers and Toughman fighters have died due to ring or training injuries. In 1983, the "Journal of the American Medical Association" called for a ban on boxing. The editor, Dr. George Lundberg, called boxing an "obscenity" that "should not be sanctioned by any civilized society." Since then, the British, Canadian and Australian Medical Associations also have called for bans on boxing.
Supporters of the ban state that boxing is the only sport where hurting the other athlete is the goal. Dr. Bill O'Neill, boxing spokesman for the British Medical Association, has supported the BMA's proposed ban on boxing: "It is the only sport where the intention is to inflict serious injury on your opponent, and we feel that we must have a total ban on boxing." Opponents respond that such a position is misguided opinion, stating that amateur boxing is scored solely according to total connecting blows with no award for "injury". They observe that many skilled professional boxers have had rewarding careers without inflicting injury on opponents by accumulating scoring blows and avoiding punches winning rounds scored 10-9 by the 10-point must system, and they note that there are many other sports where concussions are much more prevalent. In 2007, one study of amateur boxers showed that protective headgear did not prevent brain damage, and another found that amateur boxers faced a high risk of brain damage. The Gothenburg study analyzed temporary levels of neurofiliment light in cerebral spinal fluid which they conclude is evidence of damage, even though the levels soon subside. More comprehensive studies of neurologiocal function on larger samples performed by Johns Hopkins University and accident rates analyzed by National Safety Council show amateur boxing is a comparatively safe sport.
In 1997, the American Association of Professional Ringside Physicians was established to create medical protocols through research and education to prevent injuries in boxing.
Professional boxing is forbidden in Norway, Iceland, Iran and North Korea. It was banned in Sweden until 2007 when the ban was lifted but strict restrictions, including four three-minute rounds for fights, were imposed. It was banned in Albania from 1965 till the fall of Communism in 1991; it is now legal.
Boxing Hall of Fame.
The sport of boxing has two internationally recognized boxing halls of fame; the International Boxing Hall of Fame (IBHOF) and the World Boxing Hall of Fame (WBHF), with the IBHOF being the more widely recognized boxing hall of fame.
The WBHF was founded by Everett L. Sanders in 1980. Since its inception the WBHOF has never had a permanent location or museum, which has allowed the more recent IBHOF to garner more publicity and prestige. Among the notable names in the WBHF are Ricardo "Finito" Lopez, Gabriel "Flash" Elorde, Michael Carbajal, Khaosai Galaxy, Henry Armstrong, Jack Johnson, Roberto Durán, George Foreman, Ceferino Garcia and Salvador Sanchez. Boxing's International Hall of Fame was inspired by a tribute an American town held for two local heroes in 1982. The town, Canastota, New York, (which is about east of Syracuse, via the New York State Thruway), honored former world welterweight/middleweight champion Carmen Basilio and his nephew, former world welterweight champion Billy Backus. The people of Canastota raised money for the tribute which inspired the idea of creating an official, annual hall of fame for notable boxers.
The International Boxing Hall of Fame opened in Canastota in 1989. The first inductees in 1990 included Jack Johnson, Benny Leonard, Jack Dempsey, Henry Armstrong, Sugar Ray Robinson, Archie Moore, and Muhammad Ali. Other world-class figures include Salvador Sanchez, Fabio Martella, Roberto "Manos de Piedra" Durán, Ricardo Lopez, Gabriel "Flash" Elorde, Vicente Saldivar, Ismael Laguna, Eusebio Pedroza, Carlos Monzón, Azumah Nelson, Rocky Marciano, Pipino Cuevas and Ken Buchanan. The Hall of Fame's induction ceremony is held every June as part of a four-day event.
The fans who come to Canastota for the Induction Weekend are treated to a number of events, including scheduled autograph sessions, boxing exhibitions, a parade featuring past and present inductees, and the induction ceremony itself.
Boxer rankings.
There are various organizations and websites, that rank boxers in both weight class and pound-for-pound manner.

</doc>
<doc id="4246" url="http://en.wikipedia.org/wiki?curid=4246" title="Bollywood">
Bollywood

Bollywood is the Hindi-language film industry based in Mumbai (Bombay), Maharashtra, India. The term is often incorrectly used to refer to the whole of Indian cinema; however, it is only a part of the large Indian film industry, which includes other production centres producing films in multiple languages. Bollywood is one of the largest film producers in India and one of the largest centres of film production in the world.
Bollywood is also formally referred to as Hindi cinema. There has been a growing presence of Indian English in dialogue and songs as well. It is common to see films that feature dialogue with English words (also known as Hinglish), phrases, or even whole sentences.
Etymology.
The name "Bollywood" is a portmanteau derived from Bombay (the former name for Mumbai) and Hollywood, the center of the American film industry. However, unlike Hollywood, Bollywood does not exist as a physical place. Though some deplore the name, arguing that it makes the industry look like a poor cousin to Hollywood, it has its own entry in the "Oxford English Dictionary".
The naming scheme for "Bollywood" was inspired by "Tollywood", the name that was used to refer to the cinema of West Bengal. Dating back to 1932, "Tollywood" was the earliest Hollywood-inspired name, referring to the Bengali film industry based in Tollygunge, Calcutta, whose name is reminiscent of "Hollywood" and was the center of the cinema of India at the time. It was this "chance juxtaposition of two pairs of rhyming syllables," Holly and Tolly, that led to the portmanteau name "Tollywood" being coined. The name "Tollywood" went on to be used as a nickname for the Bengali film industry by the popular Kolkata-based "Junior Statesman" youth magazine, establishing a precedent for other film industries to use similar-sounding names, eventually leading to the term "Bollywood" being coined. However, more popularly, Tollywood is now used to refer to the Telugu Film Industry in Andhra Pradesh.
The term "Bollywood" itself has origins in the 1970s, when India overtook America as the world's largest film producer. Credit for the term has been claimed by several different people, including the lyricist, filmmaker and scholar Amit Khanna, and the journalist Bevinda Collaco.
History.
"Raja Harishchandra" (1913), by Dadasaheb Phalke, is known as the first silent feature film made in India. By the 1930s, the industry was producing over 200 films per annum. The first Indian sound film, Ardeshir Irani's "Alam Ara" (1931), was a major commercial success. There was clearly a huge market for talkies and musicals; Bollywood and all the regional film industries quickly switched to sound filming.
The 1930s and 1940s were tumultuous times: India was buffeted by the Great Depression, World War II, the Indian independence movement, and the violence of the Partition. Most Bollywood films were unabashedly escapist, but there were also a number of filmmakers who tackled tough social issues, or used the struggle for Indian independence as a backdrop for their plots.
In 1937, Ardeshir Irani, of "Alam Ara" fame, made the first colour film in Hindi, "Kisan Kanya". The next year, he made another colour film, a version of "Mother India". However, colour did not become a popular feature until the late 1950s. At this time, lavish romantic musicals and melodramas were the staple fare at the cinema.
Golden Age.
Following India's independence, the period from the late 1940s to the 1960s is regarded by film historians as the "Golden Age" of Hindi cinema. Some of the most critically acclaimed Hindi films of all time were produced during this period. Examples include the Guru Dutt films "Pyaasa" (1957) and "Kaagaz Ke Phool" (1959) and the Raj Kapoor films "Awaara" (1951) and "Shree 420" (1955). These films expressed social themes mainly dealing with working-class urban life in India; "Awaara" presented the city as both a nightmare and a dream, while "Pyaasa" critiqued the unreality of city life. Some of the most famous epic films of Hindi cinema were also produced at the time, including Mehboob Khan's "Mother India" (1957), which was nominated for the Academy Award for Best Foreign Language Film, and K. Asif's "Mughal-e-Azam" (1960). "Madhumati" (1958), directed by Bimal Roy and written by Ritwik Ghatak, popularised the theme of reincarnation in Western popular culture. Other acclaimed mainstream Hindi filmmakers at the time included Kamal Amrohi and Vijay Bhatt. Successful actors at the time included Dev Anand, Dilip Kumar, Raj Kapoor and Guru Dutt, while successful actresses included Nargis, Vyjayanthimala, Meena Kumari, Nutan, Madhubala, Waheeda Rehman and Mala Sinha.
While commercial Hindi cinema was thriving, the 1950s also saw the emergence of a new Parallel Cinema movement. Though the movement was mainly led by Bengali cinema, it also began gaining prominence in Hindi cinema. Early examples of Hindi films in this movement include Chetan Anand's "Neecha Nagar" (1946) and Bimal Roy's "Do Bigha Zamin" (1953). Their critical acclaim, as well as the latter's commercial success, paved the way for Indian neorealism and the "Indian New Wave". Some of the internationally acclaimed Hindi filmmakers involved in the movement included Mani Kaul, Kumar Shahani, Ketan Mehta, Govind Nihalani, Shyam Benegal and Vijaya Mehta.
Ever since the social realist film "Neecha Nagar" won the Grand Prize at the first Cannes Film Festival, Hindi films were frequently in competition for the Palme d'Or at the Cannes Film Festival throughout the 1950s and early 1960s, with some of them winning major prizes at the festival. Guru Dutt, while overlooked in his own lifetime, had belatedly generated international recognition much later in the 1980s. Dutt is now regarded as one of the greatest Asian filmmakers of all time, alongside the more famous Indian Bengali filmmaker Satyajit Ray. The 2002 "Sight & Sound" critics' and directors' poll of greatest filmmakers ranked Dutt at No. 73 on the list. Some of his films are now included among the greatest films of all time, with "Pyaasa" (1957) being featured in Time magazine's "All-TIME" 100 best movies list, and with both "Pyaasa" and "Kaagaz Ke Phool" (1959) tied at #160 in the 2002 "Sight & Sound" critics' and directors' poll of all-time greatest films. Several other Hindi films from this era were also ranked in the "Sight & Sound" poll, including Raj Kapoor's "Awaara" (1951), Vijay Bhatt's "Baiju Bawra" (1952), Mehboob Khan's "Mother India" (1957) and K. Asif's "Mughal-e-Azam" (1960) all tied at #346 on the list.
Modern cinema.
In the late 1960s and early 1970s, romance movies and action films starred actors like Rajesh Khanna, Dharmendra, Sanjeev Kumar and Shashi Kapoor and actresses like Sharmila Tagore, Mumtaz and Asha Parekh. In the mid-1970s, romantic confections made way for gritty, violent films about gangsters (see Indian mafia) and bandits. Amitabh Bachchan, the star known for his "angry young man" roles, rode the crest of this trend with actors like Mithun Chakraborty, Anil Kapoor and Sunny Deol, which lasted into the early 1990s. Actresses from this era included Hema Malini, Jaya Bachchan and Rekha.
Some Hindi filmmakers such as Shyam Benegal continued to produce realistic Parallel Cinema throughout the 1970s, alongside Mani Kaul, Kumar Shahani, Ketan Mehta, Govind Nihalani and Vijaya Mehta. However, the 'art film' bent of the Film Finance Corporation came under criticism during a Committee on Public Undertakings investigation in 1976, which accused the body of not doing enough to encourage commercial cinema. The 1970s thus saw the rise of commercial cinema in the form of enduring films such as "Sholay" (1975), which solidified Amitabh Bachchan's position as a lead actor. The devotional classic "Jai Santoshi Ma" was also released in 1975. Another important film from 1975 was "Deewar", directed by Yash Chopra and written by Salim-Javed. A crime film pitting "a policeman against his brother, a gang leader based on real-life smuggler Haji Mastan", portrayed by Amitabh Bachchan, it was described as being "absolutely key to Indian cinema" by Danny Boyle. The most internationally acclaimed Hindi film of the 1980s was Mira Nair's "Salaam Bombay!" (1988), which won the Camera d'Or at the 1988 Cannes Film Festival and was nominated for the Academy Award for Best Foreign Language Film.
During the late 1980s and early 1990s, the pendulum swung back toward family-centric romantic musicals with the success of such films as "Qayamat Se Qayamat Tak" (1988), "Maine Pyar Kiya" (1989), "Dil" (1990), "Hum Aapke Hain Kaun" (1994) and "Dilwale Dulhania Le Jayenge" (1995), making stars out of a new generation of actors (such as Aamir Khan, Salman Khan and Shahrukh Khan) and actresses (such as Sridevi, Madhuri Dixit, Juhi Chawla and Kajol). In that point of time, action and comedy films were also successful, with actors like Govinda and actresses such as Raveena Tandon and Karisma Kapoor appearing in popular comedy films, and stunt actor Akshay Kumar gaining popularity for performing dangerous stunts in action films in his well known Khiladi (film series) and other action films. Furthermore, this decade marked the entry of new performers in arthouse and independent films, some of which succeeded commercially, the most influential example being "Satya" (1998), directed by Ram Gopal Varma and written by Anurag Kashyap. The critical and commercial success of "Satya" led to the emergence of a distinct genre known as "Mumbai noir", urban films reflecting social problems in the city of Mumbai. This led to a resurgence of Parallel Cinema by the end of the decade. These films often featured actors like Nana Patekar, Manoj Bajpai, Manisha Koirala, Tabu and Urmila Matondkar, whose performances were usually critically acclaimed.
The 2000s saw a growth in Bollywood's popularity in the world. This led the nation's filmmaking to new heights in terms of quality, cinematography and innovative story lines as well as technical advances in areas such as special effects, animation, and so on. Some of the largest production houses, among them Yash Raj Films and Dharma Productions were the producers of new modern films. Some popular films of the decade were "Koi... Mil Gaya" (2003), "Kal Ho Naa Ho" (2003), "Veer-Zaara" (2004), "Dhoom" (2004), "Hum Tum" (2004), "Dhoom 2" (2006), "Krrish" (2006), and "Jab We Met" (2007). These films starred established actors. However, the mid-2000s also saw the rise of a new generation of popular actors like Hrithik Roshan, Saif Ali Khan, Shahid Kapoor, and Abhishek Bachchan, as well as a new generation of popular actresses like Rani Mukerji, Preity Zinta, Aishwarya Rai, Kareena Kapoor, and Priyanka Chopra.
In the early 2010s, established actors like Salman Khan and Akshay Kumar became known for making big-budget "masala" entertainers like "Dabangg" and "Rowdy Rathore" opposite younger actresses like Sonakshi Sinha. These films were often not critically acclaimed, but were often major commercial successes. While most stars from the 2000s continued their successful careers into the next decade, the 2010s also saw the rise of a new crop of actors like Ranbir Kapoor, Imran Khan, Ranveer Singh, and Arjun Kapoor, as well as actresses like Vidya Balan, Katrina Kaif, Deepika Padukone, Anushka Sharma, and Parineeti Chopra.
The Hindi film industry has preferred films that appeal to all segments of the audience (see the discussion in Ganti, 2004, cited in references), and has resisted making films that target narrow audiences. It was believed that aiming for a broad spectrum would maximise box office receipts. However, filmmakers may be moving towards accepting some box-office segmentation, between films that appeal to rural Indians, and films that appeal to urban and overseas audiences.
Influences for Bollywood.
Gokulsing and Dissanayake identify six major influences that have shaped the conventions of Indian popular cinema:
Influence of Bollywood.
Perhaps the biggest influence of Bollywood has been on nationalism in India itself, where along with rest of Indian cinema, it has become part and parcel of the 'Indian story'. In the words of the economist and Bollywood biographer Lord Meghnad Desai,
Cinema actually has been the most vibrant medium for telling India its own story, the story of its struggle for independence, its constant struggle to achieve national integration and to emerge as a global presence.
In the 2000s, Bollywood began influencing musical films in the Western world, and played a particularly instrumental role in the revival of the American musical film genre. Baz Luhrmann stated that his musical film "Moulin Rouge!" (2001) was directly inspired by Bollywood musicals. The film incorporated an Indian-themed play based on the ancient Sanskrit drama "Mṛcchakatika" and a Bollywood-style dance sequence with a song from the film "China Gate". The critical and financial success of "Moulin Rouge!" renewed interest in the then-moribund Western musical genre, and subsequently films such as "Chicago, The Producers, Rent", "Dreamgirls", "Hairspray", "", "Across the Universe", "The Phantom of the Opera", "Enchanted" and "Mamma Mia!" were produced, fuelling a renaissance of the genre.
A. R. Rahman, an Indian film composer, wrote the music for Andrew Lloyd Webber's "Bombay Dreams", and a musical version of "Hum Aapke Hain Koun" has played in London's West End. The Bollywood musical "Lagaan" (2001) was nominated for the Academy Award for Best Foreign Language Film, and two other Bollywood films "Devdas" (2002) and "Rang De Basanti" (2006) were nominated for the BAFTA Award for Best Foreign Language Film. Danny Boyle's "Slumdog Millionaire" (2008), which has won four Golden Globes and eight Academy Awards, was also directly inspired by Bollywood films, and is considered to be a "homage to Hindi commercial cinema". The theme of reincarnation was also popularised in Western popular culture through Bollywood films, with "Madhumati" (1958) inspiring the Hollywood film "The Reincarnation of Peter Proud" (1975), which in turn inspired the Bollywood film "Karz" (1980), which in turn influenced another Hollywood film "Chances Are" (1989). The 1975 film "Chhoti Si Baat" is believed to have inspired "Hitch" (2005), which in turn inspired the Bollywood film "Partner" (2007).
The influence of Bollywood "filmi" music can also be seen in popular music elsewhere in the world. In 1978, technopop pioneers Haruomi Hosono and Ryuichi Sakamoto of the Yellow Magic Orchestra produced an electronic album "Cochin Moon" based on an experimental fusion between electronic music and Bollywood-inspired Indian music. Devo's 1988 hit song "Disco Dancer" was inspired by the song "I am a Disco Dancer" from the Bollywood film "Disco Dancer" (1982). The 2002 song "Addictive", sung by Truth Hurts and produced by DJ Quik and Dr. Dre, was lifted from Lata Mangeshkar's "Thoda Resham Lagta Hai" from "Jyoti" (1981). The Black Eyed Peas' Grammy Award winning 2005 song "Don't Phunk with My Heart" was inspired by two 1970s Bollywood songs: "Ye Mera Dil Yaar Ka Diwana" from "Don" (1978) and "Ae Nujawan Hai Sub" from "Apradh" (1972). Both songs were originally composed by Kalyanji Anandji, sung by Asha Bhosle, and featured the dancer Helen. Also in 2005, the Kronos Quartet re-recorded several R. D. Burman compositions, with Asha Bhosle as the singer, into an album "You've Stolen My Heart: Songs from R.D. Burman's Bollywood", which was nominated for "Best Contemporary World Music Album" at the 2006 Grammy Awards. "Filmi" music composed by A. R. Rahman (who would later win two Academy Awards for the "Slumdog Millionaire" soundtrack) has frequently been sampled by musicians elsewhere in the world, including the Singaporean artist Kelly Poon, the Uzbek artist Iroda Dilroz, the French rap group La Caution, the American artist Ciara, and the German band Löwenherz, among others. Many Asian Underground artists, particularly those among the overseas Indian diaspora, have also been inspired by Bollywood music.
Genre conventions.
Bollywood films are mostly musicals and are expected to contain catchy music in the form of song-and-dance numbers woven into the script. A film's success often depends on the quality of such musical numbers. Indeed, a film's music is often released before the movie and helps increase the audience.
Indian audiences expect full value for their money, with a good entertainer generally referred to as "paisa" "vasool", (literally, "money's worth"). Songs and dances, love triangles, comedy and dare-devil thrills are all mixed up in a three-hour extravaganza with an intermission. They are called "masala" films, after the Hindi word for a spice mixture. Like "masalas", these movies are a mixture of many things such as action, comedy, romance and so on. Most films have heroes who are able to fight off villains all by themselves.
Bollywood plots have tended to be melodramatic. They frequently employ formulaic ingredients such as star-crossed lovers and angry parents, love triangles, family ties, sacrifice, corrupt politicians, kidnappers, conniving villains, courtesans with hearts of gold, long-lost relatives and siblings separated by fate, dramatic reversals of fortune, and convenient coincidences.
There have always been Indian films with more artistic aims and more sophisticated stories, both inside and outside the Bollywood tradition (see Parallel Cinema). They often lost out at the box office to movies with more mass appeal. Bollywood conventions are changing, however. A large Indian diaspora in English-speaking countries, and increased Western influence at home, have nudged Bollywood films closer to Hollywood models.
Film critic Lata Khubchandani writes, "our earliest films ... had liberal doses of sex and kissing scenes in them. Strangely, it was after Independence the censor board came into being and so did all the strictures." Plots now tend to feature Westernised urbanites dating and dancing in clubs rather than centring on pre-arranged marriages. Though these changes can widely be seen in contemporary Bollywood, traditional conservative ways of Indian culture continue to exist in India outside the industry and an element of resistance by some to western-based influences. Despite this, Bollywood continues to play a major role in fashion in India. Some studies into fashion in India have revealed that some people are unaware that the changing nature of fashion in Bollywood films are often influenced by globalisation; many consider the clothes worn by Bollywood actors as authentically Indian.
Cast and crew.
Bollywood employs people from all parts of India. It attracts thousands of aspiring actors and actresses, all hoping for a break in the industry. Models and beauty contestants, television actors, theatre actors and even common people come to Mumbai with the hope and dream of becoming a star. Just as in Hollywood, very few succeed. Since many Bollywood films are shot abroad, many foreign extras are employed too.
Very few non-Indian actors are able to make a mark in Bollywood, though many have tried from time to time. There have been some exceptions, of which one recent example is the hit film "Rang De Basanti", where the lead actress is Alice Patten, an Englishwoman. "Kisna", "Lagaan", and "" also featured foreign actors. Of late, Emma Brown Garett, an Australian born actress, has starred in a few Indian films.
Bollywood can be very clannish, and the relatives of film-industry insiders have an edge in getting coveted roles in films or being part of a film's crew. However, industry connections are no guarantee of a long career: competition is fierce and if film industry scions do not succeed at the box office, their careers will falter. Some of the biggest stars, such as Rajesh Khanna, Dharmendra, Amitabh Bachchan, Shahrukh Khan and Akshay Kumar have succeeded despite a lack of any show business connections. For film clans, see List of Hindi film clans.
Sound.
Sound in Bollywood films was once rarely recorded on location (otherwise known as sync sound). Therefore, the sound was usually created (or re-created) entirely in the studio, with the actors reciting their lines as their images appear on-screen in the studio in the process known as "looping in the sound" or ADR—with the foley and sound effects added later. This created several problems, since the sound in these films usually occurs a frame or two earlier or later than the mouth movements or gestures. The actors had to act twice: once on-location, once in the studio—and the emotional level on set is often very difficult to re-create. Commercial Indian films, not just the Hindi-language variety, are known for their lack of ambient sound, so there is a silence underlying everything instead of the background sound and noises usually employed in films to create aurally perceivable depth and environment.
The ubiquity of ADR in Bollywood cinema became prevalent in the early 1960s with the arrival of the Arriflex 3 camera, which required a blimp (cover) in order to shield the sound of the camera, for which it was notorious, from on-location filming. Commercial Indian filmmakers, known for their speed, never bothered to blimp the camera, and its excessive noise required that everything had to be re-created in the studio. Eventually, this became the standard for Indian films.
The trend was bucked in 2001, after a 30-year hiatus of synchronised sound, with the film "Lagaan", in which producer-star Aamir Khan insisted that the sound be done on location. This opened up a heated debate on the use and economic feasibility of on-location sound, and several Bollywood films have employed on-location sound since then.
Bollywood song and dance.
Bollywood film music is called filmi music (from Hindi, meaning "of films"). Songs from Bollywood movies are generally pre-recorded by professional playback singers, with the actors then lip synching the words to the song on-screen, often while dancing. While most actors, especially today, are excellent dancers, few are also singers. One notable exception was Kishore Kumar, who starred in several major films in the 1950s while also having a stellar career as a playback singer. K. L. Saigal, Suraiyya, and Noor Jehan were also known as both singers and actors. Some actors in the last thirty years have sung one or more songs themselves; for a list, see Singing actors and actresses in Indian cinema.
Playback singers are prominently featured in the opening credits and have their own fans who will go to an otherwise lackluster movie just to hear their favourites. Going by the quality as well as the quantity of the songs they rendered, most notable singers of Bollywood are Lata Mangeshkar, Asha Bhosle, Geeta Dutt, Shamshad Begum, Kavita Krishnamurthy, Sadhana Sargam and Alka Yagnik among female playback singers; and K. L. Saigal, Talat Mahmood, Mukesh, Mohammed Rafi, Manna Dey, Hemant Kumar, Kishore Kumar, Kumar Sanu, Udit Narayan and Sonu Nigam among male playback singers. Kishore Kumar and Mohammed Rafi are often considered arguably the finest of the singers that have lent their voice to Bollywood songs, followed by Lata Mangeshkar, who, through the course of a career spanning over six decades, has recorded thousands of songs for Indian movies. The composers of film music, known as music directors, are also well-known. Their songs can make or break a film and usually do. Remixing of film songs with modern beats and rhythms is a common occurrence today, and producers may even release remixed versions of some of their films' songs along with the films' regular soundtrack albums.
The dancing in Bollywood films, especially older ones, is primarily modelled on Indian dance: classical dance styles, dances of historic northern Indian courtesans (tawaif), or folk dances. In modern films, Indian dance elements often blend with Western dance styles (as seen on MTV or in Broadway musicals), though it is usual to see Western pop "and" pure classical dance numbers side by side in the same film. The hero or heroine will often perform with a troupe of supporting dancers. Many song-and-dance routines in Indian films feature unrealistically instantaneous shifts of location or changes of costume between verses of a song. If the hero and heroine dance and sing a duet, it is often staged in beautiful natural surroundings or architecturally grand settings. This staging is referred to as a "picturisation".
Songs typically comment on the action taking place in the movie, in several ways. Sometimes, a song is worked into the plot, so that a character has a reason to sing. Other times, a song is an externalisation of a character's thoughts, or presages an event that has not occurred yet in the plot of the movie. In this case, the event is often two characters falling in love. The songs are also often referred to as a "dream sequence", and anything can happen that would not normally happen in the real world.
Previously song and dance scenes often used to be shot in Kashmir, but due to political unrest in Kashmir since the end of the 1980s, those scenes have since then often been shot in Western Europe, particularly in Switzerland and Austria.
Bollywood films have always used what are now called "item numbers". A physically attractive female character (the "item girl"), often completely unrelated to the main cast and plot of the film, performs a catchy song and dance number in the film. In older films, the "item number" may be performed by a courtesan ("tawaif") dancing for a rich client or as part of a cabaret show. The actress Helen was famous for her cabaret numbers. In modern films, item numbers may be inserted as discotheque sequences, dancing at celebrations, or as stage shows.
For the last few decades Bollywood producers have been releasing the film's soundtrack, as tapes or CDs, before the main movie release, hoping that the music will pull audiences into the cinema later. Often the soundtrack is more popular than the movie. In the last few years some producers have also been releasing music videos, usually featuring a song from the film. However, some promotional videos feature a song which is not included in the movie.
Dialogues and lyrics.
The film script or lines of dialogue (called "dialogues" in Indian English) and the song lyrics are often written by different people.
Dialogues are usually written in an unadorned Hindi that would be understood by the largest possible audience. Some movies, however, have used regional dialects to evoke a village setting, or old-fashioned, courtly, Persian-influenced Urdu in Mughal era historical films. Jyotika Virdi, in her book "The cinematic imagiNation" , wrote about the presence of Urdu in Hindi films: "Urdu is often used in film titles, screenplay, lyrics, the language of love, war, and martyrdom." However, she further discussed its decline over the years: "The extent of Urdu used in commercial Hindi cinema has not been stable ... the decline of Urdu is mirrored in Hindi films ... It is true that many Urdu words have survived and have become part of Hindi cinema's popular vocabulary. But that is as far as it goes." Contemporary mainstream movies also make great use of English. According to "Bollywood Audiences Editorial", "English has begun to challenge the ideological work done by Urdu." Some movie scripts are first written in Latin script. Characters may shift from one language to the other to express a certain atmosphere (for example, English in a business setting and Hindi in an informal one).
Cinematic language, whether in dialogues or lyrics, is often melodramatic and invokes God, family, mother, duty, and self-sacrifice liberally. Song lyrics are often about love. Bollywood song lyrics, especially in the old movies, frequently use the poetic vocabulary of court Urdu, with many Persian loanwords. Another source for love lyrics is the long Hindu tradition of poetry about the amours of Krishna, Radha, and the gopis, as referenced in films such as "Jhanak Jhanak Payal Baje" and "Lagaan".
Music directors often prefer working with certain lyricists, to the point that the lyricist and composer are seen as a team. This phenomenon is compared to the pairings of American composers and songwriters that created old-time Broadway musicals.
Finances.
Bollywood films are multi-million dollar productions, with the most expensive productions costing up to 1 billion rupees (roughly USD 20 million). The latest Science fiction movie "Ra.One" was made at an immense budget of 135 crores (roughly USD 27 million), making it the most expensive movie ever produced in Bollywood. Sets, costumes, special effects, and cinematography were less than world-class up until the mid-to-late 1990s, although with some notable exceptions. As Western films and television gain wider distribution in India itself, there is an increasing pressure for Bollywood films to attain the same production levels, particularly in areas such as action and special effects. Recent Bollywood films have employed international technicians to improve in these areas, such as "Krrish" (2006) which has action choreographed by Hong Kong based Tony Ching. The increasing accessibility to professional action and special effects, coupled with rising film budgets, has seen an explosion in the action and sci-fi genres.
Sequences shot overseas have proved a real box office draw, so Mumbai film crews are increasingly filming in Australia, Canada, New Zealand, the United Kingdom, the United States, continental Europe and elsewhere. Nowadays, Indian producers are winning more and more funding for big-budget films shot within India as well, such as "Lagaan", "Devdas" and other recent films.
Funding for Bollywood films often comes from private distributors and a few large studios. Indian banks and financial institutions were forbidden from lending money to movie studios. However, this ban has now been lifted. As finances are not regulated, some funding also comes from illegitimate sources, such as the Mumbai underworld. The Mumbai underworld has been known to be involved in the production of several films, and are notorious for patronising several prominent film personalities. On occasion, they have been known to use money and muscle power to get their way in cinematic deals. In January 2000, Mumbai mafia hitmen shot Rakesh Roshan, a film director and father of star Hrithik Roshan. In 2001, the Central Bureau of Investigation seized all prints of the movie "Chori Chori Chupke Chupke" after the movie was found to be funded by members of the Mumbai underworld.
Another problem facing Bollywood is widespread copyright infringement of its films. Often, bootleg DVD copies of movies are available before the prints are officially released in cinemas. Manufacturing of bootleg DVD, VCD, and VHS copies of the latest movie titles is a well established 'small scale industry' in parts of South Asia and South East Asia. The Federation of Indian Chambers of Commerce and Industry (FICCI) estimates that the Bollywood industry loses $100 million annually in loss of revenue from pirated home videos and DVDs. Besides catering to the homegrown market, demand for these copies is large amongst some sections of the Indian diaspora, too. (In fact, bootleg copies are the only way people in Pakistan can watch Bollywood movies, since the Government of Pakistan has banned their sale, distribution and telecast). Films are frequently broadcast without compensation by countless small cable TV companies in India and other parts of South Asia. Small convenience stores run by members of the Indian diaspora in the US and the UK regularly stock tapes and DVDs of dubious provenance, while consumer copying adds to the problem. The availability of illegal copies of movies on the Internet also contributes to the piracy problem.
Satellite TV, television and imported foreign films are making huge inroads into the domestic Indian entertainment market. In the past, most Bollywood films could make money; now fewer tend to do so. However, most Bollywood producers make money, recouping their investments from many sources of revenue, including selling ancillary rights. There are also increasing returns from theatres in Western countries like the United Kingdom, Canada, and the United States, where Bollywood is slowly getting noticed. As more Indians migrate to these countries, they form a growing market for upscale Indian films.
For a comparison of Hollywood and Bollywood financial figures, see chart. It shows tickets sold in 2002 and total revenue estimates. Bollywood sold 3.6 billion tickets and had total revenues (theatre tickets, DVDs, television and so on.) of US$1.3 billion, whereas Hollywood films sold 2.6 billion tickets and generated total revenues (again from all formats) of US$51 billion.
Advertising.
Many Indian artists used to make a living by hand-painting movie billboards and posters (The well-known artist M.F. Hussain used to paint film posters early in his career). This was because human labour was found to be cheaper than printing and distributing publicity material. Now, a majority of the huge and ubiquitous billboards in India's major cities are created with computer-printed vinyl. The old hand-painted posters, once regarded as ephemera, are becoming increasingly collectible as folk art.
Releasing the film music, or music videos, before the actual release of the film can also be considered a form of advertising. A popular tune is believed to help pull audiences into the theatres.
Bollywood publicists have begun to use the Internet as a venue for advertising. Most of the better-funded film releases now have their own websites, where browsers can view trailers, stills, and information about the story, cast, and crew.
Bollywood is also used to advertise other products. Product placement, as used in Hollywood, is widely practised in Bollywood.
Bollywood movie stars appear in print and television advertisements for other products, such as watches or soap (see Celebrity endorsement). Advertisers say that a star endorsement boosts sales.
Awards.
The Filmfare Awards ceremony is one of the most prominent film events given for Hindi films in India. The Indian screen magazine "Filmfare" started the first Filmfare Awards in 1954, and awards were given to the best films of 1953. The ceremony was referred to as the "Clare Awards" after the magazine's editor. Modelled after the poll-based merit format of the Academy of Motion Picture Arts and Sciences, individuals may submit their votes in separate categories. A dual voting system was developed in 1956. Like the Oscars, the Filmfare awards are frequently accused of bias towards commercial success rather than artistic merit.
As the Filmfare, the National Film Awards were introduced in 1954. Since 1973, the Indian government has sponsored the National Film Awards, awarded by the government run Directorate of Film Festivals (DFF). The DFF screens not only Bollywood films, but films from all the other regional movie industries and independent/art films. These awards are handed out at an annual ceremony presided over by the President of India. Under this system, in contrast to the National Film Awards, which are decided by a panel appointed by Indian Government, the Filmfare Awards are voted for by both the public and a committee of experts.
Additional ceremonies held within India are:
Ceremonies held overseas are:
Most of these award ceremonies are lavishly staged spectacles, featuring singing, dancing, and numerous celebrities.
Popularity and appeal.
Besides being popular among the India diaspora, such far off locations as Nigeria to Egypt to Senegal and to Russia generations of non-Indian fans have grown up with Bollywood during the years, bearing witness to the cross-cultural appeal of Indian movies. Over the last years of the twentieth century and beyond, Bollywood progressed in its popularity as it entered the consciousness of Western audiences and producers, with Western actors now actively seeking roles in Bollywood movies.
Africa.
Historically, Hindi films have been distributed to some parts of Africa, largely by Lebanese businessmen. "Mother India" (1957), for example, continued to be played in Nigeria decades after its release. Indian movies have also gained ground so as to alter the style of Hausa fashions, songs have also been copied by Hausa singers and stories have influenced the writings of Nigerian novelists. Stickers of Indian films and stars decorate taxis and buses in Northern Nigeria, while posters of Indian films adorn the walls of tailor shops and mechanics' garages in the country. Unlike in Europe and North America where Indian films largely cater to the expatriate Indian market yearning to keep in touch with their homeland, in West Africa, as in many other parts of the world, such movies rose in popularity despite the lack of a significant Indian audience, where movies are about an alien culture, based on a religion wholly different, and, for the most part, a language that is unintelligible to the viewers. One such explanation for this lies in the similarities between the two cultures. Other similarities include wearing turbans; the presence of animals in markets; porters carrying large bundles, chewing sugar cane; youths riding Bajaj motor scooters; wedding celebrations, and so forth. With the strict Muslim culture, Indian movies were said to show "respect" toward women, where Hollywood movies were seen to have "no shame". In Indian movies women were modestly dressed, men and women rarely kiss, and there is no nudity, thus Indian movies are said to "have culture" that Hollywood films lack. The latter choice was a failure because "they don't base themselves on the problems of the people," where the former is based socialist values and on the reality of developing countries emerging from years of colonialism. Indian movies also allowed for a new youth culture to follow without such ideological baggage as "becoming western."
Several Bollywood personalities have avenued to the continent for both shooting movies and off-camera projects. The film "Padmashree Laloo Prasad Yadav" (2005) was one of many movies shot in South Africa. "Dil Jo Bhi Kahey" (2005) was shot almost entirely in Mauritius, which has a large ethnically Indian population.
Ominously, however, the popularity of old Bollywood versus a new, changing Bollywood seems to be diminishing the popularity on the continent. The changing style of Bollywood has begun to question such an acceptance. The new era features more sexually explicit and violent films. Nigerian viewers, for example, commented that older films of the 1950s and 1960s had culture to the newer, more westernised picturizations. The old days of India avidly "advocating decolonization ... and India's policy was wholly influenced by his missionary zeal to end racial domination and discrimination in the African territories" were replaced by newer realities. The emergence of Nollywood, Africa's local movie industry has also contributed to the declining popularity of Bollywood films. A greater globalised world worked in tandem with the sexualisation of Indian films so as to become more like American films, thus negating the preferred values of an old Bollywood and diminishing Indian soft power.
Additionally, classic Bollywood actors like Kishore Kumar and Amitabh Bachchan have historically enjoyed popularity in Egypt and Somalia. In Ethiopia, Bollwood movies are shown alongside Hollywood productions in Piazza theatres, such as the Cinema Ethiopia in Addis Ababa. In the Maghreb, Bollywood films are also broadcast, though local aesthetics tend much more toward expressive or auteur cinema than commercial fare.
Asia.
Bollywood films are widely watched in South Asian countries, including Bangladesh, Nepal, Pakistan and Sri Lanka.
Many Pakistanis watch Bollywood films, as they understand Hindi (due to its linguistic similarity to Urdu). Pakistan banned the legal import of Bollywood movies in 1965. However, trade in pirated DVDs and illegal cable broadcasts ensured the continued popularity of Bollywood releases in Pakistan. Exceptions were made for a few films, such as the 2006 colorised re-release of the classic "Mughal-e-Azam" or the 2006 film "Taj Mahal". Early in 2008, the Pakistani government eased the ban and allowed the import of even more movies; 16 were screened in 2008. Continued easing followed in 2009 and 2010. The new policy is opposed by nationalists and representatives of Pakistan's small film industry but is embraced by cinema owners, who are making profits after years of low receipts.
Bollywood movies are popular in Afghanistan due to the country's proximity to the Indian subcontinent and cultural perspectives present in the movies. A number of Bollywood movies were filmed inside Afghanistan while some dealt with the country, including "Dharmatma", "Kabul Express", "Khuda Gawah" and "Escape From Taliban". Hindi films have been popular in Arab countries, including Palestine, Jordan, Egypt and the Gulf countries.
Imported Indian films are usually subtitled in Arabic upon the film's release. Since the early 2000s, Bollywood has progressed in Israel. Special channels dedicated to Indian films have been displayed on cable television. Bollywood films are popular in Southeast Asia (particularly in Maritime Southeast Asia) and Central Asia (particularly in Uzbekistan and Tajikistan).
Bollywood films are widely appreciated in East Asian countries such as China, Japan, South Korea and etc. Some Hindi movies had success in the China and South Korea, Japan in the 1940s and 1950s and are popular till today. The most popular Hindi films in that country were "Dr. Kotnis Ki Amar Kahani" (1946), "Awaara" (1951) and "Do Bigha Zamin" (1953). Raj Kapoor was a famous movie star in China, and the song "Awara Hoon" ("I am a Tramp") was popular in the country. Since then, Hindi films significantly declined in popularity in China, until the Academy Award nominated "Lagaan" (2001) became the first Indian film to have a nation-wide release there in decades. The Chinese filmmaker He Ping was impressed by "Lagaan", especially its soundtrack, and thus hired the film's music composer A. R. Rahman to score the soundtrack for his film "Warriors of Heaven and Earth" (2003). Several older Hindi films have a cult following in Japan, particularly the films directed by Guru Dutt.
Indian films are the most popular foreign films in Tajikistan, and Hindi-Urdu departments are very large in the country.
Europe.
The awareness of Hindi cinema is substantial in the United Kingdom, where they frequently enter the UK top ten. Many films, such as "Kabhi Khushi Kabhie Gham" (2001) have been set in London. Bollywood is also appreciated in France, Germany, the Netherlands, and the Scandinavian countries. Various Bollywood movies are dubbed in German and shown on the German television channel RTL II on a regular basis.
Bollywood films are particularly popular in the former Soviet Union. Bollywood films have been dubbed into Russian, and shown in prominent theatres such as Mosfilm and Lenfilm.
Ashok Sharma, Indian Ambassador to Suriname, who has served three times in the Commonwealth of Independent States region during his diplomatic career said:
The film "Mera Naam Joker" (1970), sought to cater to such an appeal and the popularity of Raj Kapoor in Russia, when it recruited Russian actress Kseniya Ryabinkina for the movie. In the contemporary era, "" (2005) was shot entirely in Russia. After the collapse of the Soviet film distribution system, Hollywood occupied the void created in the Russian film market. This made things difficult for Bollywood as it was losing market share to Hollywood. However, Russian newspapers report that there is a renewed interest in Bollywood among young Russians.
North America.
Bollywood has experienced a marked growth in revenue in Canada and the United States, particularly popular amongst the South Asian communities in large cities, such as Toronto, Chicago, and New York City. Yash Raj Films, one of India's largest production houses and distributors, reported in September 2005 that Bollywood films in the United States earn around $100 million a year through theatre screenings, video sales and the sale of movie soundtracks. In other words, films from India do more business in the United States than films from any other non-English speaking country. Numerous films in the mid-1990s and onwards have been largely, or entirely, shot in New York, Los Angeles, Vancouver and Toronto. Bollywood's immersion in the traditional Hollywood domain was further tied with such films as "The Guru" (2002) and "" (2007) trying to popularise the Bollywood-theme for Hollywood.
Oceania.
Bollywood is not as successful in the Oceanic countries and Pacific Islands such as New Guinea. However, it ranks second to Hollywood in countries such as Fiji, with its large Indian minority, Australia and New Zealand.
Australia is one of the countries where there is a large South Asian Diaspora. Bollywood is popular amongst non-Asians in the country as well. Since 1997 the country has provided a backdrop for an increasing number of Bollywood films. Indian filmmakers have been attracted to Australia's diverse locations and landscapes, and initially used it as the setting for song-and-dance sequences, which demonstrated the contrast between the values. However, nowadays, Australian locations are becoming more important to the plot of Bollywood films. Hindi films shot in Australia usually incorporate aspects of Australian lifestyle. The Yash Raj Film "Salaam Namaste" (2005) became the first Indian film to be shot entirely in Australia and was the most successful Bollywood film of 2005 in the country. This was followed by "Heyy Babyy" (2007) "Chak De! India" (2007) and "Singh Is Kinng" (2008) which turned out to be box office successes. Following the release of "Salaam Namaste", on a visit to India the then prime minister John Howard also sought, having seen the film, to have more Indian movies shooting in the country to boost tourism, where the Bollywood and cricket nexus, was further tightened with Steve Waugh's appointment as tourism ambassador to India. Australian actress Tania Zaetta, who co-starred in "Salaam Namaste", among other Bollywood films, expressed her keenness to expand her career in Bollywood.
South America.
Bollywood movies are not influential in many countries of South America, though Bollywood culture and dance is recognised. However, due to significant South Asian diasporic communities in Suriname and Guyana, Hindi language movies are popular. In 2006, "Dhoom 2" became the first Bollywood film to be shot in Rio de Janeiro, Brazil.
In January 2012, it was announced that UTV Motion Pictures would be releasing movies in Peru, starting with "Guzaarish".
Plagiarism.
Constrained by rushed production schedules and small budgets, some Bollywood writers and musicians have been known to resort to plagiarism. Ideas, plot lines, tunes or riffs have been copied from other Indian film industries or foreign films (including Hollywood and other Asian films) without acknowledgement of the original source. This has led to criticism towards the film industry.
Before the 1990s, this could be done with impunity. Copyright enforcement was lax in India and few actors or directors ever saw an official contract. The Hindi film industry was not widely known to non-Indian audiences (excluding the Soviet states), who would not even be aware that their material was being copied. Audiences may also not have been aware of the plagiarism since many audiences in India were unfamiliar with foreign films and music. While copyright enforcement in India is still somewhat lenient, Bollywood and other film industries are much more aware of each other now and Indian audiences are more familiar with foreign movies and music. Organizations like the India EU Film Initiative seek to foster a community between film makers and industry professional between India and the EU.
One of the common justifications of plagiarism in Bollywood in the media is that producers often play a safer option by remaking popular Hollywood films in an Indian context. Screenwriters generally produce original scripts, but due to financial uncertainty and insecurity over the success of a film many were rejected. Screenwriters themselves have been criticised for lack of creativity which happened due to tight schedules and restricted funds in the industry to employ better screenwriters. Certain filmmakers see plagiarism in Bollywood as an integral part of globalisation where American and western cultures are firmly embedding themselves into Indian culture, which is manifested, amongst other mediums, in Bollywood films. Vikram Bhatt, director of films such as "Raaz", a remake of "What Lies Beneath", and "Kasoor", a remake of "Jagged Edge", has spoken about the strong influence of American culture and desire to produce box office hits based along the same lines in Bollywood. He said, "Financially, I would be more secure knowing that a particular piece of work has already done well at the box office. Copying is endemic everywhere in India. Our TV shows are adaptations of American programmes. We want their films, their cars, their planes, their Diet Cokes and also their attitude. The American way of life is creeping into our culture." Mahesh Bhatt has said, "If you hide the source, you're a genius. There's no such thing as originality in the creative sphere".
There have been very few cases of film copyright violations taken to court because of serious delays in the legal process, and due to the long time they take to decide a case. There have been some notable cases of conflict though. The makers of "Partner" (2007) and "Zinda" (2005) have been targeted by the owners and distributors of the original films, "Hitch" and "Oldboy". American Studio Twentieth Century Fox brought the Mumbai-based B.R. Films to court over its forthcoming "Banda Yeh Bindaas Hai", allegedly an illegal remake of its 1992 film "My Cousin Vinny". B.R. Films eventually settled out of court by paying the studio at a cost of about $200,000, paving the way for the film's release. Some on the other hand do comply with copyright law, with Orion Pictures in 2008 securing the rights to remake the Hollywood film "Wedding Crashers".

</doc>
<doc id="4248" url="http://en.wikipedia.org/wiki?curid=4248" title="Bowls">
Bowls

Bowls or lawn bowls is a sport in which the objective is to roll biased balls so that they stop close to a smaller ball called a "jack" or "kitty". It is played on a pitch which may be flat (for "flat-green bowls") or convex or uneven (for "crown-green bowls"). It is normally played outdoors (although there are many indoor venues) and the outdoor surface is either natural grass, artificial turf, or cotula (in New Zealand).
History.
It has been traced certainly to the 13th century, and conjecturally to the 12th. William Fitzstephen (d. about 1190), in his biography of Thomas Becket, gives a graphic sketch of the London of his day and, writing of the summer amusements of the young men, says that on holidays they were "exercised in Leaping, Shooting, Wrestling, Casting of Stones jactu lapidum, and Throwing of Javelins fitted with Loops for the Purpose, which they strive to fling before the Mark; they also use Bucklers, like fighting Men." It is commonly supposed that by jactus lapidum, Fitzstephen meant the game of bowls, but though it is possible that round stones may sometimes have been employed in an early variety of the game - and there is a record of iron bowls being used, though at a much later date, on festive occasions at Nairn, - nevertheless the inference seems unwarranted. The jactus lapidum of which he speaks may have been more akin to shotput. It is beyond dispute, however, that the game, at any rate in a rudimentary form, was played in the 13th century. A manuscript of that period in the royal library, Windsor (No. 20, E iv.), contains a drawing representing two players aiming at a small cone instead of an earthenware ball or jack. The world's oldest surviving bowling green is the Southampton Old Bowling Green, which was first used in 1299.
Another manuscript of the same century has a crude but spirited picture which brings us into close touch with the existing game. Three figures are introduced and a jack. The first player's bowl has come to rest just in front of the jack; the second has delivered his bowl and is following after it with one of those eccentric contortions still not unusual on modern greens, the first player meanwhile making a repressive gesture with his hand, as if to urge the bowl to stop short of his own; the third player is depicted as in the act of delivering his bowl. A 14th-century manuscript, Book of Prayers, in the Francis Douce collection in the Bodleian Library at Oxford contains a drawing in which two persons are shown, but they bowl to no mark. Strutt (Sports and Pastimes) suggests that the first player's bowl may have been regarded by the second player as a species of jack; but in that case it is not clear what was the first player's target. In these three earliest illustrations of the pastime it is worth noting that each player has one bowl only, and that the attitude in delivering it was as various five or six hundred years ago as it is to-day. In the third he stands almost upright; in the first he kneels; in the second he stoops, halfway between the upright and the kneeling position.
The game eventually came under the ban of king and parliament, both fearing it might jeopardise the practice of archery, then so important in battle. Statutes forbidding it and other sports were enacted in the reigns of Edward III, Richard II and other monarchs. Even when, on the invention of gunpowder and firearms, the bow had fallen into disuse as a weapon of war, the prohibition was continued. The discredit attaching to bowling alleys, first established in London in 1455, probably encouraged subsequent repressive legislation, for many of the alleys were connected with taverns frequented by the dissolute and gamesters. The word "bowls" occurs for the first time in the statute of 1511 in which Henry VIII confirmed previous enactments against unlawful games. By a further act of 1541—which was not repealed until 1845—artificers, labourers, apprentices, servants and the like were forbidden to play bowls at any time except Christmas, and then only in their master's house and presence. It was further enjoined that any one playing bowls outside his own garden or orchard was liable to a penalty of 6s. 8d.(6 shillings and 8 pence), while those possessed of lands of the yearly value of £100 might obtain licences to play on their own private greens.
In 1864 William Wallace Mitchell (1803–1884), a Glasgow Cotton Merchant, published his "Manual of Bowls Playing" following his work as the secretary formed in 1849 by Scottish bowling clubs which became the basis of the rules of the modern game.
Young Mitchell was only 11 when he played on Kilmarnock Bowling green, the oldest club in Scotland, instituted in 1740.
The patenting of the first lawn mower in 1830, in Britain, is strongly believed to have been the catalyst, world-wide, for the preparation of modern-style greens, sporting ovals, playing fields, pitches, grass courts, etc. This is turn led to the codification of modern rules for many sports, including lawn bowls, most football codes, lawn tennis and others.
National Bowling Associations were established in the late 1800s. In the then Victorian Colony (now State of Victoria in Australia), the (Royal) Victorian Bowling Association was formed in 1880 and The Scottish Bowling Association was established in 1892, although there had been a failed attempt in 1848 by 200 Scottish clubs.
Today the sport is played in over 40 countries with more than 50 member national authorities.
The home of the modern game is still Scotland with the World Bowls centre in Edinburgh at Caledonia House,1 Redheughs Rigg, South Gyle, Edinburgh, EH12 9DQ.
Game.
Lawn bowls is usually played on a large, rectangular, precisely levelled and manicured grass or synthetic surface known as a bowling green which is divided into parallel playing strips called rinks. In the simplest competition, singles, one of the two opponents flips a coin to see who wins the "mat" and begins a segment of the competition (in bowling parlance, an "end"), by placing the mat and rolling the jack to the other end of the green to serve as a target. Once it has come to rest, the jack is aligned to the centre of the rink and the players take turns to roll their bowls from the mat towards the jack and thereby build up the "head".
A bowl may curve outside the rink boundary on its path, but must come to rest within the rink boundary to remain in play. Bowls falling into the ditch are dead and removed from play, except in the event when one has "touched" the jack on its way. "Touchers" are marked with chalk and remain alive in play even though they are in the ditch. Similarly if the jack is knocked into the ditch it is still alive unless it is out of bounds to the side resulting in a "dead" end which is replayed, though according to international rules the jack is "respotted" to the centre of the rink and the end is continued. After each competitor has delivered all of their bowls (four each in singles and pairs, three each in triples, and two bowls each in fours), the distance of the closest bowls to the jack is determined (the jack may have been displaced) and points, called "shots", are awarded for each bowl which a competitor has closer than the opponent's nearest to the jack. For instance, if a competitor has bowled two bowls closer to the jack than their opponent's nearest, they are awarded two shots. The exercise is then repeated for the next end, a game of bowls typically being of twenty-one ends.
Lawn bowls is played on grass and variations from green to green are common. Greens come in all shapes and sizes, fast, slow, big crown, small crown and so on.
Scoring.
Scoring systems vary from competition to competition. Games can be decided when:
Games to a specified number of ends may also be drawn. The draw may stand, or the opponents may be required to play an extra end to decide the winner. These provisions are always published beforehand in the event's Conditions of Play.
In the Laws of the Sport of Bowls
the winner in a singles game is the first player to score 21 shots. In all other disciplines (pairs, triples, fours) the winner is the team who has scored the most shots after 21/25 ends of play. Often local tournaments will play shorter games (often 10 or 12 ends). Some competitions use a "set" scoring system, with the first to seven points awarded a set in a best-or-three or best-of-five set match. As well as singles competition, there can be two (pairs), three (triples) and four-player (fours) teams. In these, teams bowl alternately, with each player within a team bowling all their bowls, then handing over to the next player. The team captain or "skip" always plays last and is instrumental in directing his team's shots and tactics. The current method of scoring in the professional tour (World Bowls Tour) is sets. Each set consists of nine ends and the player with the most shots at the end of a set wins the set. If the score is tied the set is halved. If a player wins two sets, or gets a win and a tie, that player wins the game. If each player wins a set, or both sets end tied, there is a 3-end tiebreaker to determine a winner.
Bias of bowls.
Bowls are designed to travel a curved path because of a weight bias which was originally produced by inserting weights in one side of the bowl. This is no longer permitted by the rules and bias is now produced entirely by the shape of the bowl. A bowler determines the bias direction of the bowl in his hand by a dimple or symbol on one side. Regulations determine the minimum bias allowed, and the range of diameters (11.6 to 13.1 cm), but within these rules bowlers can and do choose bowls to suit their own preference. They were originally made from lignum vitae, a dense wood giving rise to the term "woods" for bowls, but are now more typically made of a hard plastic composite material.
Bowls were once only available coloured black or brown but they are now available in a variety of colours. They have unique symbol markings engraved on them for identification. Since many bowls look the same, coloured, adhesive stickers or labels are also used to mark the bowls of each team in bowls matches. Some local associations agree on specific colours for stickers for each of the clubs in their area. Provincial or national colours are often assigned in national and international competitions. These stickers are used by officials to distinguish teams.
Bowls have symbols unique to the set of four for identification. The side of the bowl with a larger symbol within a circle indicates the side away from the bias. That side with a smaller symbol within a smaller circle is the bias side toward which the bowl will turn. It is not uncommon for players to deliver a "wrong bias" shot from time to time and see their carefully aimed bowl crossing neighbouring rinks rather than heading towards their jack.
When bowling there are several types of delivery. "Draw" shots are those where the bowl is rolled to a specific location without causing too much disturbance of bowls already in the head. For a right-handed bowler, "forehand draw" or "finger peg" is initially aimed to the right of the jack, and curves in to the left. The same bowler can deliver a "backhand draw" or "thumb peg" by turning the bowl over in his hand and curving it the opposite way, from left to right. In both cases, the bowl is rolled as close to the jack as possible, unless tactics demand otherwise. A "drive" or "fire" or "strike" involves bowling with force with the aim of knocking either the jack or a specific bowl out of play - and with the drive's speed, there is virtually no noticeable (or, at least, much less) curve on the shot. An "upshot" or "yard on" shot involves delivering the bowl with an extra degree of weight (often referred to as "controlled" weight or "rambler"), enough to displace the jack or disturb other bowls in the head without killing the end. A "block" shot is one that is intentionally placed short to defend from a drive or to stop an oppositions draw shot. The challenge in all these shots is to be able to adjust line and length accordingly, the faster the delivery, the narrower the line or "green".
Variations of play.
Particularly in team competition there can be a large number of bowls on the green towards the conclusion of the end, and this gives rise to complex tactics. Teams "holding shot" with the closest bowl will often make their subsequent shots not with the goal of placing the bowl near the jack, but in positions to make it difficult for opponents to get their bowls into the head, or to places where the jack might be deflected to if the opponent attempts to disturb the head.
There are many different ways to set up the game. Crown Green Bowling utilises the entire green. A player can send the jack anywhere on the green in this game and the green itself is more akin to a golf green, with lots of undulation. It is only played with two bowls each, the Jack also has a bias and is only slightly smaller than the Bowls. The game is played usually to 21-up in Singles and Doubles format with some competitions playing to 31-up. The Panel (Professional Crown Green Bowls) is played at the Red Lion, Westhoughton daily and is played to 41-up with greenside betting throughout play. The game of Crown Green Bowls is looking to grow with the introduction of the Portuguese Masters in October and recent interest from Sky to re-televise the sport.
Singles, triples and fours and Australian pairs are some ways the game can be played. In singles, two people play against each other and the first to reach 21, 25 or 31 shots (as decided by the controlling body) is the winner. In one variation of singles play, each player uses two bowls only and the game is played over 21 ends. A player concedes the game before the 21st end if the score difference is such that it is impossible to draw equal or win within the 21 ends. If the score is equal after 21 ends, an extra end is played to decide the winner. An additional scoring method is set play. This comprises two sets over nine ends. Should a player win a set each, they then play a further 3 ends that will decide the winner.
Pairs allows both people on a team to play Skip and Lead. The lead throws two bowls, the skip delivers two, then the lead delivers his remaining two, the skip then delivers his remaining two bowls. Each end, the leads and skips switch positions. This is played over 21 ends or sets play. Triples is with three players while Fours is with four players in each team and is played over 21 ends.
Another pairs variation is 242 pairs (also known as Australian Pairs). In the first end of the game the A players lead off with 2 bowls each, then the B players play 4 bowls each, before the A players complete the end with their final 2 bowls. The A players act as lead and skip in the same end. In the second end the roles are reversed with the A players being in the middle. This alternating pattern continues through the game which is typically over 15 ends.
Short Mat Bowls is an all-year sport unaffected by weather conditions and it does not require a permanent location as the rink mats can be rolled up and stowed away. This makes it particularly appropriate for small communities as it can be played in village halls, schools, sports and social clubs, hotels and so on. where space is restricted and is also required for other purposes: it is even played on North Sea oil rigs where space is really at a premium.
Bowls are played by the blind and paraplegic. Blind bowlers are extremely skilful. A string is run out down the centre of the lane & wherever the jack lands it is moved across to the string and the length is called out by a sighted marker, when the woods are sent the distance from the jack is called out, in yards, feet and inches-the position in relation to the jack is given using the clock,12.00 is behind the jack. The world's best are a match for the best club level sighted bowlers .
Competitions.
There is a World Indoor Bowls Championships and also World Bowls Events.
Bowls is one of the "core sports" that must be included at each edition of the Commonwealth Games. With the exception of the 1966 Games, the sport has been included in all Games since their inception in 1930. Delhi, India hosted the 2010 Commonwealth Games, with Natalie Melmore (England) and Robert Weale (Wales) winning the singles gold medals. Glasgow, Scotland will host the 2014 Commonwealth Games.

</doc>
<doc id="4249" url="http://en.wikipedia.org/wiki?curid=4249" title="Barcelonnette">
Barcelonnette

Barcelonnette () is a commune of France and a subprefecture in the department of Alpes-de-Haute-Provence, in the Provence-Alpes-Côte d'Azur region. It is located in the southern French Alps, at the crossroads between Provence, Piedmont and the Dauphiné, and is the largest town in the Ubaye Valley. The town's inhabitants are known as "Barcelonnettes".
Toponymy.
Barcelonette was founded and named in 1231, by Ramon Berenguer IV, Count of Provence. While the town's name is generally seen as a diminutive form of Barcelona in Spain, Albert Dauzat and Charles Rostaing point out an earlier attestation of the name "Barcilona" in Barcelonnette in around 1200, and suggest that it is derived instead from two earlier stems signifying a mountain, *"bar" and *"cin" (the latter of which is also seen in the name of Mont Cenis).
In the Vivaro-Alpine dialect of Occitan, the town is known as "Barcilona de Provença" or more rarely "Barciloneta" according to the classical norm; under the Mistralian norm it is called "Barcilouna de Prouvença" or "Barcilouneto". In "Valéian" (the dialect of Occitan spoken in the Ubaye Valley), it is called "Barcilouna de Prouvença" or "Barcilounéta". "Barcino Nova" is the town's Latin name meaning "new Barcelona"; "Barcino" was the Roman name for Barcelona in Spain from its foundation by Emperor Augustus in 10 BC, and it was only changed to "Barcelona" in the Middle Ages.
The inhabitants of the town are called "Barcelonnettes", or "Vilandroises" in Valéian.
History.
Origins.
The Barcelonnette region was populated by Ligures from the 1st millennium BC onwards, and the arrival of the Celts several centuries later led to the formation of a mixed Celto-Ligurian people, the Vesubians. Polybius described the Vesubians as belligerent but nonetheless civilised and mercantile, and Julius Caesar praised their bravery. The work "History of the Gauls" also places the Vesubians in the Ubaye Valley.
Following the Roman conquest of Provence, Barcelonnette was included in a small province with modern Embrun as its capital and governed by Albanus Bassalus. This was integrated soon afterwards into Gallia Narbonensis. In 36 AD, Emperor Nero transferred Barcelonnette to the province of the Cottian Alps. The town was known as "Rigomagensium" under the Roman Empire and was the capital of a civitas (a provincial subdivision), though no Roman money has yet been found in the canton of Barcelonnette.
Medieval town.
The town of Barcelonnette was founded in 1231 by Ramon Berenguer IV, Count of Provence. According to Charles Rostaing, this act of formal "foundation", according certain privileges to the town, was a means of regenerating the destroyed town of "Barcilona". The town was afforded a "consulat" (giving it the power to administer and defend itself) in 1240.
Control of the area in the Middle Ages swung between the Counts of Savoy and of Provence. In 1388, after Count Louis II of Provence had left to conquer Naples, the Count of Savoy Amadeus VIII took control of Barcelonnette; however, it returned to Provençal control in 1390, with the d'Audiffret family as its lords. On the death of Louis II in 1417 it reverted to Savoy, and, although Count René again retook the area for Provence in 1471, it had returned to Savoyard dominance by the start of the 16th century, by which point the County of Provence had become united with the Kingdom of France due to the death of Count Charles V in 1481.
Ancien Régime.
During Charles V's invasion of Provence in 1536, Francis I of France sent the Count of Fürstenberg's 6000 "Landsknechte" to ravage the area in a scorched earth policy. Barcelonnette and the Ubaye Valley remained under French sovereignty until the second Treaty of Cateau-Cambrésis on 3 April 1559.
In 1588 the troops of François, Duke of Lesdiguières entered the town and set fire to the church and convent during their campaign against the Duke of Savoy. In 1600, after the Treaty of Vervins, conflict returned between Henry IV of France and Savoy, and Lesdiguières retook Barcelonnette until the conclusion of the Treaty of Lyon on 17 January the following year. In 1628, during the War of the Mantuan Succession, Barcelonnette and the other towns of the Ubaye Valley were pillaged and burned by Jacques du Blé d'Uxelles and his troops, as they passed through towards Italy to the Duke of Mantua's aid. The town was retaken by the Duke of Savoy in 1630; and in 1691 it was captured by the troops of the Marquis de Vins during the War of the League of Augsburg.
Between 1614 and 1713, Barcelonnette was the seat of one of the four prefectures under the jurisdiction of the Senate of Nice. At this time, the community of Barcelonnette successfully purchased the "seigneurie" of the town as it was put to auction by the Duke of Savoy; it thereby gained its own justicial powers. In 1646, a college was founded in Barcelonnette.
A "significant" part of the town's inhabitants had, by the 16th century, converted to Protestantism, and were repressed during the French Wars of Religion.
The "viguerie" of Barcelonnette (also comprising Saint-Martin and Entraunes) was reattached to France in 1713 as part of a territorial exchange with the Duchy of Savoy during the Treaties of Utrecht. The town remained the site of a "viguerie" until the French Revolution. A decree of the council of state on 25 December 1714 reunited Barcelonnete with the general government of Provence.
Revolution.
Barcelonnette was one of few settlements in Haute-Provence to acquire a Masonic Lodge before the Revolution, in fact having two:
In March 1789, riots took place as a result of a crisis in wheat production. In July, the Great Fear of aristocratic reprisal against the ongoing French Revolution struck France, arriving in the Barcelonnette area on 31 July 1789 (when the news of the storming of the Bastille first reached the town) before spreading towards Digne.
This agitation continued in the Ubaye Valley; a new revolt broke out on 14 June, and famine was declared in April 1792. The patriotic society of the commune was one of the first 21 created in Alpes-de-Haute-Provence, in spring 1792, by the envoys of the departmental administration. Around a third of the male population attended at the club. Another episode of political violence occurred in August 1792. 
Barcelonnette was the seat of the District of Barcelonnette from 1790 to 1800.
Modern history.
In December 1851, the town was home to a movement of republican resistance towards Napoleon III's coup. Though only a minority of the population, the movement rebelled on Sunday 7 December, the day after the news of the coup arrived. Town officials and gendarmes were disarmed and placed in the maison d'arrêt. A committee of public health was created on 8 December; on 9 December the inhabitants of Jausiers and its surroundings formed a colony under the direction of general councillor Brès, and Mayor Signoret of Saint-Paul-sur-Ubaye. This was stopped, however, on 10 December before it could reach Barcelonnette, as the priest of the subprefecture had intervened. On 11 December, several officials escaped and found refuge in L'Argentière in Piedmont. The arrival of troops on 16 December put a final end to the republican resistance without bloodshed, and 57 insurgents were tried; 38 were condemned to deportation (though several were pardoned in April).
Between 1850 and 1950, Barcelonnette was the source of a wave of emigration to Mexico. Among these emigrants was Jean Baptiste Ebrard, founder of the Liverpool department store chain in Mexico; Marcelo Ebrard, the head of government of Mexico City from 2006 to 2012, is also descended from them. On the edges of Barcelonnette and Jausiers there are several houses and villas of colonial style (known as "maisons mexicaines"), constructed by emigrants to Mexico who returned to France between 1870 and 1930. A plaque in the town commemorates the deaths of ten Mexican citizens who returned to Barcelonnette to fight in the First World War.
During the Second World War, 26 Jews were arrested in Barcelonnette before being deported. The 89th "compagnie de travailleurs étrangers" (Company of Foreign Workers), consisting of foreigners judged as undesirable by the Third Republic and the Vichy regime and committed to forced labour, was established in Barcelonnette.
The 11th Battalion of "Chasseurs alpins" was garrisoned at Barcelonnette between 1948 and 1990.
Geography.
Barcelonnette is situated in the wide and fertile Ubaye Valley, of which it is the largest town. It lies at an elevation of 1132 m (3717 ft) on the right bank of the Ubaye River, and is surrounded by mountains which reach peaks of over 3000 m; the tallest of these is the Needle of Chambeyron at 3412 m. Barcelonnette is situated 210 km from Turin, 91 km from Nice and 68 km from Gap.
Biodiversity.
As a result of its relief and geographic situation, the Ubaye Valley has an "abundance of plant and animal species". The fauna is largely constituted of golden eagles, marmots, ibex and vultures, and the flora includes a large proportion of larches, génépis and white asphodels.
Climate.
The Ubaye Valley has an alpine climate and winters are harsh as a result of the altitude, but there are only light winds as a result of the relief. There are on average almost 300 days of sun and 700 mm of rain per year.
Hazards.
None of the 200 communes of the department is entirely free of seismic risk; the canton of Barcelonnette is placed in zone 1b (low risk) by the determinist classification of 1991 based on seismic history, and zone 4 (average risk) according to the probabilistic EC8 classification of 2011. The commune is also vulnerable to avalanches, forest fires, floods, and landslides. Barcelonnette is also exposed to the possibility of a technological hazard in that road transport of dangerous materials is allowed to pass through on the RD900.
The town has been subject to several orders of natural disaster: floods and mudslides in 1994 and 2008, and landslides in 1996 and 1999. The strongest recorded earthquakes in the region occurred on 5 April 1959, with its epicentre at Saint-Paul-sur-Ubaye and a recorded intensity of 6.5 at Barcelonnette, and on 17 February 1947, with its epicentre at Prazzo over the Italian border.
Architecture.
The subprefecture has been situated since 1978 in a "maison mexicaine", the Villa l'Ubayette, constructed between 1901 and 1903.
Population.
In 1471, the community of Barcelonnette (including several surrounding parishes) comprised 421 fires (households). In 1765, it had 6674 inhabitants, but emigration, particularly to Mexico, slowed the town's growth in the period before the Second World War. According to the census of 2007, Barcelonnette has a population of 2766 (municipal population) or 2939 (total) across a total of 16.42 km2. The town is characterised by low population density. Between 1990 and 1999 the town's annual mean population growth was -0.6%, though between 1999 and 2007 this increased to an average of -0.1%.
Economy.
The city is mainly a tourist and resort centre, serving many ski lodges. The Pra Loup resort is 7 km from Barcelonnette; Le Sauze is 5 km away. It and the Ubaye Valley are served by the Barcelonnette - Saint-Pons Airport. Notably, Barcelonnette is the only subprefecture of France not be served by rail transport; the Ubaye line which would have linked Chorges to Barcelonnette was never completed as a result of the First World War and the construction of the Serre-Ponçon Dam between 1955 and 1961.
Education.
An "école normale" (an institute for training primary school teachers) was founded in Barcelonnette in 1833, and remained there until 1888 when it was transferred to Digne. The "lycée André-Honnorat de Barcelonnette", originally the "collège Saint-Maurice" and renamed after the politician André Honnorat in 1919, is located in the town; Pierre-Gilles de Gennes and Carole Merle both studied there. Currently, three schools exist in Barcelonnette: a public nursery school, a public elementary school, and a private school (under a contract by which the teachers are paid by the national education system).
In 2010 the "lycée André-Honnorat" opened a boarding school aimed at gifted students of poorer social backgrounds, in order to give them better conditions in which to study. It is located in the "Quartier Craplet", formerly the garrison of the 11th Battalion of "Chasseurs Alpins" and then the French Army's "Centre d'instruction et d'entraînement au combat en montagne" (CIECM).
International links.
Barcelonnette is twinned with:
It is also the site of a Mexican honorary consulate.

</doc>
